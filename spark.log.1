2019-02-13 19:27:09,565  ERROR --- [main]  AverageTest(line:18) : args error: please set the input and output path.
2019-02-13 19:28:04,010   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-13 19:28:04,482   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-13 19:28:04,693   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordCount
2019-02-13 19:28:04,852   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-13 19:28:04,853   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-13 19:28:04,854   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-13 19:28:04,855   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-13 19:28:04,857   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-13 19:28:06,697   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 23099.
2019-02-13 19:28:06,770   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-13 19:28:06,820   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-13 19:28:06,827   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-13 19:28:06,827   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-13 19:28:06,846   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-fad033ad-3d91-4982-8867-184bc6a1f0f1
2019-02-13 19:28:06,891   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-13 19:28:06,916   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-13 19:28:07,068   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5293ms
2019-02-13 19:28:07,175   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-13 19:28:07,196   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5424ms
2019-02-13 19:28:07,234   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 19:28:07,234   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-13 19:28:07,268   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,269   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,275   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,278   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,278   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,279   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,280   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,281   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,282   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,282   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,283   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,284   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,284   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,285   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,286   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,287   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,291   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,292   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,293   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,293   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,304   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,307   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,308   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,310   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-13 19:28:07,312   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-13 19:28:07,507   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-13 19:28:07,587   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23108.
2019-02-13 19:28:07,588   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:23108
2019-02-13 19:28:07,592   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-13 19:28:07,632   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 23108, None)
2019-02-13 19:28:07,638   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:23108 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 23108, None)
2019-02-13 19:28:07,643   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 23108, None)
2019-02-13 19:28:07,643   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 23108, None)
2019-02-13 19:28:07,959   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:08,578   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-13 19:28:08,603   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 19:28:08,606   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-13 19:28:08,626   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-13 19:28:08,642   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-13 19:28:08,643   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-13 19:28:08,650   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-13 19:28:08,658   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-13 19:28:08,668   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-13 19:28:08,669   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-13 19:28:08,670   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-ab666e8c-05c7-4b8e-9bd4-c39de897969f
2019-02-13 19:28:53,319   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-13 19:28:53,709   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-13 19:28:53,832   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordCount
2019-02-13 19:28:53,944   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-13 19:28:53,944   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-13 19:28:53,945   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-13 19:28:53,945   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-13 19:28:53,946   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-13 19:28:55,351   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 23204.
2019-02-13 19:28:55,389   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-13 19:28:55,423   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-13 19:28:55,427   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-13 19:28:55,427   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-13 19:28:55,444   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-fb311180-17cd-4af7-9e11-ea8b188574f3
2019-02-13 19:28:55,479   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-13 19:28:55,499   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-13 19:28:55,611   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3931ms
2019-02-13 19:28:55,689   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-13 19:28:55,704   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4026ms
2019-02-13 19:28:55,729   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 19:28:55,729   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-13 19:28:55,752   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,753   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,753   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,758   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,759   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,759   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,760   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,761   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,762   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,762   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,763   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,764   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,764   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,764   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,765   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,765   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,766   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,766   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,774   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,774   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,775   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,777   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,778   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-13 19:28:55,779   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-13 19:28:55,914   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-13 19:28:55,972   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23213.
2019-02-13 19:28:55,973   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:23213
2019-02-13 19:28:55,974   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-13 19:28:56,009   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 23213, None)
2019-02-13 19:28:56,012   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:23213 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 23213, None)
2019-02-13 19:28:56,016   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 23213, None)
2019-02-13 19:28:56,016   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 23213, None)
2019-02-13 19:28:56,242   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-13 19:28:56,754   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at AverageTest.java:34
2019-02-13 19:28:56,792   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at AverageTest.java:22)
2019-02-13 19:28:56,796   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at AverageTest.java:34) with 1 output partitions
2019-02-13 19:28:56,796   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at AverageTest.java:34)
2019-02-13 19:28:56,797   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-02-13 19:28:56,799   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-02-13 19:28:56,805   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at AverageTest.java:22), which has no missing parents
2019-02-13 19:28:56,895   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-13 19:28:56,908   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 1048.8 MB)
2019-02-13 19:28:57,016   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1975.0 B, free 1048.8 MB)
2019-02-13 19:28:57,019   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:23213 (size: 1975.0 B, free: 1048.8 MB)
2019-02-13 19:28:57,022   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:28:57,039   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at AverageTest.java:22) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:28:57,041   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-13 19:28:57,127   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7539 bytes)
2019-02-13 19:28:57,141   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-13 19:28:57,270   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1033 bytes result sent to driver
2019-02-13 19:28:57,284   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 187 ms on localhost (executor driver) (1/1)
2019-02-13 19:28:57,288   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-13 19:28:57,298   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (parallelizePairs at AverageTest.java:22) finished in 0.466 s
2019-02-13 19:28:57,298   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-13 19:28:57,299   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-13 19:28:57,299   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-02-13 19:28:57,300   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-13 19:28:57,303   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[1] at combineByKey at AverageTest.java:30), which has no missing parents
2019-02-13 19:28:57,323   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 1048.8 MB)
2019-02-13 19:28:57,329   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-13 19:28:57,330   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:23213 (size: 2.3 KB, free: 1048.8 MB)
2019-02-13 19:28:57,331   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:28:57,333   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[1] at combineByKey at AverageTest.java:30) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:28:57,333   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-13 19:28:57,337   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-13 19:28:57,338   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-13 19:28:57,566   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-13 19:28:57,568   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 8 ms
2019-02-13 19:28:57,603   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1429 bytes result sent to driver
2019-02-13 19:28:57,605   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 269 ms on localhost (executor driver) (1/1)
2019-02-13 19:28:57,606   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-13 19:28:57,607   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at AverageTest.java:34) finished in 0.286 s
2019-02-13 19:28:57,612   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at AverageTest.java:34, took 0.858639 s
2019-02-13 19:28:57,639   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-13 19:28:57,647   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 19:28:57,648   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-13 19:28:57,664   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-13 19:28:57,704   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-13 19:28:57,704   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-13 19:28:57,711   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-13 19:28:57,713   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-13 19:28:57,720   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-13 19:28:57,720   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-13 19:28:57,721   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-6371aeab-0861-4d1f-bda3-d2b9fc76a878
2019-02-13 19:29:18,938   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-13 19:29:19,310   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-13 19:29:19,433   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordCount
2019-02-13 19:29:19,542   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-13 19:29:19,544   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-13 19:29:19,544   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-13 19:29:19,545   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-13 19:29:19,545   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-13 19:29:20,917   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 23266.
2019-02-13 19:29:20,958   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-13 19:29:20,986   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-13 19:29:20,989   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-13 19:29:20,989   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-13 19:29:21,004   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-26306558-cc98-4996-b576-76c91290186f
2019-02-13 19:29:21,039   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-13 19:29:21,058   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-13 19:29:21,170   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3819ms
2019-02-13 19:29:21,242   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-13 19:29:21,258   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @3908ms
2019-02-13 19:29:21,284   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 19:29:21,284   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-13 19:29:21,306   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,311   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,312   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,313   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,313   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,314   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,314   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,315   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,316   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,317   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,318   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,318   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,319   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,320   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,320   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,321   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,322   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,322   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,323   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,324   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,330   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,331   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,331   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,332   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,333   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-13 19:29:21,335   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-13 19:29:21,466   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-13 19:29:21,524   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23276.
2019-02-13 19:29:21,525   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:23276
2019-02-13 19:29:21,527   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-13 19:29:21,562   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 23276, None)
2019-02-13 19:29:21,566   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:23276 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 23276, None)
2019-02-13 19:29:21,570   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 23276, None)
2019-02-13 19:29:21,571   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 23276, None)
2019-02-13 19:29:21,793   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-13 19:29:22,274   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at AverageTest.java:34
2019-02-13 19:29:22,307   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at AverageTest.java:22)
2019-02-13 19:29:22,311   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at AverageTest.java:34) with 1 output partitions
2019-02-13 19:29:22,312   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at AverageTest.java:34)
2019-02-13 19:29:22,312   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-02-13 19:29:22,314   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-02-13 19:29:22,320   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at AverageTest.java:22), which has no missing parents
2019-02-13 19:29:22,399   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-13 19:29:22,410   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 1048.8 MB)
2019-02-13 19:29:22,504   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1975.0 B, free 1048.8 MB)
2019-02-13 19:29:22,507   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:23276 (size: 1975.0 B, free: 1048.8 MB)
2019-02-13 19:29:22,509   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:29:22,526   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at AverageTest.java:22) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:29:22,528   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-13 19:29:22,606   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7539 bytes)
2019-02-13 19:29:22,620   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-13 19:29:22,731   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1033 bytes result sent to driver
2019-02-13 19:29:22,747   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 173 ms on localhost (executor driver) (1/1)
2019-02-13 19:29:22,750   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-13 19:29:22,757   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (parallelizePairs at AverageTest.java:22) finished in 0.412 s
2019-02-13 19:29:22,758   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-13 19:29:22,759   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-13 19:29:22,759   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-02-13 19:29:22,759   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-13 19:29:22,763   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[1] at combineByKey at AverageTest.java:30), which has no missing parents
2019-02-13 19:29:22,783   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 1048.8 MB)
2019-02-13 19:29:22,787   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-13 19:29:22,788   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:23276 (size: 2.3 KB, free: 1048.8 MB)
2019-02-13 19:29:22,788   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:29:22,790   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[1] at combineByKey at AverageTest.java:30) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:29:22,791   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-13 19:29:22,795   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-13 19:29:22,795   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-13 19:29:23,003   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-13 19:29:23,005   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 8 ms
2019-02-13 19:29:23,039   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1429 bytes result sent to driver
2019-02-13 19:29:23,041   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 248 ms on localhost (executor driver) (1/1)
2019-02-13 19:29:23,042   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-13 19:29:23,043   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at AverageTest.java:34) finished in 0.261 s
2019-02-13 19:29:23,050   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at AverageTest.java:34, took 0.774638 s
2019-02-13 19:29:23,076   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at AverageTest.java:38
2019-02-13 19:29:23,077   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at AverageTest.java:38) with 1 output partitions
2019-02-13 19:29:23,078   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 3 (collect at AverageTest.java:38)
2019-02-13 19:29:23,078   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 2)
2019-02-13 19:29:23,078   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-13 19:29:23,078   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 3 (MapPartitionsRDD[2] at map at AverageTest.java:36), which has no missing parents
2019-02-13 19:29:23,082   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 1048.8 MB)
2019-02-13 19:29:23,084   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 1048.8 MB)
2019-02-13 19:29:23,090   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:23276 (size: 2.7 KB, free: 1048.8 MB)
2019-02-13 19:29:23,091   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:29:23,092   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at map at AverageTest.java:36) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:29:23,092   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 1 tasks
2019-02-13 19:29:23,093   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-13 19:29:23,093   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 2)
2019-02-13 19:29:23,099   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-13 19:29:23,099   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-02-13 19:29:23,103   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 2). 1235 bytes result sent to driver
2019-02-13 19:29:23,104   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 2) in 11 ms on localhost (executor driver) (1/1)
2019-02-13 19:29:23,104   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-02-13 19:29:23,105   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 3 (collect at AverageTest.java:38) finished in 0.025 s
2019-02-13 19:29:23,106   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: collect at AverageTest.java:38, took 0.029411 s
2019-02-13 19:29:23,107   INFO --- [main]  AverageTest(line:39) : success
2019-02-13 19:29:23,113   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 19:29:23,115   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-13 19:29:23,125   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-13 19:29:23,164   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-13 19:29:23,165   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-13 19:29:23,175   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-13 19:29:23,177   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-13 19:29:23,184   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-13 19:29:23,186   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-13 19:29:23,187   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-aa06b6ac-f052-4886-88f9-bab7c55a4cfc
2019-02-13 19:30:23,509   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-13 19:30:23,888   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-13 19:30:24,012   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordCount
2019-02-13 19:30:24,124   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-13 19:30:24,125   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-13 19:30:24,126   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-13 19:30:24,126   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-13 19:30:24,127   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-13 19:30:25,483   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 23394.
2019-02-13 19:30:25,522   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-13 19:30:25,551   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-13 19:30:25,554   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-13 19:30:25,554   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-13 19:30:25,570   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-ae60eb0b-9942-4c23-b0ed-8e623f3d2188
2019-02-13 19:30:25,607   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-13 19:30:25,628   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-13 19:30:25,740   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3849ms
2019-02-13 19:30:25,815   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-13 19:30:25,831   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @3941ms
2019-02-13 19:30:25,855   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 19:30:25,855   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-13 19:30:25,878   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,882   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,883   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,883   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,884   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,884   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,885   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,886   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,886   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,886   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,887   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,888   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,888   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,888   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,889   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,889   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,891   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,891   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,892   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,892   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,900   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,902   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,904   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-13 19:30:25,906   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-13 19:30:26,033   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-13 19:30:26,088   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23404.
2019-02-13 19:30:26,089   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:23404
2019-02-13 19:30:26,091   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-13 19:30:26,126   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 23404, None)
2019-02-13 19:30:26,130   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:23404 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 23404, None)
2019-02-13 19:30:26,133   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 23404, None)
2019-02-13 19:30:26,134   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 23404, None)
2019-02-13 19:30:26,356   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-13 19:30:26,842   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at AverageTest.java:34
2019-02-13 19:30:26,875   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at AverageTest.java:22)
2019-02-13 19:30:26,879   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at AverageTest.java:34) with 1 output partitions
2019-02-13 19:30:26,879   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at AverageTest.java:34)
2019-02-13 19:30:26,881   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-02-13 19:30:26,883   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-02-13 19:30:26,889   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at AverageTest.java:22), which has no missing parents
2019-02-13 19:30:26,971   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-13 19:30:26,981   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 1048.8 MB)
2019-02-13 19:30:27,074   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1975.0 B, free 1048.8 MB)
2019-02-13 19:30:27,078   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:23404 (size: 1975.0 B, free: 1048.8 MB)
2019-02-13 19:30:27,080   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:30:27,096   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at AverageTest.java:22) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:30:27,097   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-13 19:30:27,175   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7539 bytes)
2019-02-13 19:30:27,190   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-13 19:30:27,304   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1033 bytes result sent to driver
2019-02-13 19:30:27,319   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 175 ms on localhost (executor driver) (1/1)
2019-02-13 19:30:27,322   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-13 19:30:27,331   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (parallelizePairs at AverageTest.java:22) finished in 0.416 s
2019-02-13 19:30:27,331   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-13 19:30:27,332   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-13 19:30:27,332   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-02-13 19:30:27,333   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-13 19:30:27,335   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[1] at combineByKey at AverageTest.java:30), which has no missing parents
2019-02-13 19:30:27,352   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 1048.8 MB)
2019-02-13 19:30:27,356   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-13 19:30:27,358   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:23404 (size: 2.3 KB, free: 1048.8 MB)
2019-02-13 19:30:27,359   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:30:27,361   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[1] at combineByKey at AverageTest.java:30) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:30:27,361   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-13 19:30:27,365   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-13 19:30:27,365   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-13 19:30:27,575   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-13 19:30:27,577   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 8 ms
2019-02-13 19:30:27,605   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1472 bytes result sent to driver
2019-02-13 19:30:27,607   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 244 ms on localhost (executor driver) (1/1)
2019-02-13 19:30:27,607   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-13 19:30:27,608   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at AverageTest.java:34) finished in 0.258 s
2019-02-13 19:30:27,614   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at AverageTest.java:34, took 0.771758 s
2019-02-13 19:30:27,638   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at AverageTest.java:38
2019-02-13 19:30:27,639   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at AverageTest.java:38) with 1 output partitions
2019-02-13 19:30:27,640   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 3 (collect at AverageTest.java:38)
2019-02-13 19:30:27,640   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 2)
2019-02-13 19:30:27,640   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-13 19:30:27,640   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 3 (MapPartitionsRDD[2] at map at AverageTest.java:36), which has no missing parents
2019-02-13 19:30:27,643   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 1048.8 MB)
2019-02-13 19:30:27,646   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 1048.8 MB)
2019-02-13 19:30:27,650   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:23404 (size: 2.7 KB, free: 1048.8 MB)
2019-02-13 19:30:27,651   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:30:27,653   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at map at AverageTest.java:36) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:30:27,653   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 1 tasks
2019-02-13 19:30:27,654   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-13 19:30:27,654   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 2)
2019-02-13 19:30:27,660   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-13 19:30:27,660   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-02-13 19:30:27,664   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 2). 1308 bytes result sent to driver
2019-02-13 19:30:27,666   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 2) in 12 ms on localhost (executor driver) (1/1)
2019-02-13 19:30:27,666   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-02-13 19:30:27,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 3 (collect at AverageTest.java:38) finished in 0.025 s
2019-02-13 19:30:27,667   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: collect at AverageTest.java:38, took 0.029699 s
2019-02-13 19:30:27,669   INFO --- [main]  AverageTest(line:39) : success
2019-02-13 19:30:27,675   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 19:30:27,676   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-13 19:30:27,687   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-13 19:30:27,726   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-13 19:30:27,726   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-13 19:30:27,737   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-13 19:30:27,740   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-13 19:30:27,746   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-13 19:30:27,748   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-13 19:30:27,748   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-634eaf0d-1196-45a1-a558-2a8c86480d26
2019-02-13 19:32:47,144   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-13 19:32:47,516   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-13 19:32:47,640   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordCount
2019-02-13 19:32:47,754   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-13 19:32:47,755   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-13 19:32:47,756   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-13 19:32:47,756   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-13 19:32:47,757   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-13 19:32:49,100   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 23658.
2019-02-13 19:32:49,131   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-13 19:32:49,159   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-13 19:32:49,162   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-13 19:32:49,162   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-13 19:32:49,178   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-d1ea6517-60df-4101-87f0-3f3608b26e65
2019-02-13 19:32:49,212   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-13 19:32:49,232   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-13 19:32:49,346   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @3816ms
2019-02-13 19:32:49,421   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-13 19:32:49,436   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @3905ms
2019-02-13 19:32:49,461   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 19:32:49,461   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-13 19:32:49,484   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,488   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,489   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,489   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,490   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,490   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,491   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,492   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,492   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,493   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,493   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,494   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,494   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,494   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,496   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,496   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,497   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,497   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,498   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,498   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,504   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,505   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,506   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,506   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,507   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-13 19:32:49,508   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-13 19:32:49,636   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-13 19:32:49,693   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23669.
2019-02-13 19:32:49,694   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:23669
2019-02-13 19:32:49,696   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-13 19:32:49,734   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 23669, None)
2019-02-13 19:32:49,738   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:23669 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 23669, None)
2019-02-13 19:32:49,741   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 23669, None)
2019-02-13 19:32:49,742   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 23669, None)
2019-02-13 19:32:49,971   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-13 19:32:50,462   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at AverageTest.java:34
2019-02-13 19:32:50,500   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at AverageTest.java:22)
2019-02-13 19:32:50,504   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at AverageTest.java:34) with 1 output partitions
2019-02-13 19:32:50,505   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at AverageTest.java:34)
2019-02-13 19:32:50,505   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-02-13 19:32:50,507   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-02-13 19:32:50,513   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at AverageTest.java:22), which has no missing parents
2019-02-13 19:32:50,609   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-13 19:32:50,620   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 1048.8 MB)
2019-02-13 19:32:50,719   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1975.0 B, free 1048.8 MB)
2019-02-13 19:32:50,723   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:23669 (size: 1975.0 B, free: 1048.8 MB)
2019-02-13 19:32:50,725   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:32:50,741   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at AverageTest.java:22) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:32:50,742   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-13 19:32:50,819   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7611 bytes)
2019-02-13 19:32:50,835   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-13 19:32:50,956   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1033 bytes result sent to driver
2019-02-13 19:32:50,970   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 182 ms on localhost (executor driver) (1/1)
2019-02-13 19:32:50,974   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-13 19:32:50,981   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (parallelizePairs at AverageTest.java:22) finished in 0.436 s
2019-02-13 19:32:50,981   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-13 19:32:50,982   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-13 19:32:50,982   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-02-13 19:32:50,982   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-13 19:32:50,985   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[1] at combineByKey at AverageTest.java:30), which has no missing parents
2019-02-13 19:32:51,002   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 1048.8 MB)
2019-02-13 19:32:51,007   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-13 19:32:51,008   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:23669 (size: 2.3 KB, free: 1048.8 MB)
2019-02-13 19:32:51,008   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:32:51,011   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[1] at combineByKey at AverageTest.java:30) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:32:51,011   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-13 19:32:51,015   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-13 19:32:51,015   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-13 19:32:51,225   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-13 19:32:51,227   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 7 ms
2019-02-13 19:32:51,256   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1470 bytes result sent to driver
2019-02-13 19:32:51,258   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 245 ms on localhost (executor driver) (1/1)
2019-02-13 19:32:51,258   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-13 19:32:51,259   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at AverageTest.java:34) finished in 0.259 s
2019-02-13 19:32:51,267   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at AverageTest.java:34, took 0.804634 s
2019-02-13 19:32:51,290   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at AverageTest.java:38
2019-02-13 19:32:51,293   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at AverageTest.java:38) with 1 output partitions
2019-02-13 19:32:51,293   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 3 (collect at AverageTest.java:38)
2019-02-13 19:32:51,293   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 2)
2019-02-13 19:32:51,293   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-13 19:32:51,293   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 3 (MapPartitionsRDD[2] at map at AverageTest.java:36), which has no missing parents
2019-02-13 19:32:51,297   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 1048.8 MB)
2019-02-13 19:32:51,299   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 1048.8 MB)
2019-02-13 19:32:51,302   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:23669 (size: 2.7 KB, free: 1048.8 MB)
2019-02-13 19:32:51,303   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:32:51,304   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at map at AverageTest.java:36) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:32:51,304   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 1 tasks
2019-02-13 19:32:51,305   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-13 19:32:51,305   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 2)
2019-02-13 19:32:51,312   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-13 19:32:51,312   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 1 ms
2019-02-13 19:32:51,315   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 2). 1308 bytes result sent to driver
2019-02-13 19:32:51,317   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 2) in 12 ms on localhost (executor driver) (1/1)
2019-02-13 19:32:51,317   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-02-13 19:32:51,318   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 3 (collect at AverageTest.java:38) finished in 0.023 s
2019-02-13 19:32:51,318   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: collect at AverageTest.java:38, took 0.028047 s
2019-02-13 19:32:51,319   INFO --- [main]  AverageTest(line:39) : success
2019-02-13 19:32:51,326   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 19:32:51,327   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-13 19:32:51,337   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-13 19:32:51,379   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-13 19:32:51,379   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-13 19:32:51,385   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-13 19:32:51,388   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-13 19:32:51,395   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-13 19:32:51,400   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-13 19:32:51,401   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-60a324e6-5528-4177-9e69-87f0d680e1f9
2019-02-13 19:47:55,274   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-13 19:47:55,748   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-13 19:47:55,906   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordCount
2019-02-13 19:47:56,033   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-13 19:47:56,034   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-13 19:47:56,035   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-13 19:47:56,035   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-13 19:47:56,036   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-13 19:47:57,561   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 25243.
2019-02-13 19:47:57,602   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-13 19:47:57,635   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-13 19:47:57,640   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-13 19:47:57,641   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-13 19:47:57,659   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-4276dd16-4557-436c-8b2d-73fd90aa373a
2019-02-13 19:47:57,709   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-13 19:47:57,732   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-13 19:47:57,865   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4363ms
2019-02-13 19:47:57,949   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-13 19:47:57,967   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4466ms
2019-02-13 19:47:57,997   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 19:47:57,997   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-13 19:47:58,023   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,028   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,028   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,029   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,030   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,030   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,031   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,032   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,032   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,033   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,033   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,034   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,034   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,035   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,036   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,036   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,036   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,037   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,037   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,038   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,047   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,048   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,050   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,050   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,052   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-13 19:47:58,053   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-13 19:47:58,197   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-13 19:47:58,271   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 25252.
2019-02-13 19:47:58,272   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:25252
2019-02-13 19:47:58,275   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-13 19:47:58,318   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 25252, None)
2019-02-13 19:47:58,322   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:25252 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 25252, None)
2019-02-13 19:47:58,327   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 25252, None)
2019-02-13 19:47:58,328   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 25252, None)
2019-02-13 19:47:58,602   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-13 19:47:59,193   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at AverageTest.java:34
2019-02-13 19:47:59,228   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at AverageTest.java:22)
2019-02-13 19:47:59,232   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at AverageTest.java:34) with 1 output partitions
2019-02-13 19:47:59,232   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at AverageTest.java:34)
2019-02-13 19:47:59,233   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-02-13 19:47:59,235   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-02-13 19:47:59,244   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at AverageTest.java:22), which has no missing parents
2019-02-13 19:47:59,360   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-13 19:47:59,377   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 1048.8 MB)
2019-02-13 19:47:59,492   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1975.0 B, free 1048.8 MB)
2019-02-13 19:47:59,496   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:25252 (size: 1975.0 B, free: 1048.8 MB)
2019-02-13 19:47:59,501   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:47:59,520   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at AverageTest.java:22) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:47:59,521   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-13 19:47:59,613   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7611 bytes)
2019-02-13 19:47:59,632   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-13 19:47:59,773   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1033 bytes result sent to driver
2019-02-13 19:47:59,788   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 218 ms on localhost (executor driver) (1/1)
2019-02-13 19:47:59,795   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-13 19:47:59,808   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (parallelizePairs at AverageTest.java:22) finished in 0.526 s
2019-02-13 19:47:59,809   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-13 19:47:59,809   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-13 19:47:59,810   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-02-13 19:47:59,811   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-13 19:47:59,817   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[1] at combineByKey at AverageTest.java:30), which has no missing parents
2019-02-13 19:47:59,844   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 1048.8 MB)
2019-02-13 19:47:59,849   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-13 19:47:59,853   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:25252 (size: 2.3 KB, free: 1048.8 MB)
2019-02-13 19:47:59,855   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:47:59,858   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[1] at combineByKey at AverageTest.java:30) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:47:59,858   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-13 19:47:59,862   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-13 19:47:59,863   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-13 19:48:00,114   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-13 19:48:00,116   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 10 ms
2019-02-13 19:48:00,158   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1527 bytes result sent to driver
2019-02-13 19:48:00,162   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 300 ms on localhost (executor driver) (1/1)
2019-02-13 19:48:00,162   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-13 19:48:00,163   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at AverageTest.java:34) finished in 0.321 s
2019-02-13 19:48:00,177   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at AverageTest.java:34, took 0.983541 s
2019-02-13 19:48:00,203   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at AverageTest.java:38
2019-02-13 19:48:00,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at AverageTest.java:38) with 1 output partitions
2019-02-13 19:48:00,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 3 (collect at AverageTest.java:38)
2019-02-13 19:48:00,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 2)
2019-02-13 19:48:00,206   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-13 19:48:00,206   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 3 (MapPartitionsRDD[2] at map at AverageTest.java:36), which has no missing parents
2019-02-13 19:48:00,209   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 1048.8 MB)
2019-02-13 19:48:00,212   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 1048.8 MB)
2019-02-13 19:48:00,214   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:25252 (size: 2.7 KB, free: 1048.8 MB)
2019-02-13 19:48:00,214   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:48:00,216   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at map at AverageTest.java:36) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:48:00,216   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 1 tasks
2019-02-13 19:48:00,217   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-13 19:48:00,217   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 2)
2019-02-13 19:48:00,222   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-13 19:48:00,223   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 1 ms
2019-02-13 19:48:00,226   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 2). 1265 bytes result sent to driver
2019-02-13 19:48:00,228   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 2) in 11 ms on localhost (executor driver) (1/1)
2019-02-13 19:48:00,228   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-02-13 19:48:00,229   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 3 (collect at AverageTest.java:38) finished in 0.022 s
2019-02-13 19:48:00,229   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: collect at AverageTest.java:38, took 0.026164 s
2019-02-13 19:48:00,230   INFO --- [main]  AverageTest(line:39) : success
2019-02-13 19:48:00,238   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 19:48:00,239   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-13 19:48:00,250   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-13 19:48:00,294   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-13 19:48:00,294   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-13 19:48:00,304   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-13 19:48:00,308   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-13 19:48:00,314   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-13 19:48:00,316   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-13 19:48:00,317   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-3356d297-3614-46b1-88a1-b91555c4ffd6
2019-02-13 19:48:26,424   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-13 19:48:26,850   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-13 19:48:26,998   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordCount
2019-02-13 19:48:27,100   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-13 19:48:27,101   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-13 19:48:27,101   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-13 19:48:27,102   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-13 19:48:27,102   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-13 19:48:28,568   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 25305.
2019-02-13 19:48:28,613   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-13 19:48:28,643   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-13 19:48:28,646   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-13 19:48:28,646   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-13 19:48:28,662   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-abf62f1e-47e1-4c27-bfd7-038c644ba3f2
2019-02-13 19:48:28,710   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-13 19:48:28,730   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-13 19:48:28,846   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4200ms
2019-02-13 19:48:28,925   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-13 19:48:28,941   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4296ms
2019-02-13 19:48:28,965   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 19:48:28,965   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-13 19:48:28,992   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-13 19:48:28,996   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-13 19:48:28,997   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-13 19:48:28,997   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-13 19:48:28,998   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-13 19:48:28,998   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-13 19:48:28,999   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-13 19:48:28,999   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,000   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,000   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,001   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,003   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,003   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,004   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,004   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,005   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,005   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,006   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,006   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,007   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,017   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,018   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,019   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,019   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,021   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-13 19:48:29,021   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-13 19:48:29,160   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-13 19:48:29,219   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 25314.
2019-02-13 19:48:29,220   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:25314
2019-02-13 19:48:29,222   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-13 19:48:29,259   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 25314, None)
2019-02-13 19:48:29,264   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:25314 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 25314, None)
2019-02-13 19:48:29,267   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 25314, None)
2019-02-13 19:48:29,268   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 25314, None)
2019-02-13 19:48:29,502   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-13 19:48:30,002   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at AverageTest.java:34
2019-02-13 19:48:30,036   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at AverageTest.java:22)
2019-02-13 19:48:30,040   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at AverageTest.java:34) with 1 output partitions
2019-02-13 19:48:30,041   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at AverageTest.java:34)
2019-02-13 19:48:30,041   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-02-13 19:48:30,043   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-02-13 19:48:30,050   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at AverageTest.java:22), which has no missing parents
2019-02-13 19:48:30,144   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-13 19:48:30,157   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 1048.8 MB)
2019-02-13 19:48:30,266   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1975.0 B, free 1048.8 MB)
2019-02-13 19:48:30,270   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:25314 (size: 1975.0 B, free: 1048.8 MB)
2019-02-13 19:48:30,272   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:48:30,290   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at AverageTest.java:22) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:48:30,291   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-13 19:48:30,369   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7611 bytes)
2019-02-13 19:48:30,385   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-13 19:48:30,499   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1033 bytes result sent to driver
2019-02-13 19:48:30,513   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 177 ms on localhost (executor driver) (1/1)
2019-02-13 19:48:30,517   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-13 19:48:30,523   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (parallelizePairs at AverageTest.java:22) finished in 0.448 s
2019-02-13 19:48:30,523   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-13 19:48:30,524   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-13 19:48:30,524   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-02-13 19:48:30,524   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-13 19:48:30,529   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[1] at combineByKey at AverageTest.java:30), which has no missing parents
2019-02-13 19:48:30,547   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 1048.8 MB)
2019-02-13 19:48:30,552   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-13 19:48:30,553   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:25314 (size: 2.3 KB, free: 1048.8 MB)
2019-02-13 19:48:30,553   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:48:30,556   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[1] at combineByKey at AverageTest.java:30) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:48:30,556   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-13 19:48:30,561   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-13 19:48:30,561   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-13 19:48:30,774   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-13 19:48:30,776   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 7 ms
2019-02-13 19:48:30,810   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1484 bytes result sent to driver
2019-02-13 19:48:30,813   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 254 ms on localhost (executor driver) (1/1)
2019-02-13 19:48:30,813   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-13 19:48:30,814   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at AverageTest.java:34) finished in 0.269 s
2019-02-13 19:48:30,819   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at AverageTest.java:34, took 0.816550 s
2019-02-13 19:48:30,844   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at AverageTest.java:38
2019-02-13 19:48:30,847   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at AverageTest.java:38) with 1 output partitions
2019-02-13 19:48:30,847   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 3 (collect at AverageTest.java:38)
2019-02-13 19:48:30,847   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 2)
2019-02-13 19:48:30,847   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-13 19:48:30,848   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 3 (MapPartitionsRDD[2] at map at AverageTest.java:36), which has no missing parents
2019-02-13 19:48:30,851   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 5.2 KB, free 1048.8 MB)
2019-02-13 19:48:30,854   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 1048.8 MB)
2019-02-13 19:48:30,856   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:25314 (size: 2.7 KB, free: 1048.8 MB)
2019-02-13 19:48:30,856   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-13 19:48:30,857   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at map at AverageTest.java:36) (first 15 tasks are for partitions Vector(0))
2019-02-13 19:48:30,858   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 1 tasks
2019-02-13 19:48:30,859   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-13 19:48:30,859   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 2)
2019-02-13 19:48:30,865   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-13 19:48:30,865   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 1 ms
2019-02-13 19:48:30,868   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 2). 1265 bytes result sent to driver
2019-02-13 19:48:30,871   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 2) in 13 ms on localhost (executor driver) (1/1)
2019-02-13 19:48:30,871   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-02-13 19:48:30,872   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 3 (collect at AverageTest.java:38) finished in 0.023 s
2019-02-13 19:48:30,872   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: collect at AverageTest.java:38, took 0.028906 s
2019-02-13 19:48:30,873   INFO --- [main]  AverageTest(line:39) : success
2019-02-13 19:48:30,879   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 19:48:30,880   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-13 19:48:30,894   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-13 19:48:30,936   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-13 19:48:30,936   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-13 19:48:30,947   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-13 19:48:30,951   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-13 19:48:30,957   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-13 19:48:30,959   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-13 19:48:30,960   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-3a3c917f-809e-43bb-bb2f-81fdd65f842d
2019-02-13 20:17:52,146   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-13 20:17:52,664   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-13 20:17:52,812   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: wordCount
2019-02-13 20:17:52,956   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-13 20:17:52,956   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-13 20:17:52,957   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-13 20:17:52,959   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-13 20:17:52,959   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-13 20:17:54,612   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 28581.
2019-02-13 20:17:54,654   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-13 20:17:54,688   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-13 20:17:54,693   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-13 20:17:54,694   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-13 20:17:54,715   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-5972e459-7278-47bf-a19b-f22ec28e91b2
2019-02-13 20:17:54,762   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-13 20:17:54,788   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-13 20:17:54,932   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4713ms
2019-02-13 20:17:55,034   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-13 20:17:55,052   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4834ms
2019-02-13 20:17:55,081   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 20:17:55,081   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-13 20:17:55,107   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,108   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,115   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,116   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,116   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,117   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,117   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,118   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,119   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,119   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,120   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,121   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,121   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,122   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,122   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,122   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,123   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,124   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,125   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,126   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,134   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,134   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,135   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,136   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,137   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-13 20:17:55,139   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-13 20:17:55,294   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-13 20:17:55,379   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 28593.
2019-02-13 20:17:55,379   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:28593
2019-02-13 20:17:55,381   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-13 20:17:55,435   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 28593, None)
2019-02-13 20:17:55,441   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:28593 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 28593, None)
2019-02-13 20:17:55,448   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 28593, None)
2019-02-13 20:17:55,449   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 28593, None)
2019-02-13 20:17:55,891   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-13 20:17:56,628   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at AggregateTest.java:33
2019-02-13 20:17:56,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at AggregateTest.java:21)
2019-02-13 20:17:56,671   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at AggregateTest.java:33) with 1 output partitions
2019-02-13 20:17:56,671   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at AggregateTest.java:33)
2019-02-13 20:17:56,672   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-02-13 20:17:56,674   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-02-13 20:17:56,680   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at AggregateTest.java:21), which has no missing parents
2019-02-13 20:17:56,764   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-13 20:17:56,776   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 4.0 KB, free 1048.8 MB)
2019-02-13 20:17:56,876   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-13 20:17:56,881   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:28593 (size: 2.3 KB, free: 1048.8 MB)
2019-02-13 20:17:56,884   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-13 20:17:56,905   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at AggregateTest.java:21) (first 15 tasks are for partitions Vector(0))
2019-02-13 20:17:56,906   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-13 20:17:56,997   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7611 bytes)
2019-02-13 20:17:57,013   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-13 20:17:57,242   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1076 bytes result sent to driver
2019-02-13 20:17:57,253   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 295 ms on localhost (executor driver) (1/1)
2019-02-13 20:17:57,257   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-13 20:17:57,265   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (parallelizePairs at AggregateTest.java:21) finished in 0.563 s
2019-02-13 20:17:57,266   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-13 20:17:57,266   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-13 20:17:57,266   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-02-13 20:17:57,267   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-13 20:17:57,271   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[1] at aggregateByKey at AggregateTest.java:29), which has no missing parents
2019-02-13 20:17:57,289   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.6 KB, free 1048.8 MB)
2019-02-13 20:17:57,294   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1048.8 MB)
2019-02-13 20:17:57,295   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:28593 (size: 2.5 KB, free: 1048.8 MB)
2019-02-13 20:17:57,296   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-13 20:17:57,298   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[1] at aggregateByKey at AggregateTest.java:29) (first 15 tasks are for partitions Vector(0))
2019-02-13 20:17:57,298   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-13 20:17:57,303   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-13 20:17:57,303   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-13 20:17:57,502   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-13 20:17:57,504   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 9 ms
2019-02-13 20:17:57,541   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1390 bytes result sent to driver
2019-02-13 20:17:57,543   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 242 ms on localhost (executor driver) (1/1)
2019-02-13 20:17:57,543   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-13 20:17:57,544   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at AggregateTest.java:33) finished in 0.257 s
2019-02-13 20:17:57,550   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at AggregateTest.java:33, took 0.921232 s
2019-02-13 20:17:57,559   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-13 20:17:57,562   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-13 20:17:57,574   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-13 20:17:57,638   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-13 20:17:57,640   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-13 20:17:57,653   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-13 20:17:57,656   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-13 20:17:57,665   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-13 20:17:57,668   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-13 20:17:57,669   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-888e0591-33d4-44b9-85c6-f5eb28cafe46
2019-02-14 10:29:35,541   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 10:29:36,376   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 10:29:36,542   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: AggregateTest
2019-02-14 10:29:36,685   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 10:29:36,688   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 10:29:36,689   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 10:29:36,689   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 10:29:36,690   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 10:29:38,776   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 50931.
2019-02-14 10:29:38,820   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 10:29:38,854   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 10:29:38,860   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 10:29:38,861   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 10:29:38,885   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-ef920c50-7dc2-4241-9fec-bde44c437eb9
2019-02-14 10:29:38,930   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 10:29:38,953   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 10:29:39,133   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @6083ms
2019-02-14 10:29:39,256   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 10:29:39,274   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @6226ms
2019-02-14 10:29:39,306   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 10:29:39,307   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 10:29:39,334   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,335   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,335   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,340   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,340   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,341   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,342   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,343   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,343   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,344   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,345   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,346   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,346   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,347   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,347   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,348   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,348   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,349   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,349   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,357   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,357   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,358   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,359   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,360   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 10:29:39,362   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 10:29:39,512   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 10:29:39,617   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50942.
2019-02-14 10:29:39,618   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:50942
2019-02-14 10:29:39,627   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 10:29:39,690   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 50942, None)
2019-02-14 10:29:39,706   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:50942 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 50942, None)
2019-02-14 10:29:39,713   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 50942, None)
2019-02-14 10:29:39,713   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 50942, None)
2019-02-14 10:29:39,991   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 10:29:40,694   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at FolderByKeyTest.java:30
2019-02-14 10:29:40,738   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at FolderByKeyTest.java:20)
2019-02-14 10:29:40,742   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at FolderByKeyTest.java:30) with 1 output partitions
2019-02-14 10:29:40,743   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at FolderByKeyTest.java:30)
2019-02-14 10:29:40,743   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-02-14 10:29:40,747   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-02-14 10:29:40,754   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at FolderByKeyTest.java:20), which has no missing parents
2019-02-14 10:29:40,958   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 10:29:40,978   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.9 KB, free 1048.8 MB)
2019-02-14 10:29:41,084   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-14 10:29:41,088   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:50942 (size: 2.3 KB, free: 1048.8 MB)
2019-02-14 10:29:41,090   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:29:41,111   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at FolderByKeyTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:29:41,112   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 10:29:41,320   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7611 bytes)
2019-02-14 10:29:41,352   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 10:29:41,663   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1076 bytes result sent to driver
2019-02-14 10:29:41,673   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 442 ms on localhost (executor driver) (1/1)
2019-02-14 10:29:41,682   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 10:29:41,694   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (parallelizePairs at FolderByKeyTest.java:20) finished in 0.898 s
2019-02-14 10:29:41,695   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-14 10:29:41,695   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-14 10:29:41,696   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-02-14 10:29:41,696   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-14 10:29:41,700   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[1] at foldByKey at FolderByKeyTest.java:28), which has no missing parents
2019-02-14 10:29:41,719   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 1048.8 MB)
2019-02-14 10:29:41,724   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1048.8 MB)
2019-02-14 10:29:41,725   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:50942 (size: 2.6 KB, free: 1048.8 MB)
2019-02-14 10:29:41,726   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:29:41,728   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[1] at foldByKey at FolderByKeyTest.java:28) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:29:41,728   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 10:29:41,733   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-14 10:29:41,733   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 10:29:42,060   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-14 10:29:42,063   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 9 ms
2019-02-14 10:29:42,098   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1433 bytes result sent to driver
2019-02-14 10:29:42,100   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 368 ms on localhost (executor driver) (1/1)
2019-02-14 10:29:42,100   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 10:29:42,101   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at FolderByKeyTest.java:30) finished in 0.384 s
2019-02-14 10:29:42,110   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at FolderByKeyTest.java:30, took 1.415672 s
2019-02-14 10:29:42,119   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 10:29:42,121   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 10:29:42,133   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 10:29:42,177   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 10:29:42,177   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 10:29:42,185   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 10:29:42,188   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 10:29:42,195   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 10:29:42,197   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 10:29:42,198   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-56e07ce9-e15e-4fe7-9e01-db633a60321a
2019-02-14 10:35:56,583   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 10:35:57,179   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 10:35:57,367   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: AggregateTest
2019-02-14 10:35:57,503   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 10:35:57,503   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 10:35:57,504   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 10:35:57,504   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 10:35:57,505   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 10:35:59,368   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 51652.
2019-02-14 10:35:59,409   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 10:35:59,444   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 10:35:59,449   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 10:35:59,450   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 10:35:59,466   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-378872bc-c0f5-479d-af30-1c29873d3b2e
2019-02-14 10:35:59,509   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 10:35:59,531   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 10:35:59,683   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5862ms
2019-02-14 10:35:59,778   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 10:35:59,796   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5976ms
2019-02-14 10:35:59,833   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 10:35:59,833   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 10:35:59,865   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,870   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,871   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,873   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,874   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,874   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,875   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,877   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,879   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,880   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,880   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,881   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,881   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,882   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,882   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,883   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,884   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,884   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,884   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,885   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,895   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,896   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,897   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,897   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,898   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 10:35:59,900   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 10:36:00,072   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 10:36:00,146   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51664.
2019-02-14 10:36:00,146   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:51664
2019-02-14 10:36:00,148   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 10:36:00,193   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 51664, None)
2019-02-14 10:36:00,198   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:51664 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 51664, None)
2019-02-14 10:36:00,203   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 51664, None)
2019-02-14 10:36:00,204   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 51664, None)
2019-02-14 10:36:00,487   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 10:36:01,129   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at sortByKeyTest.java:28
2019-02-14 10:36:01,177   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at sortByKeyTest.java:20)
2019-02-14 10:36:01,180   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at sortByKeyTest.java:28) with 1 output partitions
2019-02-14 10:36:01,181   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at sortByKeyTest.java:28)
2019-02-14 10:36:01,181   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-02-14 10:36:01,184   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-02-14 10:36:01,191   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at sortByKeyTest.java:20), which has no missing parents
2019-02-14 10:36:01,296   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 10:36:01,318   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.2 KB, free 1048.8 MB)
2019-02-14 10:36:01,452   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1048.8 MB)
2019-02-14 10:36:01,459   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:51664 (size: 2.0 KB, free: 1048.8 MB)
2019-02-14 10:36:01,461   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:36:01,486   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at sortByKeyTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:36:01,488   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 10:36:01,595   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7611 bytes)
2019-02-14 10:36:01,623   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 10:36:01,770   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 904 bytes result sent to driver
2019-02-14 10:36:01,782   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 230 ms on localhost (executor driver) (1/1)
2019-02-14 10:36:01,785   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 10:36:01,792   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (parallelizePairs at sortByKeyTest.java:20) finished in 0.572 s
2019-02-14 10:36:01,793   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-14 10:36:01,793   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-14 10:36:01,794   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-02-14 10:36:01,794   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-14 10:36:01,802   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[1] at sortByKey at sortByKeyTest.java:27), which has no missing parents
2019-02-14 10:36:01,844   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 1048.8 MB)
2019-02-14 10:36:01,849   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-14 10:36:01,851   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:51664 (size: 2.3 KB, free: 1048.8 MB)
2019-02-14 10:36:01,851   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:36:01,853   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[1] at sortByKey at sortByKeyTest.java:27) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:36:01,853   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 10:36:01,859   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-14 10:36:01,859   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 10:36:02,176   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-14 10:36:02,179   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 17 ms
2019-02-14 10:36:02,250   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1465 bytes result sent to driver
2019-02-14 10:36:02,252   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 395 ms on localhost (executor driver) (1/1)
2019-02-14 10:36:02,252   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 10:36:02,253   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at sortByKeyTest.java:28) finished in 0.415 s
2019-02-14 10:36:02,260   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at sortByKeyTest.java:28, took 1.130443 s
2019-02-14 10:36:02,276   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 10:36:02,280   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 10:36:02,294   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 10:36:02,355   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 10:36:02,355   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 10:36:02,362   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 10:36:02,365   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 10:36:02,375   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 10:36:02,378   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 10:36:02,379   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-677ca626-ffbf-435a-af1c-55a3898ef436
2019-02-14 10:43:01,536   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 10:43:02,056   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 10:43:02,187   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: AggregateTest
2019-02-14 10:43:02,299   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 10:43:02,299   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 10:43:02,300   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 10:43:02,300   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 10:43:02,301   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 10:43:04,211   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 52456.
2019-02-14 10:43:04,260   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 10:43:04,293   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 10:43:04,297   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 10:43:04,298   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 10:43:04,316   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-f4d3d1b8-6922-4761-be96-adeb9cf1efd7
2019-02-14 10:43:04,355   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 10:43:04,379   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 10:43:04,516   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4937ms
2019-02-14 10:43:04,615   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 10:43:04,635   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5057ms
2019-02-14 10:43:04,673   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 10:43:04,674   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 10:43:04,705   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,709   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,709   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,710   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,710   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,711   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,711   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,712   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,713   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,713   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,714   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,714   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,715   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,715   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,716   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,716   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,717   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,717   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,718   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,718   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,726   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,727   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,728   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,728   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,729   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 10:43:04,731   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 10:43:04,872   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 10:43:04,964   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52468.
2019-02-14 10:43:04,965   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:52468
2019-02-14 10:43:04,967   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 10:43:05,014   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 52468, None)
2019-02-14 10:43:05,019   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:52468 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 52468, None)
2019-02-14 10:43:05,024   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 52468, None)
2019-02-14 10:43:05,025   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 52468, None)
2019-02-14 10:43:05,330   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 10:43:06,023   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at JoinTest.java:30
2019-02-14 10:43:06,062   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at JoinTest.java:20)
2019-02-14 10:43:06,065   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at JoinTest.java:20)
2019-02-14 10:43:06,068   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at JoinTest.java:30) with 1 output partitions
2019-02-14 10:43:06,068   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at JoinTest.java:30)
2019-02-14 10:43:06,069   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
2019-02-14 10:43:06,071   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
2019-02-14 10:43:06,077   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at JoinTest.java:20), which has no missing parents
2019-02-14 10:43:06,160   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 10:43:06,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 1952.0 B, free 1048.8 MB)
2019-02-14 10:43:06,267   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1306.0 B, free 1048.8 MB)
2019-02-14 10:43:06,271   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:52468 (size: 1306.0 B, free: 1048.8 MB)
2019-02-14 10:43:06,273   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:43:06,293   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at JoinTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:43:06,294   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 10:43:06,326   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (ParallelCollectionRDD[0] at parallelizePairs at JoinTest.java:20), which has no missing parents
2019-02-14 10:43:06,329   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 1952.0 B, free 1048.8 MB)
2019-02-14 10:43:06,336   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1302.0 B, free 1048.8 MB)
2019-02-14 10:43:06,337   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:52468 (size: 1302.0 B, free: 1048.8 MB)
2019-02-14 10:43:06,338   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:43:06,339   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (ParallelCollectionRDD[0] at parallelizePairs at JoinTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:43:06,340   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 10:43:06,381   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7513 bytes)
2019-02-14 10:43:06,398   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 10:43:06,486   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 904 bytes result sent to driver
2019-02-14 10:43:06,493   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7513 bytes)
2019-02-14 10:43:06,494   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 10:43:06,497   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 149 ms on localhost (executor driver) (1/1)
2019-02-14 10:43:06,503   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 10:43:06,511   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (parallelizePairs at JoinTest.java:20) finished in 0.400 s
2019-02-14 10:43:06,513   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-14 10:43:06,513   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set(ShuffleMapStage 1)
2019-02-14 10:43:06,514   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-02-14 10:43:06,514   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-14 10:43:06,524   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 861 bytes result sent to driver
2019-02-14 10:43:06,526   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 33 ms on localhost (executor driver) (1/1)
2019-02-14 10:43:06,526   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 10:43:06,528   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (parallelizePairs at JoinTest.java:20) finished in 0.201 s
2019-02-14 10:43:06,528   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-14 10:43:06,529   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-14 10:43:06,529   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-02-14 10:43:06,529   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-14 10:43:06,529   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[3] at join at JoinTest.java:27), which has no missing parents
2019-02-14 10:43:06,533   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.5 KB, free 1048.8 MB)
2019-02-14 10:43:06,540   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1048.8 MB)
2019-02-14 10:43:06,542   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:52468 (size: 2.5 KB, free: 1048.8 MB)
2019-02-14 10:43:06,543   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:43:06,545   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[3] at join at JoinTest.java:27) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:43:06,545   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-02-14 10:43:06,548   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7204 bytes)
2019-02-14 10:43:06,548   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-02-14 10:43:06,837   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-14 10:43:06,840   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 9 ms
2019-02-14 10:43:06,846   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-14 10:43:06,846   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-02-14 10:43:06,892   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1534 bytes result sent to driver
2019-02-14 10:43:06,893   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 347 ms on localhost (executor driver) (1/1)
2019-02-14 10:43:06,894   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-02-14 10:43:06,894   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at JoinTest.java:30) finished in 0.362 s
2019-02-14 10:43:06,899   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at JoinTest.java:30, took 0.875879 s
2019-02-14 10:43:06,910   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 10:43:06,912   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 10:43:06,927   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 10:43:07,003   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 10:43:07,004   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 10:43:07,010   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 10:43:07,013   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 10:43:07,021   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 10:43:07,024   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 10:43:07,024   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-0ab35085-4224-498b-a8fc-a4affa8c5d54
2019-02-14 10:45:19,155   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 10:45:19,652   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 10:45:19,811   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: AggregateTest
2019-02-14 10:45:19,938   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 10:45:19,939   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 10:45:19,940   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 10:45:19,940   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 10:45:19,941   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 10:45:21,875   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 52742.
2019-02-14 10:45:21,925   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 10:45:21,957   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 10:45:21,959   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 10:45:21,960   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 10:45:21,976   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-527d4933-1021-48ba-a4f1-26f0e3b26756
2019-02-14 10:45:22,014   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 10:45:22,036   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 10:45:22,159   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5211ms
2019-02-14 10:45:22,238   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 10:45:22,253   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5307ms
2019-02-14 10:45:22,278   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 10:45:22,279   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 10:45:22,316   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,317   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,317   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,318   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,318   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,319   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,319   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,320   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,321   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,321   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,322   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,322   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,323   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,323   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,324   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,324   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,326   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,326   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,335   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,335   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 10:45:22,339   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 10:45:22,477   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 10:45:22,544   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52753.
2019-02-14 10:45:22,545   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:52753
2019-02-14 10:45:22,548   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 10:45:22,587   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 52753, None)
2019-02-14 10:45:22,592   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:52753 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 52753, None)
2019-02-14 10:45:22,596   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 52753, None)
2019-02-14 10:45:22,597   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 52753, None)
2019-02-14 10:45:22,849   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 10:45:23,466   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at JoinTest.java:34
2019-02-14 10:45:23,495   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at JoinTest.java:20)
2019-02-14 10:45:23,496   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 1 (parallelizePairs at JoinTest.java:28)
2019-02-14 10:45:23,499   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at JoinTest.java:34) with 1 output partitions
2019-02-14 10:45:23,500   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at JoinTest.java:34)
2019-02-14 10:45:23,500   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
2019-02-14 10:45:23,502   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
2019-02-14 10:45:23,508   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at JoinTest.java:20), which has no missing parents
2019-02-14 10:45:23,596   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 10:45:23,611   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 1952.0 B, free 1048.8 MB)
2019-02-14 10:45:23,708   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1306.0 B, free 1048.8 MB)
2019-02-14 10:45:23,711   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:52753 (size: 1306.0 B, free: 1048.8 MB)
2019-02-14 10:45:23,714   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:45:23,733   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at JoinTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:45:23,735   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 10:45:23,771   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (ParallelCollectionRDD[1] at parallelizePairs at JoinTest.java:28), which has no missing parents
2019-02-14 10:45:23,773   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 1952.0 B, free 1048.8 MB)
2019-02-14 10:45:23,788   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1304.0 B, free 1048.8 MB)
2019-02-14 10:45:23,789   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:52753 (size: 1304.0 B, free: 1048.8 MB)
2019-02-14 10:45:23,789   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:45:23,791   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (ParallelCollectionRDD[1] at parallelizePairs at JoinTest.java:28) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:45:23,791   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 10:45:23,833   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7513 bytes)
2019-02-14 10:45:23,843   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 10:45:23,937   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 904 bytes result sent to driver
2019-02-14 10:45:23,946   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7399 bytes)
2019-02-14 10:45:23,947   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 10:45:23,950   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 153 ms on localhost (executor driver) (1/1)
2019-02-14 10:45:23,956   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 10:45:23,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (parallelizePairs at JoinTest.java:20) finished in 0.421 s
2019-02-14 10:45:23,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-14 10:45:23,965   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set(ShuffleMapStage 1)
2019-02-14 10:45:23,966   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-02-14 10:45:23,966   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-14 10:45:23,979   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 861 bytes result sent to driver
2019-02-14 10:45:23,980   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 34 ms on localhost (executor driver) (1/1)
2019-02-14 10:45:23,981   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 10:45:23,981   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (parallelizePairs at JoinTest.java:28) finished in 0.210 s
2019-02-14 10:45:23,982   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-14 10:45:23,982   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-14 10:45:23,982   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-02-14 10:45:23,982   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-14 10:45:23,983   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[4] at join at JoinTest.java:31), which has no missing parents
2019-02-14 10:45:23,988   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.5 KB, free 1048.8 MB)
2019-02-14 10:45:23,991   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1048.8 MB)
2019-02-14 10:45:23,993   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:52753 (size: 2.5 KB, free: 1048.8 MB)
2019-02-14 10:45:23,993   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:45:23,996   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[4] at join at JoinTest.java:31) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:45:23,997   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-02-14 10:45:23,999   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7204 bytes)
2019-02-14 10:45:23,999   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-02-14 10:45:24,278   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-14 10:45:24,279   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 7 ms
2019-02-14 10:45:24,285   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-14 10:45:24,285   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-02-14 10:45:24,331   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1269 bytes result sent to driver
2019-02-14 10:45:24,332   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 335 ms on localhost (executor driver) (1/1)
2019-02-14 10:45:24,333   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-02-14 10:45:24,333   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at JoinTest.java:34) finished in 0.347 s
2019-02-14 10:45:24,339   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at JoinTest.java:34, took 0.873000 s
2019-02-14 10:45:24,349   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 10:45:24,350   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 10:45:24,363   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 10:45:24,441   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 10:45:24,441   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 10:45:24,448   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 10:45:24,451   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 10:45:24,459   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 10:45:24,461   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 10:45:24,466   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-5a455723-6742-4f93-9e41-c780d95608be
2019-02-14 10:55:32,221   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 10:55:32,649   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 10:55:32,811   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: JoinTest
2019-02-14 10:55:32,957   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 10:55:32,958   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 10:55:32,959   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 10:55:32,959   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 10:55:32,960   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 10:55:34,975   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 54067.
2019-02-14 10:55:35,027   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 10:55:35,068   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 10:55:35,071   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 10:55:35,071   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 10:55:35,092   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-1d4de333-1deb-461b-96ed-0e320bc20491
2019-02-14 10:55:35,130   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 10:55:35,152   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 10:55:35,298   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5049ms
2019-02-14 10:55:35,390   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 10:55:35,410   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5163ms
2019-02-14 10:55:35,438   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 10:55:35,439   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 10:55:35,463   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,465   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,469   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,470   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,470   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,471   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,471   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,472   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,473   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,473   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,474   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,474   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,475   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,476   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,476   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,477   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,477   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,478   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,481   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,483   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,495   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,501   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,502   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,504   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,505   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 10:55:35,508   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 10:55:35,928   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 10:55:36,054   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54078.
2019-02-14 10:55:36,055   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:54078
2019-02-14 10:55:36,058   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 10:55:36,110   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 54078, None)
2019-02-14 10:55:36,115   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:54078 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 54078, None)
2019-02-14 10:55:36,119   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 54078, None)
2019-02-14 10:55:36,119   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 54078, None)
2019-02-14 10:55:36,370   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 10:55:37,010   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at JoinTest.java:30
2019-02-14 10:55:37,043   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at JoinTest.java:20)
2019-02-14 10:55:37,046   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 1 (parallelizePairs at JoinTest.java:27)
2019-02-14 10:55:37,048   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at JoinTest.java:30) with 1 output partitions
2019-02-14 10:55:37,049   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at JoinTest.java:30)
2019-02-14 10:55:37,050   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
2019-02-14 10:55:37,052   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
2019-02-14 10:55:37,068   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at JoinTest.java:20), which has no missing parents
2019-02-14 10:55:37,208   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 10:55:37,241   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 1952.0 B, free 1048.8 MB)
2019-02-14 10:55:37,388   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1306.0 B, free 1048.8 MB)
2019-02-14 10:55:37,397   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:54078 (size: 1306.0 B, free: 1048.8 MB)
2019-02-14 10:55:37,404   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:55:37,424   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at JoinTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:55:37,426   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 10:55:37,458   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (ParallelCollectionRDD[1] at parallelizePairs at JoinTest.java:27), which has no missing parents
2019-02-14 10:55:37,462   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 1952.0 B, free 1048.8 MB)
2019-02-14 10:55:37,473   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1304.0 B, free 1048.8 MB)
2019-02-14 10:55:37,475   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:54078 (size: 1304.0 B, free: 1048.8 MB)
2019-02-14 10:55:37,476   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:55:37,478   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (ParallelCollectionRDD[1] at parallelizePairs at JoinTest.java:27) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:55:37,478   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 10:55:37,521   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7513 bytes)
2019-02-14 10:55:37,545   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 10:55:37,733   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 904 bytes result sent to driver
2019-02-14 10:55:37,739   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7399 bytes)
2019-02-14 10:55:37,739   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 10:55:37,742   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 255 ms on localhost (executor driver) (1/1)
2019-02-14 10:55:37,747   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 10:55:37,755   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (parallelizePairs at JoinTest.java:20) finished in 0.645 s
2019-02-14 10:55:37,756   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-14 10:55:37,757   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set(ShuffleMapStage 1)
2019-02-14 10:55:37,757   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-02-14 10:55:37,758   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-14 10:55:37,767   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 818 bytes result sent to driver
2019-02-14 10:55:37,769   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 31 ms on localhost (executor driver) (1/1)
2019-02-14 10:55:37,770   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (parallelizePairs at JoinTest.java:27) finished in 0.310 s
2019-02-14 10:55:37,770   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-14 10:55:37,770   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-14 10:55:37,770   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-02-14 10:55:37,771   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-14 10:55:37,771   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[4] at join at JoinTest.java:29), which has no missing parents
2019-02-14 10:55:37,773   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 10:55:37,779   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.5 KB, free 1048.8 MB)
2019-02-14 10:55:37,788   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1048.8 MB)
2019-02-14 10:55:37,790   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:54078 (size: 2.5 KB, free: 1048.8 MB)
2019-02-14 10:55:37,790   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:55:37,792   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[4] at join at JoinTest.java:29) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:55:37,792   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-02-14 10:55:37,795   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7204 bytes)
2019-02-14 10:55:37,795   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-02-14 10:55:38,098   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-14 10:55:38,100   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 8 ms
2019-02-14 10:55:38,105   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-14 10:55:38,108   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 3 ms
2019-02-14 10:55:38,147   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1312 bytes result sent to driver
2019-02-14 10:55:38,149   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 356 ms on localhost (executor driver) (1/1)
2019-02-14 10:55:38,149   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-02-14 10:55:38,150   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at JoinTest.java:30) finished in 0.375 s
2019-02-14 10:55:38,156   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at JoinTest.java:30, took 1.146296 s
2019-02-14 10:55:38,165   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 10:55:38,167   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 10:55:38,178   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 10:55:38,255   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 10:55:38,256   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 10:55:38,262   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 10:55:38,265   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 10:55:38,273   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 10:55:38,276   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 10:55:38,277   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-01b410fd-b8b6-447c-8918-ade941e8b957
2019-02-14 10:56:55,554   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 10:56:56,016   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 10:56:56,209   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: JoinTest
2019-02-14 10:56:56,406   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 10:56:56,407   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 10:56:56,408   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 10:56:56,409   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 10:56:56,410   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 10:56:59,094   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 10:56:59,793   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 10:56:59,988   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 10:57:00,117   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 10:57:00,118   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 10:57:00,118   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 10:57:00,119   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 10:57:00,119   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 10:57:01,843   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 54251.
2019-02-14 10:57:01,881   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 10:57:01,915   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 10:57:01,919   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 10:57:01,920   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 10:57:01,944   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-674d8371-8b20-4ea8-9a61-529445f38119
2019-02-14 10:57:01,981   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 10:57:02,003   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 10:57:02,137   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5583ms
2019-02-14 10:57:02,259   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 10:57:02,281   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5730ms
2019-02-14 10:57:02,318   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 10:57:02,318   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 10:57:02,344   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,345   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,346   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,347   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,353   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,354   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,355   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,357   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,357   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,358   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,358   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,359   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,359   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,360   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,361   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,361   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,361   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,362   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,363   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,363   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,370   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,373   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,375   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,376   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,376   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 10:57:02,378   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 10:57:02,536   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 10:57:02,609   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54260.
2019-02-14 10:57:02,610   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:54260
2019-02-14 10:57:02,611   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 10:57:02,654   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 54260, None)
2019-02-14 10:57:02,661   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:54260 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 54260, None)
2019-02-14 10:57:02,665   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 54260, None)
2019-02-14 10:57:02,665   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 54260, None)
2019-02-14 10:57:03,003   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 10:57:03,884   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at CoGroupTest.java:31
2019-02-14 10:57:03,947   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at CoGroupTest.java:20)
2019-02-14 10:57:03,949   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 1 (parallelizePairs at CoGroupTest.java:27)
2019-02-14 10:57:03,956   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at CoGroupTest.java:31) with 1 output partitions
2019-02-14 10:57:03,957   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at CoGroupTest.java:31)
2019-02-14 10:57:03,957   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
2019-02-14 10:57:03,960   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
2019-02-14 10:57:03,967   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at CoGroupTest.java:20), which has no missing parents
2019-02-14 10:57:04,079   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 10:57:04,094   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 1952.0 B, free 1048.8 MB)
2019-02-14 10:57:04,233   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1306.0 B, free 1048.8 MB)
2019-02-14 10:57:04,241   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:54260 (size: 1306.0 B, free: 1048.8 MB)
2019-02-14 10:57:04,243   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:57:04,265   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at CoGroupTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:57:04,267   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 10:57:04,301   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (ParallelCollectionRDD[1] at parallelizePairs at CoGroupTest.java:27), which has no missing parents
2019-02-14 10:57:04,307   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 1952.0 B, free 1048.8 MB)
2019-02-14 10:57:04,316   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1304.0 B, free 1048.8 MB)
2019-02-14 10:57:04,317   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:54260 (size: 1304.0 B, free: 1048.8 MB)
2019-02-14 10:57:04,319   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:57:04,324   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (ParallelCollectionRDD[1] at parallelizePairs at CoGroupTest.java:27) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:57:04,325   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 10:57:04,374   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7513 bytes)
2019-02-14 10:57:04,394   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 10:57:04,501   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 904 bytes result sent to driver
2019-02-14 10:57:04,511   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7419 bytes)
2019-02-14 10:57:04,512   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 10:57:04,516   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 184 ms on localhost (executor driver) (1/1)
2019-02-14 10:57:04,528   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 10:57:04,541   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (parallelizePairs at CoGroupTest.java:20) finished in 0.538 s
2019-02-14 10:57:04,542   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-14 10:57:04,542   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set(ShuffleMapStage 1)
2019-02-14 10:57:04,543   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-02-14 10:57:04,544   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-14 10:57:04,558   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 818 bytes result sent to driver
2019-02-14 10:57:04,564   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 53 ms on localhost (executor driver) (1/1)
2019-02-14 10:57:04,566   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (parallelizePairs at CoGroupTest.java:27) finished in 0.263 s
2019-02-14 10:57:04,566   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-14 10:57:04,566   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-14 10:57:04,566   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-02-14 10:57:04,566   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-14 10:57:04,568   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[4] at cogroup at CoGroupTest.java:30), which has no missing parents
2019-02-14 10:57:04,568   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 10:57:04,576   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 1048.8 MB)
2019-02-14 10:57:04,583   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1048.8 MB)
2019-02-14 10:57:04,585   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:54260 (size: 2.6 KB, free: 1048.8 MB)
2019-02-14 10:57:04,586   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-14 10:57:04,591   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[4] at cogroup at CoGroupTest.java:30) (first 15 tasks are for partitions Vector(0))
2019-02-14 10:57:04,592   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-02-14 10:57:04,594   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7204 bytes)
2019-02-14 10:57:04,595   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-02-14 10:57:04,953   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-14 10:57:04,958   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 11 ms
2019-02-14 10:57:04,966   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-14 10:57:04,966   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-02-14 10:57:05,021   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 4078 bytes result sent to driver
2019-02-14 10:57:05,025   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 433 ms on localhost (executor driver) (1/1)
2019-02-14 10:57:05,025   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-02-14 10:57:05,028   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at CoGroupTest.java:31) finished in 0.454 s
2019-02-14 10:57:05,034   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at CoGroupTest.java:31, took 1.150247 s
2019-02-14 10:57:05,048   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 10:57:05,049   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 10:57:05,064   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 10:57:05,154   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 10:57:05,154   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 10:57:05,162   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 10:57:05,165   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 10:57:05,174   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 10:57:05,176   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 10:57:05,177   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-3b8a34ad-dd71-4f88-9d67-a1c122adb8a7
2019-02-14 11:01:26,892   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 11:01:27,307   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 11:01:27,444   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 11:01:27,545   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 11:01:27,546   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 11:01:27,547   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 11:01:27,548   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 11:01:27,549   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 11:01:29,602   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 54825.
2019-02-14 11:01:29,665   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 11:01:29,716   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 11:01:29,723   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 11:01:29,724   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 11:01:30,272   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-b3749810-2926-4eb0-953e-16a2d7a24b13
2019-02-14 11:01:30,326   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 11:01:30,361   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 11:01:30,524   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5771ms
2019-02-14 11:01:30,629   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 11:01:30,648   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5897ms
2019-02-14 11:01:30,677   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:01:30,677   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 11:01:30,707   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,713   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,713   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,715   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,716   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,717   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,717   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,719   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,720   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,720   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,721   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,722   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,723   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,723   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,724   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,724   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,725   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,725   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,726   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,726   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,733   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,734   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,735   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,736   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,737   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 11:01:30,738   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 11:01:30,907   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 11:01:30,989   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54837.
2019-02-14 11:01:30,990   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:54837
2019-02-14 11:01:30,992   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 11:01:31,039   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 54837, None)
2019-02-14 11:01:31,046   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:54837 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 54837, None)
2019-02-14 11:01:31,052   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 54837, None)
2019-02-14 11:01:31,053   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 54837, None)
2019-02-14 11:01:31,362   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 11:01:32,438   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at CoGroupTest.java:32
2019-02-14 11:01:32,486   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 0 (parallelizePairs at CoGroupTest.java:20)
2019-02-14 11:01:32,489   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 1 (parallelizePairs at CoGroupTest.java:27)
2019-02-14 11:01:32,493   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at CoGroupTest.java:32) with 1 output partitions
2019-02-14 11:01:32,494   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at CoGroupTest.java:32)
2019-02-14 11:01:32,497   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
2019-02-14 11:01:32,504   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
2019-02-14 11:01:32,513   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at CoGroupTest.java:20), which has no missing parents
2019-02-14 11:01:32,634   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 11:01:32,650   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 1952.0 B, free 1048.8 MB)
2019-02-14 11:01:32,828   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1306.0 B, free 1048.8 MB)
2019-02-14 11:01:32,834   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:54837 (size: 1306.0 B, free: 1048.8 MB)
2019-02-14 11:01:32,838   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 11:01:32,858   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelizePairs at CoGroupTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 11:01:32,859   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 11:01:32,896   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (ParallelCollectionRDD[1] at parallelizePairs at CoGroupTest.java:27), which has no missing parents
2019-02-14 11:01:32,899   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 1952.0 B, free 1048.8 MB)
2019-02-14 11:01:32,908   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1304.0 B, free 1048.8 MB)
2019-02-14 11:01:32,909   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:54837 (size: 1304.0 B, free: 1048.8 MB)
2019-02-14 11:01:32,910   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 11:01:32,912   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (ParallelCollectionRDD[1] at parallelizePairs at CoGroupTest.java:27) (first 15 tasks are for partitions Vector(0))
2019-02-14 11:01:32,912   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 11:01:32,957   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7513 bytes)
2019-02-14 11:01:32,974   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 11:01:33,095   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 904 bytes result sent to driver
2019-02-14 11:01:33,100   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7452 bytes)
2019-02-14 11:01:33,101   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 11:01:33,106   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 183 ms on localhost (executor driver) (1/1)
2019-02-14 11:01:33,111   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 11:01:33,123   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (parallelizePairs at CoGroupTest.java:20) finished in 0.573 s
2019-02-14 11:01:33,124   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-14 11:01:33,125   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set(ShuffleMapStage 1)
2019-02-14 11:01:33,125   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-02-14 11:01:33,126   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-14 11:01:33,144   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 818 bytes result sent to driver
2019-02-14 11:01:33,146   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 46 ms on localhost (executor driver) (1/1)
2019-02-14 11:01:33,147   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (parallelizePairs at CoGroupTest.java:27) finished in 0.250 s
2019-02-14 11:01:33,147   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-14 11:01:33,148   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-14 11:01:33,148   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-02-14 11:01:33,148   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-14 11:01:33,148   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[4] at cogroup at CoGroupTest.java:31), which has no missing parents
2019-02-14 11:01:33,150   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 11:01:33,153   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 1048.8 MB)
2019-02-14 11:01:33,160   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1048.8 MB)
2019-02-14 11:01:33,163   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:54837 (size: 2.6 KB, free: 1048.8 MB)
2019-02-14 11:01:33,163   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-14 11:01:33,165   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[4] at cogroup at CoGroupTest.java:31) (first 15 tasks are for partitions Vector(0))
2019-02-14 11:01:33,165   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-02-14 11:01:33,169   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7204 bytes)
2019-02-14 11:01:33,169   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-02-14 11:01:33,498   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-14 11:01:33,499   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 9 ms
2019-02-14 11:01:33,506   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-14 11:01:33,506   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 1 ms
2019-02-14 11:01:33,580   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 4174 bytes result sent to driver
2019-02-14 11:01:33,584   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 418 ms on localhost (executor driver) (1/1)
2019-02-14 11:01:33,584   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-02-14 11:01:33,585   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at CoGroupTest.java:32) finished in 0.433 s
2019-02-14 11:01:33,592   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at CoGroupTest.java:32, took 1.152846 s
2019-02-14 11:01:33,603   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:01:33,605   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 11:01:33,618   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 11:01:33,714   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 11:01:33,715   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 11:01:33,724   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 11:01:33,728   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 11:01:33,739   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 11:01:33,742   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 11:01:33,742   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-f870ae40-09a0-4226-9825-b57c947c0ee2
2019-02-14 11:12:31,113   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 11:12:31,519   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 11:12:31,648   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 11:12:31,757   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 11:12:31,758   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 11:12:31,758   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 11:12:31,759   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 11:12:31,759   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 11:12:33,453   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 56118.
2019-02-14 11:12:33,486   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 11:12:33,521   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 11:12:33,525   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 11:12:33,526   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 11:12:33,546   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-ee3a8bed-0816-4c26-9d35-32feaaac275c
2019-02-14 11:12:33,584   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 11:12:33,607   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 11:12:33,742   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4472ms
2019-02-14 11:12:33,826   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 11:12:33,842   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4574ms
2019-02-14 11:12:33,867   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:12:33,867   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 11:12:33,892   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,896   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,897   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,897   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,898   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,898   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,899   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,900   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,900   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,902   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,902   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,904   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,905   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,907   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,907   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,908   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,909   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,909   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,909   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,917   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,918   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,920   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,920   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,921   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 11:12:33,923   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 11:12:34,072   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 11:12:34,137   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56129.
2019-02-14 11:12:34,138   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:56129
2019-02-14 11:12:34,140   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 11:12:34,177   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 56129, None)
2019-02-14 11:12:34,180   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:56129 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 56129, None)
2019-02-14 11:12:34,185   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 56129, None)
2019-02-14 11:12:34,185   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 56129, None)
2019-02-14 11:12:34,426   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 11:12:34,966   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at CartesianTest.java:33
2019-02-14 11:12:35,016   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at CartesianTest.java:33) with 1 output partitions
2019-02-14 11:12:35,017   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at CartesianTest.java:33)
2019-02-14 11:12:35,017   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 11:12:35,019   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 11:12:35,026   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (CartesianRDD[2] at cartesian at CartesianTest.java:32), which has no missing parents
2019-02-14 11:12:35,131   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 11:12:35,144   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.5 KB, free 1048.8 MB)
2019-02-14 11:12:35,257   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1501.0 B, free 1048.8 MB)
2019-02-14 11:12:35,260   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:56129 (size: 1501.0 B, free: 1048.8 MB)
2019-02-14 11:12:35,264   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 11:12:35,282   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (CartesianRDD[2] at cartesian at CartesianTest.java:32) (first 15 tasks are for partitions Vector(0))
2019-02-14 11:12:35,283   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 11:12:35,366   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7751 bytes)
2019-02-14 11:12:35,382   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 11:12:35,634   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1269 bytes result sent to driver
2019-02-14 11:12:35,642   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 312 ms on localhost (executor driver) (1/1)
2019-02-14 11:12:35,646   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 11:12:35,651   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at CartesianTest.java:33) finished in 0.597 s
2019-02-14 11:12:35,659   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at CartesianTest.java:33, took 0.691715 s
2019-02-14 11:12:35,674   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:12:35,675   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 11:12:35,688   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 11:12:35,707   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 11:12:35,707   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 11:12:35,716   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 11:12:35,720   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 11:12:35,729   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 11:12:35,733   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 11:12:35,734   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-fa542980-6550-412e-9cda-9a90950b05aa
2019-02-14 11:15:38,001   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 11:15:38,440   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 11:15:38,611   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 11:15:38,740   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 11:15:38,741   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 11:15:38,741   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 11:15:38,742   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 11:15:38,742   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 11:15:40,986   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 56474.
2019-02-14 11:15:41,020   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 11:15:41,049   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 11:15:41,052   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 11:15:41,053   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 11:15:41,070   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-20d5f7ae-f9c6-4343-b8ab-40a322a26c07
2019-02-14 11:15:41,108   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 11:15:41,131   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 11:15:41,275   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5379ms
2019-02-14 11:15:41,354   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 11:15:41,373   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5479ms
2019-02-14 11:15:41,400   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@18c5d94{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:15:41,400   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 11:15:41,426   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1ef51f2{/jobs,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,430   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@527389{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,431   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5a3466{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,431   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,432   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/stages,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,432   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,434   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,436   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,436   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,437   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,438   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/storage,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,438   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,438   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,439   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,440   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/environment,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,440   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,441   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/executors,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,441   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,442   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,442   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,449   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/static,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,450   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c10d84{/,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,451   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15fe372{/api,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,451   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,452   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6ef90e{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 11:15:41,453   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 11:15:41,588   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 11:15:41,650   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56483.
2019-02-14 11:15:41,650   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:56483
2019-02-14 11:15:41,652   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 11:15:41,694   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 56483, None)
2019-02-14 11:15:41,699   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:56483 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 56483, None)
2019-02-14 11:15:41,703   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 56483, None)
2019-02-14 11:15:41,704   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 56483, None)
2019-02-14 11:15:41,990   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c612bd{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 11:15:42,499   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at CartesianTest.java:26
2019-02-14 11:15:42,534   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at CartesianTest.java:26) with 1 output partitions
2019-02-14 11:15:42,534   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at CartesianTest.java:26)
2019-02-14 11:15:42,535   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 11:15:42,536   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 11:15:42,542   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (CartesianRDD[2] at cartesian at CartesianTest.java:24), which has no missing parents
2019-02-14 11:15:42,631   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 11:15:42,645   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.8 KB, free 1048.8 MB)
2019-02-14 11:15:42,743   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1671.0 B, free 1048.8 MB)
2019-02-14 11:15:42,748   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:56483 (size: 1671.0 B, free: 1048.8 MB)
2019-02-14 11:15:42,751   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 11:15:42,770   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (CartesianRDD[2] at cartesian at CartesianTest.java:24) (first 15 tasks are for partitions Vector(0))
2019-02-14 11:15:42,771   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 11:15:42,898   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7561 bytes)
2019-02-14 11:15:42,914   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 11:15:43,223   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1056 bytes result sent to driver
2019-02-14 11:15:43,232   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 404 ms on localhost (executor driver) (1/1)
2019-02-14 11:15:43,238   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 11:15:43,243   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at CartesianTest.java:26) finished in 0.674 s
2019-02-14 11:15:43,253   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at CartesianTest.java:26, took 0.753760 s
2019-02-14 11:15:43,265   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@18c5d94{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:15:43,267   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 11:15:43,280   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 11:15:43,301   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 11:15:43,301   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 11:15:43,307   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 11:15:43,311   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 11:15:43,319   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 11:15:43,323   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 11:15:43,324   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-afa13c24-6295-4b90-a936-d9eafba4854e
2019-02-14 11:28:02,487   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 11:28:02,930   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 11:28:03,069   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 11:28:03,199   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 11:28:03,201   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 11:28:03,201   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 11:28:03,202   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 11:28:03,202   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 11:28:04,876   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 57999.
2019-02-14 11:28:04,913   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 11:28:04,942   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 11:28:04,945   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 11:28:04,946   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 11:28:04,962   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-037a0b77-7edc-491c-b641-edfe1edda310
2019-02-14 11:28:05,012   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 11:28:05,032   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 11:28:05,150   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4861ms
2019-02-14 11:28:05,228   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 11:28:05,245   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4956ms
2019-02-14 11:28:05,275   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:28:05,275   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 11:28:05,304   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,304   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,306   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,306   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,306   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,307   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,309   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,310   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,310   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,311   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,311   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,312   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,312   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,313   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,314   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,314   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,314   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,315   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,315   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,322   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,323   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,326   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,326   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 11:28:05,328   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 11:28:05,465   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 11:28:05,528   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58009.
2019-02-14 11:28:05,528   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:58009
2019-02-14 11:28:05,530   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 11:28:05,568   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 58009, None)
2019-02-14 11:28:05,574   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:58009 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 58009, None)
2019-02-14 11:28:05,578   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 58009, None)
2019-02-14 11:28:05,579   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 58009, None)
2019-02-14 11:28:05,823   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 11:28:06,373   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at PipeTest.java:22
2019-02-14 11:28:06,406   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at PipeTest.java:22) with 1 output partitions
2019-02-14 11:28:06,407   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at PipeTest.java:22)
2019-02-14 11:28:06,408   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 11:28:06,409   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 11:28:06,416   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (PipedRDD[1] at pipe at PipeTest.java:21), which has no missing parents
2019-02-14 11:28:06,510   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 11:28:06,522   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.1 KB, free 1048.8 MB)
2019-02-14 11:28:06,619   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1814.0 B, free 1048.8 MB)
2019-02-14 11:28:06,623   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:58009 (size: 1814.0 B, free: 1048.8 MB)
2019-02-14 11:28:06,627   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 11:28:06,645   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (PipedRDD[1] at pipe at PipeTest.java:21) (first 15 tasks are for partitions Vector(0))
2019-02-14 11:28:06,647   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 11:28:06,709   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7370 bytes)
2019-02-14 11:28:06,722   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 11:28:06,988  ERROR --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:91) : Exception in task 0.0 in stage 0.0 (TID 0)
java.io.IOException: Cannot run program "pipe.sh": CreateProcess error=193, %1  Win32 
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.rdd.PipedRDD.compute(PipedRDD.scala:111)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: CreateProcess error=193, %1  Win32 
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:386)
	at java.lang.ProcessImpl.start(ProcessImpl.java:137)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 11 more
2019-02-14 11:28:07,025   WARN --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:66) : Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.io.IOException: Cannot run program "pipe.sh": CreateProcess error=193, %1  Win32 
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.rdd.PipedRDD.compute(PipedRDD.scala:111)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: CreateProcess error=193, %1  Win32 
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:386)
	at java.lang.ProcessImpl.start(ProcessImpl.java:137)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 11 more

2019-02-14 11:28:07,028  ERROR --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:70) : Task 0 in stage 0.0 failed 1 times; aborting job
2019-02-14 11:28:07,030   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 11:28:07,036   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Cancelling stage 0
2019-02-14 11:28:07,037   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Killing all running tasks in stage 0: Stage cancelled
2019-02-14 11:28:07,040   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at PipeTest.java:22) failed in 0.595 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.io.IOException: Cannot run program "pipe.sh": CreateProcess error=193, %1  Win32 
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
	at org.apache.spark.rdd.PipedRDD.compute(PipedRDD.scala:111)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: CreateProcess error=193, %1  Win32 
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(ProcessImpl.java:386)
	at java.lang.ProcessImpl.start(ProcessImpl.java:137)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
	... 11 more

Driver stacktrace:
2019-02-14 11:28:07,045   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 failed: collect at PipeTest.java:22, took 0.672804 s
2019-02-14 11:28:07,055   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-14 11:28:07,063   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:28:07,064   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 11:28:07,078   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 11:28:07,098   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 11:28:07,099   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 11:28:07,105   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 11:28:07,107   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 11:28:07,115   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 11:28:07,115   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 11:28:07,116   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-fcbee124-f02c-4f00-8945-d94e52b58310
2019-02-14 11:29:45,397   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 11:29:45,836   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 11:29:45,988   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 11:29:46,121   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 11:29:46,122   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 11:29:46,122   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 11:29:46,123   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 11:29:46,123   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 11:29:47,925   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 58284.
2019-02-14 11:29:47,966   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 11:29:48,003   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 11:29:48,007   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 11:29:48,007   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 11:29:48,024   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-782586c4-1ccc-4088-8121-fa84a82b9b33
2019-02-14 11:29:48,064   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 11:29:48,089   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 11:29:48,227   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4610ms
2019-02-14 11:29:48,309   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 11:29:48,328   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4712ms
2019-02-14 11:29:48,355   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@8ba540{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:29:48,356   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 11:29:48,386   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@8c1f44{/jobs,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,387   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,388   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,389   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,390   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/stages,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,390   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,391   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,392   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,392   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,393   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,393   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/storage,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34ace1{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fe6067{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,395   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1071df8{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,395   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e41af9{/environment,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,396   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ace400{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,396   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1404bea{/executors,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,396   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5f7627{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,397   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1018107{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,397   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@bcf243{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,406   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e6eb25{/static,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,407   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1ff4fae{/,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,408   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1aa6d6c{/api,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,409   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@227100{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,409   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15894c8{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 11:29:48,410   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 11:29:48,597   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:29:48,662   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to h131/192.168.102.131:7077 after 42 ms (0 ms spent in bootstraps)
2019-02-14 11:30:08,597   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:30:28,599   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:30:48,601  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-02-14 11:30:48,602   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-02-14 11:30:48,632   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58420.
2019-02-14 11:30:48,633   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:58420
2019-02-14 11:30:48,635   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@8ba540{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:30:48,639   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 11:30:48,643   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 11:30:48,654   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-02-14 11:30:48,664   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-02-14 11:30:48,669   WARN --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-02-14 11:30:48,680   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 11:30:48,695   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 11:30:48,696   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 11:30:48,702   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 58420, None)
2019-02-14 11:30:48,709   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 11:30:48,710   WARN --- [stop-spark-context]  org.apache.spark.metrics.MetricsSystem(line:66) : Stopping a MetricsSystem that is not running
2019-02-14 11:30:48,711   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:58420 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 58420, None)
2019-02-14 11:30:48,713   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 11:30:48,717   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 58420, None)
2019-02-14 11:30:48,718   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 58420, None)
2019-02-14 11:30:48,725   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 11:30:48,978   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d93e0d{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 11:30:48,988  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.NullPointerException
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:560)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at PipeTest.main(PipeTest.java:18)
2019-02-14 11:30:48,991   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-02-14 11:30:48,996   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 11:30:48,997   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-bffdbc5b-387c-49af-b03f-46fe77608f06
2019-02-14 11:30:48,998  ERROR --- [spark-listener-group-appStatus]  org.apache.spark.scheduler.AsyncEventQueue(line:91) : Listener AppStatusListener threw an exception
java.lang.NullPointerException
	at org.apache.spark.status.AppStatusListener.onApplicationEnd(AppStatusListener.scala:157)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:57)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:91)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:76)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:92)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:83)
2019-02-14 11:38:43,855   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 11:38:44,313   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 11:38:44,492   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 11:38:44,624   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 11:38:44,625   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 11:38:44,625   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 11:38:44,625   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 11:38:44,626   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 11:38:46,173   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 59653.
2019-02-14 11:38:46,212   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 11:38:46,252   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 11:38:46,257   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 11:38:46,257   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 11:38:46,281   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-7d7ab158-a30c-4a17-be5b-b15ddef7b753
2019-02-14 11:38:46,334   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 11:38:46,359   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 11:38:46,491   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4512ms
2019-02-14 11:38:46,575   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 11:38:46,595   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4618ms
2019-02-14 11:38:46,626   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:38:46,626   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 11:38:46,654   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e33a58{/jobs,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,655   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,655   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,656   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,656   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/stages,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,657   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,657   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,658   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1071df8{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,659   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e41af9{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,659   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ace400{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,659   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1404bea{/storage,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,660   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5f7627{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,661   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1018107{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,661   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@bcf243{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,662   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e6eb25{/environment,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,663   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@435e70{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,663   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@19bf286{/executors,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,663   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@be305c{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,664   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@9f4bad{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,665   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d731d0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,673   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34f74e{/static,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,674   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15894c8{/,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,675   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@33046b{/api,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,675   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@164c4b7{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,676   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@163c727{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 11:38:46,677   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 11:38:46,698   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Added JAR C:\Users\10160\eclipse-workspace\spark-vlearn\target\spark-learn-1.0-SNAPSHOT-jar-with-dependencies.jar at spark://DESKTOP-Q1PPPMM:59653/jars/spark-learn-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 1550115526697
2019-02-14 11:38:46,865   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:38:46,940   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to h131/192.168.102.131:7077 after 50 ms (0 ms spent in bootstraps)
2019-02-14 11:39:06,868   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:39:26,868   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:39:46,870  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-02-14 11:39:46,870   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-02-14 11:39:46,888   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:39:46,889   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 11:39:46,894   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59786.
2019-02-14 11:39:46,894   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:59786
2019-02-14 11:39:46,899   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 11:39:46,901   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-02-14 11:39:46,906   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-02-14 11:39:46,916   WARN --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-02-14 11:39:46,932   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 11:39:46,957   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 11:39:46,958   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 11:39:46,981   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 11:39:46,981   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 59786, None)
2019-02-14 11:39:46,983   WARN --- [stop-spark-context]  org.apache.spark.metrics.MetricsSystem(line:66) : Stopping a MetricsSystem that is not running
2019-02-14 11:39:46,987  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.NullPointerException
	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:64)
	at org.apache.spark.storage.BlockManager.initialize(BlockManager.scala:252)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:510)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at PipeTest.main(PipeTest.java:21)
2019-02-14 11:39:46,989   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 11:39:46,990   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-02-14 11:39:46,994   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 11:39:46,997   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-1caf313c-62aa-4e20-baca-b4dab0fba072
2019-02-14 11:39:47,004   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 11:39:47,006   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-1caf313c-62aa-4e20-baca-b4dab0fba072\userFiles-c6b3d587-bd5a-4811-91ef-b939fc5ae772
2019-02-14 11:41:37,541   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 11:41:38,155   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 11:41:38,313   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 11:41:38,445   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 11:41:38,446   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 11:41:38,447   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 11:41:38,448   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 11:41:38,448   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 11:41:40,606   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 60037.
2019-02-14 11:41:40,690   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 11:41:40,742   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 11:41:40,745   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 11:41:40,748   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 11:41:40,806   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-7a3b2126-6144-4ecc-95a0-d86bbfe3947a
2019-02-14 11:41:40,903   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 11:41:40,971   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 11:41:41,223   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @6525ms
2019-02-14 11:41:41,332   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 11:41:41,347   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @6651ms
2019-02-14 11:41:41,374   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@94f09b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:41:41,374   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 11:41:41,408   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62c01e{/jobs,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,408   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,409   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,410   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,411   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/stages,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,412   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,413   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,414   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34ace1{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,415   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fe6067{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,416   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1071df8{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,417   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e41af9{/storage,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,419   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ace400{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,420   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1404bea{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,421   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5f7627{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,422   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1018107{/environment,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,422   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@bcf243{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,424   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e6eb25{/executors,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,425   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@435e70{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,425   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@19bf286{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,426   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@be305c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,434   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@9f4bad{/static,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,435   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@11b9a3{/,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,437   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@227100{/api,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,438   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29b776{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,438   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@178826f{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 11:41:41,440   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.103.206:4040
2019-02-14 11:41:41,468   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Added JAR C:\Users\10160\eclipse-workspace\spark-vlearn\target\spark-learn-1.0-SNAPSHOT-jar-with-dependencies.jar at spark://192.168.103.206:60037/jars/spark-learn-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 1550115701467
2019-02-14 11:41:41,771   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:41:41,874   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to h131/192.168.102.131:7077 after 68 ms (0 ms spent in bootstraps)
2019-02-14 11:42:01,769   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:42:21,770   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:42:41,772  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-02-14 11:42:41,773   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-02-14 11:42:41,821   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60169.
2019-02-14 11:42:41,822   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on 192.168.103.206:60169
2019-02-14 11:42:41,831   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 11:42:41,834   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@94f09b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:42:41,836   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://192.168.103.206:4040
2019-02-14 11:42:41,857   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-02-14 11:42:41,872   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-02-14 11:42:41,884   WARN --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-02-14 11:42:41,902   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 11:42:41,930   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 11:42:41,930   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 11:42:41,945   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, 192.168.103.206, 60169, None)
2019-02-14 11:42:41,951   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 11:42:41,952   WARN --- [stop-spark-context]  org.apache.spark.metrics.MetricsSystem(line:66) : Stopping a MetricsSystem that is not running
2019-02-14 11:42:41,953   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.103.206:60169 with 1048.8 MB RAM, BlockManagerId(driver, 192.168.103.206, 60169, None)
2019-02-14 11:42:41,959   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 11:42:41,960   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, 192.168.103.206, 60169, None)
2019-02-14 11:42:41,961   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, 192.168.103.206, 60169, None)
2019-02-14 11:42:41,976   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 11:42:42,302   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@177b641{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 11:42:42,316  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.NullPointerException
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:560)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at PipeTest.main(PipeTest.java:22)
2019-02-14 11:42:42,322   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-02-14 11:42:42,330   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 11:42:42,332   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-d13bb465-bcd5-408e-b6fe-4de0d5b67e52
2019-02-14 11:42:42,334  ERROR --- [spark-listener-group-appStatus]  org.apache.spark.scheduler.AsyncEventQueue(line:91) : Listener AppStatusListener threw an exception
java.lang.NullPointerException
	at org.apache.spark.status.AppStatusListener.onApplicationEnd(AppStatusListener.scala:157)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:57)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:91)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:76)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:92)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:83)
2019-02-14 11:43:53,758   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 11:43:54,461   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 11:43:54,646   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 11:43:54,799   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 11:43:54,799   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 11:43:54,800   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 11:43:54,801   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 11:43:54,801   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 11:43:56,584   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 60331.
2019-02-14 11:43:56,627   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 11:43:56,669   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 11:43:56,676   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 11:43:56,676   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 11:43:56,704   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-7bcacc2a-26d7-4581-8fbb-d37f0df09f31
2019-02-14 11:43:56,757   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 11:43:56,786   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 11:43:56,942   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5634ms
2019-02-14 11:43:57,040   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 11:43:57,062   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5756ms
2019-02-14 11:43:57,104   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1605f2f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:43:57,105   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 11:43:57,143   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@10ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,144   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,145   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,146   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,147   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/stages,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,148   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,148   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,149   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,150   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,150   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,151   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34ace1{/storage,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,151   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,152   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,153   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,157   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ace400{/environment,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,158   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1404bea{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,159   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5f7627{/executors,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,161   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1018107{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,161   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@bcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,162   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,171   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@435e70{/static,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,171   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1aa6d6c{/,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,172   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1968a76{/api,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,173   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,173   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@33046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 11:43:57,175   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 11:43:57,197   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Added JAR C:\Users\10160\eclipse-workspace\spark-vlearn\target\spark-learn-1.0-SNAPSHOT-jar-with-dependencies.jar at spark://DESKTOP-Q1PPPMM:60331/jars/spark-learn-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 1550115837196
2019-02-14 11:43:57,404   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:43:57,485   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to h131/192.168.102.131:7077 after 55 ms (0 ms spent in bootstraps)
2019-02-14 11:44:17,407   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:44:37,410   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:44:57,414  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-02-14 11:44:57,414   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-02-14 11:44:57,425   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1605f2f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:44:57,430   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 11:44:57,437   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60535.
2019-02-14 11:44:57,438   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:60535
2019-02-14 11:44:57,441   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 11:44:57,446   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-02-14 11:44:57,454   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-02-14 11:44:57,467   WARN --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-02-14 11:44:57,482   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 11:44:57,503   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 11:44:57,504   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 11:44:57,522   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 60535, None)
2019-02-14 11:44:57,523   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 11:44:57,524   WARN --- [stop-spark-context]  org.apache.spark.metrics.MetricsSystem(line:66) : Stopping a MetricsSystem that is not running
2019-02-14 11:44:57,525  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
	at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:64)
	at org.apache.spark.storage.BlockManager.initialize(BlockManager.scala:252)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:510)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at PipeTest.main(PipeTest.java:25)
Caused by: org.apache.spark.SparkException: Could not find BlockManagerMaster.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:135)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:229)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:523)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:91)
	... 6 more
2019-02-14 11:44:57,537   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-02-14 11:44:57,538   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 11:44:57,541   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 11:44:57,543   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-40482d15-a6ca-4312-8913-16664f7c6568\userFiles-0f94bb54-6174-4c27-8191-99da901a8060
2019-02-14 11:44:57,548   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 11:44:57,549   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-40482d15-a6ca-4312-8913-16664f7c6568
2019-02-14 11:48:26,997   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 11:48:27,472   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 11:48:27,634   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 11:48:27,772   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 11:48:27,772   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 11:48:27,773   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 11:48:27,773   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 11:48:27,774   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 11:48:29,485   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 61129.
2019-02-14 11:48:29,528   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 11:48:29,560  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.IllegalArgumentException: System memory 259522560 must be at least 471859200. Please increase heap size using the --driver-memory option or spark.driver.memory in Spark configuration.
	at org.apache.spark.memory.UnifiedMemoryManager$.getMaxMemory(UnifiedMemoryManager.scala:219)
	at org.apache.spark.memory.UnifiedMemoryManager$.apply(UnifiedMemoryManager.scala:199)
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:330)
	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:185)
	at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:257)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:424)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at PipeTest.main(PipeTest.java:21)
2019-02-14 11:48:29,587   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 11:48:44,361   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 11:48:44,828   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 11:48:44,962   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 11:48:45,081   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 11:48:45,082   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 11:48:45,083   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 11:48:45,083   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 11:48:45,084   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 11:48:46,595   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 61185.
2019-02-14 11:48:46,632   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 11:48:46,679   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 11:48:46,682   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 11:48:46,683   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 11:48:46,702   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-24324fde-f650-44e0-9aa1-351c58622b20
2019-02-14 11:48:46,746   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 11:48:46,773   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 11:48:46,918   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4421ms
2019-02-14 11:48:47,011   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 11:48:47,029   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4535ms
2019-02-14 11:48:47,068   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:48:47,068   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 11:48:47,101   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e33a58{/jobs,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,101   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,102   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,103   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,103   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/stages,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,103   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,104   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,105   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1071df8{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,105   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e41af9{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,106   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ace400{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,107   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1404bea{/storage,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,108   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5f7627{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1018107{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@bcf243{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,110   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e6eb25{/environment,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,110   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@435e70{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,111   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@19bf286{/executors,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,111   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@be305c{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,112   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@9f4bad{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,112   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d731d0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,119   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34f74e{/static,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,120   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15894c8{/,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,121   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@33046b{/api,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,122   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@164c4b7{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,123   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@163c727{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 11:48:47,125   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.103.206:4040
2019-02-14 11:48:47,146   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Added JAR C:\Users\10160\eclipse-workspace\spark-vlearn\target\spark-learn-1.0-SNAPSHOT-jar-with-dependencies.jar at spark://192.168.103.206:61185/jars/spark-learn-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 1550116127145
2019-02-14 11:48:47,331   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:48:47,401   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to h131/192.168.102.131:7077 after 47 ms (0 ms spent in bootstraps)
2019-02-14 11:49:07,332   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:49:24,580   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 11:49:25,019   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 11:49:25,158   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 11:49:25,286   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 11:49:25,286   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 11:49:25,287   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 11:49:25,287   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 11:49:25,288   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 11:49:26,874   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 61291.
2019-02-14 11:49:26,913   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 11:49:26,945   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 11:49:26,948   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 11:49:26,949   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 11:49:26,968   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-5d80c1eb-3efa-4dfa-8399-97354d365fa7
2019-02-14 11:49:27,013   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 11:49:27,036   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 11:49:27,164   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4764ms
2019-02-14 11:49:27,249   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 11:49:27,267   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4868ms
2019-02-14 11:49:27,290   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:49:27,291   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 11:49:27,318   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e33a58{/jobs,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,318   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,319   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,319   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,320   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/stages,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,320   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,321   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,322   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1071df8{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,322   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e41af9{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,323   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ace400{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,324   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1404bea{/storage,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5f7627{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,326   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1018107{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,327   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@bcf243{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,328   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e6eb25{/environment,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,329   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@435e70{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,330   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@19bf286{/executors,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,331   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@be305c{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,331   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@9f4bad{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,332   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d731d0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,346   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34f74e{/static,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,348   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15894c8{/,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,349   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@33046b{/api,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,350   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@164c4b7{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,351   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@163c727{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 11:49:27,353   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.103.206:4040
2019-02-14 11:49:27,382   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Added JAR C:\Users\10160\eclipse-workspace\spark-vlearn\target\spark-learn-1.0-SNAPSHOT-jar-with-dependencies.jar at spark://192.168.103.206:61291/jars/spark-learn-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 1550116167381
2019-02-14 11:49:27,537   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:49:27,608   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to h131/192.168.102.131:7077 after 40 ms (0 ms spent in bootstraps)
2019-02-14 11:49:47,538   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:50:07,538   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://h131:7077...
2019-02-14 11:50:27,540  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-02-14 11:50:27,540   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-02-14 11:50:27,552   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 11:50:27,556   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://192.168.103.206:4040
2019-02-14 11:50:27,558   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61443.
2019-02-14 11:50:27,558   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on 192.168.103.206:61443
2019-02-14 11:50:27,561   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 11:50:27,567   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-02-14 11:50:27,572   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-02-14 11:50:27,581   WARN --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-02-14 11:50:27,591   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 11:50:27,610   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 11:50:27,611   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 11:50:27,627   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, 192.168.103.206, 61443, None)
2019-02-14 11:50:27,633   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 11:50:27,634   WARN --- [stop-spark-context]  org.apache.spark.metrics.MetricsSystem(line:66) : Stopping a MetricsSystem that is not running
2019-02-14 11:50:27,637   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.103.206:61443 with 1048.8 MB RAM, BlockManagerId(driver, 192.168.103.206, 61443, None)
2019-02-14 11:50:27,639   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 11:50:27,644   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, 192.168.103.206, 61443, None)
2019-02-14 11:50:27,645   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, 192.168.103.206, 61443, None)
2019-02-14 11:50:27,656   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 11:50:28,047   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@153ddfc{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 11:50:28,061  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.NullPointerException
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:560)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at PipeTest.main(PipeTest.java:21)
2019-02-14 11:50:28,066   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-02-14 11:50:28,072   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 11:50:28,076   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-f0673157-d900-42b9-bd78-d02b7ae5dc3f
2019-02-14 11:50:28,077  ERROR --- [spark-listener-group-appStatus]  org.apache.spark.scheduler.AsyncEventQueue(line:91) : Listener AppStatusListener threw an exception
java.lang.NullPointerException
	at org.apache.spark.status.AppStatusListener.onApplicationEnd(AppStatusListener.scala:157)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:57)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:91)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:76)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:92)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:83)
2019-02-14 13:47:10,624   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 13:47:11,080   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 13:47:11,236   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 13:47:11,370   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 13:47:11,370   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 13:47:11,371   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 13:47:11,371   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 13:47:11,372   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 13:47:12,994   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 9638.
2019-02-14 13:47:13,039   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 13:47:13,077   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 13:47:13,082   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 13:47:13,083   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 13:47:13,106   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-6421676e-e641-48e3-a2c6-2df371c56465
2019-02-14 13:47:13,166   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 13:47:13,193   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 13:47:13,357   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4724ms
2019-02-14 13:47:13,476   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 13:47:13,498   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4865ms
2019-02-14 13:47:13,544   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@18c5d94{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 13:47:13,544   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 13:47:13,583   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1ef51f2{/jobs,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,584   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@527389{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,584   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5a3466{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,585   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,586   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/stages,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,586   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,587   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,589   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,591   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,591   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/storage,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,593   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,594   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,595   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/environment,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,595   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,596   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/executors,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,597   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,597   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,598   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,609   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/static,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,612   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c10d84{/,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,613   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15fe372{/api,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,614   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,615   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6ef90e{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 13:47:13,616   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 13:47:13,834   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 13:47:13,945   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9647.
2019-02-14 13:47:13,946   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:9647
2019-02-14 13:47:13,949   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 13:47:14,010   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 9647, None)
2019-02-14 13:47:14,017   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:9647 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 9647, None)
2019-02-14 13:47:14,022   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 9647, None)
2019-02-14 13:47:14,023   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 9647, None)
2019-02-14 13:47:14,328   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c612bd{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 13:47:14,896   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at MapValuesTest.java:32
2019-02-14 13:47:14,934   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at MapValuesTest.java:32) with 1 output partitions
2019-02-14 13:47:14,934   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at MapValuesTest.java:32)
2019-02-14 13:47:14,935   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 13:47:14,936   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 13:47:14,941   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[2] at mapValues at MapValuesTest.java:31), which has no missing parents
2019-02-14 13:47:15,032   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 13:47:15,043   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 1048.8 MB)
2019-02-14 13:47:15,144   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1892.0 B, free 1048.8 MB)
2019-02-14 13:47:15,148   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:9647 (size: 1892.0 B, free: 1048.8 MB)
2019-02-14 13:47:15,150   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 13:47:15,169   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at mapValues at MapValuesTest.java:31) (first 15 tasks are for partitions Vector(0))
2019-02-14 13:47:15,170   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 13:47:15,256   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7524 bytes)
2019-02-14 13:47:15,271   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 13:47:15,591   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 883 bytes result sent to driver
2019-02-14 13:47:15,598   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 377 ms on localhost (executor driver) (1/1)
2019-02-14 13:47:15,602   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 13:47:15,609   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at MapValuesTest.java:32) finished in 0.636 s
2019-02-14 13:47:15,617   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at MapValuesTest.java:32, took 0.721098 s
2019-02-14 13:47:15,629   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@18c5d94{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 13:47:15,631   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 13:47:15,647   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 13:47:15,666   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 13:47:15,666   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 13:47:15,674   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 13:47:15,677   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 13:47:15,684   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 13:47:15,686   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 13:47:15,687   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-1491a49e-11d9-40c1-95c8-d337bda312b1
2019-02-14 13:59:49,747   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 13:59:50,240   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 13:59:50,396   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 13:59:50,503   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 13:59:50,504   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 13:59:50,504   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 13:59:50,505   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 13:59:50,505   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 13:59:52,027   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 11005.
2019-02-14 13:59:52,072   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 13:59:52,100   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 13:59:52,103   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 13:59:52,103   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 13:59:52,119   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-88707378-8bc0-40e5-afe3-1833fc96a04a
2019-02-14 13:59:52,172   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 13:59:52,193   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 13:59:52,315   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4339ms
2019-02-14 13:59:52,391   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 13:59:52,407   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4432ms
2019-02-14 13:59:52,430   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 13:59:52,430   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 13:59:52,458   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,459   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,459   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,460   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,461   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,461   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,462   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,462   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,463   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,464   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,465   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,467   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,469   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,471   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,471   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,472   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,473   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,473   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,474   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,474   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,480   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,481   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,483   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,484   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,484   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 13:59:52,486   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 13:59:52,627   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 13:59:52,688   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11019.
2019-02-14 13:59:52,688   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:11019
2019-02-14 13:59:52,690   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 13:59:52,727   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 11019, None)
2019-02-14 13:59:52,731   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:11019 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 11019, None)
2019-02-14 13:59:52,736   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 11019, None)
2019-02-14 13:59:52,736   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 11019, None)
2019-02-14 13:59:52,977   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 13:59:53,484   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: reduce at ReduceTest.java:27
2019-02-14 13:59:53,520   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (reduce at ReduceTest.java:27) with 1 output partitions
2019-02-14 13:59:53,521   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (reduce at ReduceTest.java:27)
2019-02-14 13:59:53,521   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 13:59:53,523   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 13:59:53,532   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelizePairs at ReduceTest.java:19), which has no missing parents
2019-02-14 13:59:53,628   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 13:59:53,642   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.4 KB, free 1048.8 MB)
2019-02-14 13:59:53,754   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1455.0 B, free 1048.8 MB)
2019-02-14 13:59:53,757   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:11019 (size: 1455.0 B, free: 1048.8 MB)
2019-02-14 13:59:53,760   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 13:59:53,782   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelizePairs at ReduceTest.java:19) (first 15 tasks are for partitions Vector(0))
2019-02-14 13:59:53,784   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 13:59:53,865   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7622 bytes)
2019-02-14 13:59:53,884   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 13:59:54,140   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 895 bytes result sent to driver
2019-02-14 13:59:54,147   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 315 ms on localhost (executor driver) (1/1)
2019-02-14 13:59:54,150   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 13:59:54,159   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (reduce at ReduceTest.java:27) finished in 0.597 s
2019-02-14 13:59:54,165   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: reduce at ReduceTest.java:27, took 0.679606 s
2019-02-14 13:59:54,170   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-14 13:59:54,179   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 13:59:54,180   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 13:59:54,191   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 13:59:54,207   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 13:59:54,207   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 13:59:54,214   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 13:59:54,216   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 13:59:54,223   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 13:59:54,224   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 13:59:54,225   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-125f08d4-4cad-49a6-8ecb-ea059ce9aa52
2019-02-14 14:09:22,592   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 14:09:22,997   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 14:09:23,164   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 14:09:23,289   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 14:09:23,290   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 14:09:23,290   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 14:09:23,291   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 14:09:23,291   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 14:09:24,729   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 12056.
2019-02-14 14:09:24,762   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 14:09:24,791   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 14:09:24,795   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 14:09:24,796   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 14:09:24,811   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-5b185a70-ab62-4e78-a78b-b41a42144695
2019-02-14 14:09:24,848   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 14:09:24,872   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 14:09:24,993   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4343ms
2019-02-14 14:09:25,067   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 14:09:25,083   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4434ms
2019-02-14 14:09:25,110   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:09:25,111   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 14:09:25,136   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,137   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,138   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,138   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,139   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,139   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,140   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,141   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,141   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,142   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,142   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,143   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,143   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,144   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,144   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,145   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,145   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,146   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,146   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,146   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,153   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,153   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,154   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,155   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,155   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 14:09:25,158   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:09:25,305   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 14:09:25,373   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 12065.
2019-02-14 14:09:25,373   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:12065
2019-02-14 14:09:25,376   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 14:09:25,426   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 12065, None)
2019-02-14 14:09:25,430   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:12065 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 12065, None)
2019-02-14 14:09:25,434   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 12065, None)
2019-02-14 14:09:25,435   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 12065, None)
2019-02-14 14:09:25,670   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:26,239   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at SimpleTest.java:22
2019-02-14 14:09:26,271   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at SimpleTest.java:22) with 1 output partitions
2019-02-14 14:09:26,271   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at SimpleTest.java:22)
2019-02-14 14:09:26,272   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:09:26,273   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:09:26,280   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20), which has no missing parents
2019-02-14 14:09:26,369   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 14:09:26,382   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.2 KB, free 1048.8 MB)
2019-02-14 14:09:26,481   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1342.0 B, free 1048.8 MB)
2019-02-14 14:09:26,485   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:12065 (size: 1342.0 B, free: 1048.8 MB)
2019-02-14 14:09:26,486   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:09:26,507   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:09:26,508   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 14:09:26,591   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:09:26,611   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 14:09:26,883   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 881 bytes result sent to driver
2019-02-14 14:09:26,891   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 336 ms on localhost (executor driver) (1/1)
2019-02-14 14:09:26,894   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 14:09:26,901   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at SimpleTest.java:22) finished in 0.597 s
2019-02-14 14:09:26,906   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at SimpleTest.java:22, took 0.667458 s
2019-02-14 14:09:26,924   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: count at SimpleTest.java:23
2019-02-14 14:09:26,925   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (count at SimpleTest.java:23) with 1 output partitions
2019-02-14 14:09:26,925   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (count at SimpleTest.java:23)
2019-02-14 14:09:26,925   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:09:26,925   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:09:26,926   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20), which has no missing parents
2019-02-14 14:09:26,928   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 2.1 KB, free 1048.8 MB)
2019-02-14 14:09:26,935   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1341.0 B, free 1048.8 MB)
2019-02-14 14:09:26,936   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:12065 (size: 1341.0 B, free: 1048.8 MB)
2019-02-14 14:09:26,937   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:09:26,938   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:09:26,938   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 14:09:26,939   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:09:26,940   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 14:09:26,945   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 666 bytes result sent to driver
2019-02-14 14:09:26,946   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 7 ms on localhost (executor driver) (1/1)
2019-02-14 14:09:26,947   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 14:09:26,947   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (count at SimpleTest.java:23) finished in 0.020 s
2019-02-14 14:09:26,948   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: count at SimpleTest.java:23, took 0.023292 s
2019-02-14 14:09:26,960   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: first at SimpleTest.java:24
2019-02-14 14:09:26,961   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 2 (first at SimpleTest.java:24) with 1 output partitions
2019-02-14 14:09:26,961   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (first at SimpleTest.java:24)
2019-02-14 14:09:26,961   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:09:26,961   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:09:26,962   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-02-14 14:09:26,962   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20), which has no missing parents
2019-02-14 14:09:26,962   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-02-14 14:09:26,962   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-02-14 14:09:26,962   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-02-14 14:09:26,962   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-02-14 14:09:26,963   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-02-14 14:09:26,963   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-02-14 14:09:26,963   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-02-14 14:09:26,963   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-02-14 14:09:26,963   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-02-14 14:09:26,963   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-02-14 14:09:26,963   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-02-14 14:09:26,963   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-02-14 14:09:26,963   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-02-14 14:09:26,963   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-02-14 14:09:26,964   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-02-14 14:09:26,966   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-02-14 14:09:26,966   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-14 14:09:26,966   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-02-14 14:09:26,967   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-02-14 14:09:26,967   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-02-14 14:09:26,967   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-02-14 14:09:26,967   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-02-14 14:09:26,967   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-02-14 14:09:26,967   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-02-14 14:09:26,967   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-02-14 14:09:26,971   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 1400.0 B, free 1048.8 MB)
2019-02-14 14:09:26,978   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:12065 (size: 1400.0 B, free: 1048.8 MB)
2019-02-14 14:09:26,979   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:09:26,980   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:09:26,980   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-02-14 14:09:26,982   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:09:26,983   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-02-14 14:09:26,991   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on DESKTOP-Q1PPPMM:12065 in memory (size: 1341.0 B, free: 1048.8 MB)
2019-02-14 14:09:26,993   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 662 bytes result sent to driver
2019-02-14 14:09:26,997   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 14 ms on localhost (executor driver) (1/1)
2019-02-14 14:09:26,998   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-02-14 14:09:26,999   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (first at SimpleTest.java:24) finished in 0.036 s
2019-02-14 14:09:27,000   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 2 finished: first at SimpleTest.java:24, took 0.039955 s
2019-02-14 14:09:27,007   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 65
2019-02-14 14:09:27,008   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 55
2019-02-14 14:09:27,008   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 68
2019-02-14 14:09:27,008   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 71
2019-02-14 14:09:27,008   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 50
2019-02-14 14:09:27,014   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: take at SimpleTest.java:25
2019-02-14 14:09:27,015   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on DESKTOP-Q1PPPMM:12065 in memory (size: 1400.0 B, free: 1048.8 MB)
2019-02-14 14:09:27,016   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 3 (take at SimpleTest.java:25) with 1 output partitions
2019-02-14 14:09:27,016   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 3 (take at SimpleTest.java:25)
2019-02-14 14:09:27,016   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:09:27,016   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:09:27,017   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 3 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20), which has no missing parents
2019-02-14 14:09:27,017   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 58
2019-02-14 14:09:27,018   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 52
2019-02-14 14:09:27,018   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 59
2019-02-14 14:09:27,018   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 62
2019-02-14 14:09:27,018   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 61
2019-02-14 14:09:27,018   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 51
2019-02-14 14:09:27,018   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 63
2019-02-14 14:09:27,018   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 69
2019-02-14 14:09:27,019   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 67
2019-02-14 14:09:27,019   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 56
2019-02-14 14:09:27,019   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 66
2019-02-14 14:09:27,019   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 57
2019-02-14 14:09:27,019   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 72
2019-02-14 14:09:27,019   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-14 14:09:27,019   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 53
2019-02-14 14:09:27,019   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 60
2019-02-14 14:09:27,019   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 74
2019-02-14 14:09:27,020   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 54
2019-02-14 14:09:27,020   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 73
2019-02-14 14:09:27,020   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 64
2019-02-14 14:09:27,020   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 70
2019-02-14 14:09:27,022   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 1400.0 B, free 1048.8 MB)
2019-02-14 14:09:27,023   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on DESKTOP-Q1PPPMM:12065 (size: 1400.0 B, free: 1048.8 MB)
2019-02-14 14:09:27,024   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:09:27,025   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 3 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:09:27,025   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 1 tasks
2019-02-14 14:09:27,026   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:09:27,026   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 3)
2019-02-14 14:09:27,030   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 3). 662 bytes result sent to driver
2019-02-14 14:09:27,031   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 3) in 5 ms on localhost (executor driver) (1/1)
2019-02-14 14:09:27,031   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-02-14 14:09:27,033   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 3 (take at SimpleTest.java:25) finished in 0.015 s
2019-02-14 14:09:27,033   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 3 finished: take at SimpleTest.java:25, took 0.018577 s
2019-02-14 14:09:27,038   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 94
2019-02-14 14:09:27,038   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 77
2019-02-14 14:09:27,038   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 81
2019-02-14 14:09:27,040   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_3_piece0 on DESKTOP-Q1PPPMM:12065 in memory (size: 1400.0 B, free: 1048.8 MB)
2019-02-14 14:09:27,042   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 86
2019-02-14 14:09:27,042   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 84
2019-02-14 14:09:27,042   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 97
2019-02-14 14:09:27,042   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 76
2019-02-14 14:09:27,043   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 82
2019-02-14 14:09:27,043   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 91
2019-02-14 14:09:27,043   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 89
2019-02-14 14:09:27,043   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 92
2019-02-14 14:09:27,043   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 96
2019-02-14 14:09:27,043   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 87
2019-02-14 14:09:27,043   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 75
2019-02-14 14:09:27,043   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 79
2019-02-14 14:09:27,044   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 83
2019-02-14 14:09:27,044   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 80
2019-02-14 14:09:27,044   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 78
2019-02-14 14:09:27,044   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 85
2019-02-14 14:09:27,044   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 93
2019-02-14 14:09:27,044   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: take at SimpleTest.java:26
2019-02-14 14:09:27,044   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 88
2019-02-14 14:09:27,044   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 90
2019-02-14 14:09:27,044   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 99
2019-02-14 14:09:27,045   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 95
2019-02-14 14:09:27,045   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 98
2019-02-14 14:09:27,046   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 4 (take at SimpleTest.java:26) with 1 output partitions
2019-02-14 14:09:27,046   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 4 (take at SimpleTest.java:26)
2019-02-14 14:09:27,046   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:09:27,047   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:09:27,047   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 4 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20), which has no missing parents
2019-02-14 14:09:27,050   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-14 14:09:27,052   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 1401.0 B, free 1048.8 MB)
2019-02-14 14:09:27,054   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on DESKTOP-Q1PPPMM:12065 (size: 1401.0 B, free: 1048.8 MB)
2019-02-14 14:09:27,055   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:09:27,056   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 4 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:09:27,057   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 4.0 with 1 tasks
2019-02-14 14:09:27,059   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:09:27,059   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 4.0 (TID 4)
2019-02-14 14:09:27,062   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 4.0 (TID 4). 702 bytes result sent to driver
2019-02-14 14:09:27,063   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 4.0 (TID 4) in 5 ms on localhost (executor driver) (1/1)
2019-02-14 14:09:27,063   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2019-02-14 14:09:27,064   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 4 (take at SimpleTest.java:26) finished in 0.016 s
2019-02-14 14:09:27,064   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 4 finished: take at SimpleTest.java:26, took 0.019935 s
2019-02-14 14:09:27,068   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 114
2019-02-14 14:09:27,068   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 105
2019-02-14 14:09:27,068   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 124
2019-02-14 14:09:27,068   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 122
2019-02-14 14:09:27,068   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 101
2019-02-14 14:09:27,068   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 102
2019-02-14 14:09:27,068   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 121
2019-02-14 14:09:27,068   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 120
2019-02-14 14:09:27,069   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 100
2019-02-14 14:09:27,069   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 116
2019-02-14 14:09:27,069   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 117
2019-02-14 14:09:27,069   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 115
2019-02-14 14:09:27,069   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 108
2019-02-14 14:09:27,069   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 119
2019-02-14 14:09:27,069   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 107
2019-02-14 14:09:27,069   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 104
2019-02-14 14:09:27,070   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 112
2019-02-14 14:09:27,070   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 111
2019-02-14 14:09:27,070   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 106
2019-02-14 14:09:27,070   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 123
2019-02-14 14:09:27,070   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 110
2019-02-14 14:09:27,070   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 103
2019-02-14 14:09:27,070   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 109
2019-02-14 14:09:27,072   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_4_piece0 on DESKTOP-Q1PPPMM:12065 in memory (size: 1401.0 B, free: 1048.8 MB)
2019-02-14 14:09:27,074   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 118
2019-02-14 14:09:27,075   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 113
2019-02-14 14:09:27,097   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: takeOrdered at SimpleTest.java:27
2019-02-14 14:09:27,100   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 5 (takeOrdered at SimpleTest.java:27) with 1 output partitions
2019-02-14 14:09:27,100   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 5 (takeOrdered at SimpleTest.java:27)
2019-02-14 14:09:27,100   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:09:27,100   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:09:27,100   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 5 (MapPartitionsRDD[1] at takeOrdered at SimpleTest.java:27), which has no missing parents
2019-02-14 14:09:27,108   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5 stored as values in memory (estimated size 3.8 KB, free 1048.8 MB)
2019-02-14 14:09:27,114   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1048.8 MB)
2019-02-14 14:09:27,117   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_5_piece0 in memory on DESKTOP-Q1PPPMM:12065 (size: 2.1 KB, free: 1048.8 MB)
2019-02-14 14:09:27,118   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:09:27,119   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[1] at takeOrdered at SimpleTest.java:27) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:09:27,119   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 5.0 with 1 tasks
2019-02-14 14:09:27,121   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:09:27,121   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 5.0 (TID 5)
2019-02-14 14:09:27,134   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 5.0 (TID 5). 1275 bytes result sent to driver
2019-02-14 14:09:27,135   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 5.0 (TID 5) in 15 ms on localhost (executor driver) (1/1)
2019-02-14 14:09:27,136   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2019-02-14 14:09:27,137   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 5 (takeOrdered at SimpleTest.java:27) finished in 0.034 s
2019-02-14 14:09:27,137   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 5 finished: takeOrdered at SimpleTest.java:27, took 0.039771 s
2019-02-14 14:09:27,144   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:09:27,149   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:09:27,161   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 14:09:27,218   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 14:09:27,219   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 14:09:27,221   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 14:09:27,223   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 14:09:27,229   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 14:09:27,232   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 14:09:27,232   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-0dd78539-5f08-4b43-b29d-09cc990ad150
2019-02-14 14:09:42,240   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 14:09:42,872   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 14:09:43,069   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 14:09:43,240   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 14:09:43,241   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 14:09:43,242   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 14:09:43,243   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 14:09:43,244   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 14:09:45,096   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 12121.
2019-02-14 14:09:45,141   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 14:09:45,185   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 14:09:45,190   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 14:09:45,190   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 14:09:45,220   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-a13c3ce0-0abb-4219-9b78-6045ca3bae61
2019-02-14 14:09:45,290   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 14:09:45,361   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 14:09:45,586   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5215ms
2019-02-14 14:09:45,675   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 14:09:45,696   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5326ms
2019-02-14 14:09:45,721   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:09:45,721   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 14:09:45,754   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,755   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,756   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,756   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,757   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,757   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,758   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,758   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,759   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,760   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,760   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,760   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,763   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,764   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,765   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,766   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,768   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,769   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,770   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,783   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,785   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,787   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,788   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,789   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 14:09:45,791   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:09:45,991   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 14:09:46,116   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 12135.
2019-02-14 14:09:46,117   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:12135
2019-02-14 14:09:46,120   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 14:09:46,196   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 12135, None)
2019-02-14 14:09:46,205   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:12135 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 12135, None)
2019-02-14 14:09:46,212   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 12135, None)
2019-02-14 14:09:46,212   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 12135, None)
2019-02-14 14:09:46,520   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 14:09:47,157   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at SimpleTest.java:22
2019-02-14 14:09:47,190   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at SimpleTest.java:22) with 1 output partitions
2019-02-14 14:09:47,191   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at SimpleTest.java:22)
2019-02-14 14:09:47,191   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:09:47,195   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:09:47,207   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20), which has no missing parents
2019-02-14 14:09:47,312   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 14:09:47,329   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.2 KB, free 1048.8 MB)
2019-02-14 14:09:47,458   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1342.0 B, free 1048.8 MB)
2019-02-14 14:09:47,462   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:12135 (size: 1342.0 B, free: 1048.8 MB)
2019-02-14 14:09:47,466   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:09:47,490   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:09:47,492   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 14:09:47,573   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:09:47,590   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 14:09:47,943   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 881 bytes result sent to driver
2019-02-14 14:09:47,959   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 419 ms on localhost (executor driver) (1/1)
2019-02-14 14:09:47,962   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 14:09:47,976   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at SimpleTest.java:22) finished in 0.735 s
2019-02-14 14:09:47,992   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at SimpleTest.java:22, took 0.833665 s
2019-02-14 14:09:48,009   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: count at SimpleTest.java:23
2019-02-14 14:09:48,010   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (count at SimpleTest.java:23) with 1 output partitions
2019-02-14 14:09:48,011   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (count at SimpleTest.java:23)
2019-02-14 14:09:48,012   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:09:48,012   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:09:48,013   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20), which has no missing parents
2019-02-14 14:09:48,016   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 2.1 KB, free 1048.8 MB)
2019-02-14 14:09:48,023   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1341.0 B, free 1048.8 MB)
2019-02-14 14:09:48,024   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:12135 (size: 1341.0 B, free: 1048.8 MB)
2019-02-14 14:09:48,025   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:09:48,026   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:09:48,026   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 14:09:48,027   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:09:48,027   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 14:09:48,034   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 623 bytes result sent to driver
2019-02-14 14:09:48,037   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 10 ms on localhost (executor driver) (1/1)
2019-02-14 14:09:48,038   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 14:09:48,043   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (count at SimpleTest.java:23) finished in 0.027 s
2019-02-14 14:09:48,045   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: count at SimpleTest.java:23, took 0.034983 s
2019-02-14 14:09:48,059   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: first at SimpleTest.java:24
2019-02-14 14:09:48,060   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 2 (first at SimpleTest.java:24) with 1 output partitions
2019-02-14 14:09:48,060   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (first at SimpleTest.java:24)
2019-02-14 14:09:48,061   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:09:48,061   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:09:48,061   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20), which has no missing parents
2019-02-14 14:09:48,061   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-02-14 14:09:48,061   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-02-14 14:09:48,062   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-02-14 14:09:48,062   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-02-14 14:09:48,062   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-02-14 14:09:48,062   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-02-14 14:09:48,062   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-02-14 14:09:48,062   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-02-14 14:09:48,063   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-14 14:09:48,067   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 1400.0 B, free 1048.8 MB)
2019-02-14 14:09:48,074   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:12135 (size: 1400.0 B, free: 1048.8 MB)
2019-02-14 14:09:48,075   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:09:48,077   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:09:48,077   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-02-14 14:09:48,079   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:09:48,079   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-02-14 14:09:48,085   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on DESKTOP-Q1PPPMM:12135 in memory (size: 1341.0 B, free: 1048.8 MB)
2019-02-14 14:09:48,088   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-02-14 14:09:48,088   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 662 bytes result sent to driver
2019-02-14 14:09:48,088   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-02-14 14:09:48,088   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-02-14 14:09:48,088   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-02-14 14:09:48,088   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-02-14 14:09:48,088   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-02-14 14:09:48,089   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-02-14 14:09:48,089   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-02-14 14:09:48,089   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-02-14 14:09:48,089   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-02-14 14:09:48,089   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-02-14 14:09:48,090   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-02-14 14:09:48,090   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 12 ms on localhost (executor driver) (1/1)
2019-02-14 14:09:48,090   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-02-14 14:09:48,092   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-02-14 14:09:48,092   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-02-14 14:09:48,092   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-02-14 14:09:48,092   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-02-14 14:09:48,093   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-02-14 14:09:48,093   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (first at SimpleTest.java:24) finished in 0.031 s
2019-02-14 14:09:48,093   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 2 finished: first at SimpleTest.java:24, took 0.033628 s
2019-02-14 14:09:48,099   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 58
2019-02-14 14:09:48,099   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 71
2019-02-14 14:09:48,100   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 53
2019-02-14 14:09:48,100   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 70
2019-02-14 14:09:48,100   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 62
2019-02-14 14:09:48,100   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 68
2019-02-14 14:09:48,100   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 64
2019-02-14 14:09:48,100   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 65
2019-02-14 14:09:48,100   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 57
2019-02-14 14:09:48,100   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 61
2019-02-14 14:09:48,101   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 63
2019-02-14 14:09:48,101   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 59
2019-02-14 14:09:48,101   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 52
2019-02-14 14:09:48,101   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 69
2019-02-14 14:09:48,101   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 67
2019-02-14 14:09:48,101   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 72
2019-02-14 14:09:48,101   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 54
2019-02-14 14:09:48,101   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 60
2019-02-14 14:09:48,101   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 50
2019-02-14 14:09:48,101   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 56
2019-02-14 14:09:48,101   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 66
2019-02-14 14:09:48,102   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 55
2019-02-14 14:09:48,102   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 73
2019-02-14 14:09:48,102   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 51
2019-02-14 14:09:48,103   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on DESKTOP-Q1PPPMM:12135 in memory (size: 1400.0 B, free: 1048.8 MB)
2019-02-14 14:09:48,106   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: take at SimpleTest.java:25
2019-02-14 14:09:48,107   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 74
2019-02-14 14:09:48,107   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 3 (take at SimpleTest.java:25) with 1 output partitions
2019-02-14 14:09:48,107   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 3 (take at SimpleTest.java:25)
2019-02-14 14:09:48,107   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:09:48,108   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:09:48,108   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 3 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20), which has no missing parents
2019-02-14 14:09:48,110   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-14 14:09:48,113   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 1400.0 B, free 1048.8 MB)
2019-02-14 14:09:48,114   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on DESKTOP-Q1PPPMM:12135 (size: 1400.0 B, free: 1048.8 MB)
2019-02-14 14:09:48,114   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:09:48,115   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 3 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:09:48,116   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 1 tasks
2019-02-14 14:09:48,117   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:09:48,117   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 3)
2019-02-14 14:09:48,121   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 3). 705 bytes result sent to driver
2019-02-14 14:09:48,123   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 3) in 7 ms on localhost (executor driver) (1/1)
2019-02-14 14:09:48,123   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-02-14 14:09:48,123   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 3 (take at SimpleTest.java:25) finished in 0.014 s
2019-02-14 14:09:48,124   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 3 finished: take at SimpleTest.java:25, took 0.017396 s
2019-02-14 14:09:48,127   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 89
2019-02-14 14:09:48,127   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 83
2019-02-14 14:09:48,127   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 96
2019-02-14 14:09:48,129   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_3_piece0 on DESKTOP-Q1PPPMM:12135 in memory (size: 1400.0 B, free: 1048.8 MB)
2019-02-14 14:09:48,131   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 90
2019-02-14 14:09:48,131   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 77
2019-02-14 14:09:48,131   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 82
2019-02-14 14:09:48,131   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 80
2019-02-14 14:09:48,131   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 91
2019-02-14 14:09:48,131   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 85
2019-02-14 14:09:48,131   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 92
2019-02-14 14:09:48,131   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 76
2019-02-14 14:09:48,132   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 93
2019-02-14 14:09:48,132   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 88
2019-02-14 14:09:48,132   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 97
2019-02-14 14:09:48,132   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 98
2019-02-14 14:09:48,132   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 99
2019-02-14 14:09:48,132   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 79
2019-02-14 14:09:48,132   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 87
2019-02-14 14:09:48,132   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 84
2019-02-14 14:09:48,132   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 75
2019-02-14 14:09:48,132   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 78
2019-02-14 14:09:48,132   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 86
2019-02-14 14:09:48,132   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 81
2019-02-14 14:09:48,133   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 94
2019-02-14 14:09:48,133   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 95
2019-02-14 14:09:48,134   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: take at SimpleTest.java:26
2019-02-14 14:09:48,134   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 4 (take at SimpleTest.java:26) with 1 output partitions
2019-02-14 14:09:48,134   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 4 (take at SimpleTest.java:26)
2019-02-14 14:09:48,135   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:09:48,136   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:09:48,137   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 4 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20), which has no missing parents
2019-02-14 14:09:48,139   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-14 14:09:48,156   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 1401.0 B, free 1048.8 MB)
2019-02-14 14:09:48,159   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on DESKTOP-Q1PPPMM:12135 (size: 1401.0 B, free: 1048.8 MB)
2019-02-14 14:09:48,160   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:09:48,160   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 4 (ParallelCollectionRDD[0] at parallelize at SimpleTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:09:48,160   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 4.0 with 1 tasks
2019-02-14 14:09:48,162   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:09:48,162   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 4.0 (TID 4)
2019-02-14 14:09:48,168   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 4.0 (TID 4). 785 bytes result sent to driver
2019-02-14 14:09:48,169   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 4.0 (TID 4) in 8 ms on localhost (executor driver) (1/1)
2019-02-14 14:09:48,169   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2019-02-14 14:09:48,170   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 4 (take at SimpleTest.java:26) finished in 0.033 s
2019-02-14 14:09:48,170   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 4 finished: take at SimpleTest.java:26, took 0.036667 s
2019-02-14 14:09:48,195   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 103
2019-02-14 14:09:48,195   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 110
2019-02-14 14:09:48,195   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 112
2019-02-14 14:09:48,196   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 121
2019-02-14 14:09:48,196   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 106
2019-02-14 14:09:48,196   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 122
2019-02-14 14:09:48,196   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 101
2019-02-14 14:09:48,196   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 113
2019-02-14 14:09:48,196   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 115
2019-02-14 14:09:48,196   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 111
2019-02-14 14:09:48,196   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 119
2019-02-14 14:09:48,196   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 100
2019-02-14 14:09:48,197   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 116
2019-02-14 14:09:48,197   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 104
2019-02-14 14:09:48,197   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 105
2019-02-14 14:09:48,197   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 120
2019-02-14 14:09:48,197   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 114
2019-02-14 14:09:48,197   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 124
2019-02-14 14:09:48,197   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 123
2019-02-14 14:09:48,197   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 109
2019-02-14 14:09:48,197   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 107
2019-02-14 14:09:48,198   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 118
2019-02-14 14:09:48,198   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 108
2019-02-14 14:09:48,200   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_4_piece0 on DESKTOP-Q1PPPMM:12135 in memory (size: 1401.0 B, free: 1048.8 MB)
2019-02-14 14:09:48,203   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 102
2019-02-14 14:09:48,204   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 117
2019-02-14 14:09:48,205   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: takeOrdered at SimpleTest.java:27
2019-02-14 14:09:48,207   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 5 (takeOrdered at SimpleTest.java:27) with 1 output partitions
2019-02-14 14:09:48,207   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 5 (takeOrdered at SimpleTest.java:27)
2019-02-14 14:09:48,208   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:09:48,208   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:09:48,209   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 5 (MapPartitionsRDD[1] at takeOrdered at SimpleTest.java:27), which has no missing parents
2019-02-14 14:09:48,219   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5 stored as values in memory (estimated size 3.8 KB, free 1048.8 MB)
2019-02-14 14:09:48,224   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1048.8 MB)
2019-02-14 14:09:48,226   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_5_piece0 in memory on DESKTOP-Q1PPPMM:12135 (size: 2.1 KB, free: 1048.8 MB)
2019-02-14 14:09:48,227   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:09:48,228   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[1] at takeOrdered at SimpleTest.java:27) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:09:48,228   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 5.0 with 1 tasks
2019-02-14 14:09:48,230   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:09:48,230   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 5.0 (TID 5)
2019-02-14 14:09:48,243   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 5.0 (TID 5). 1275 bytes result sent to driver
2019-02-14 14:09:48,244   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 5.0 (TID 5) in 15 ms on localhost (executor driver) (1/1)
2019-02-14 14:09:48,245   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2019-02-14 14:09:48,246   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 5 (takeOrdered at SimpleTest.java:27) finished in 0.034 s
2019-02-14 14:09:48,247   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 5 finished: takeOrdered at SimpleTest.java:27, took 0.041547 s
2019-02-14 14:09:48,257   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:09:48,259   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:09:48,271   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 14:09:48,336   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 14:09:48,336   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 14:09:48,338   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 14:09:48,341   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 14:09:48,347   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 14:09:48,350   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 14:09:48,350   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-a6f693b6-83e7-4e59-9a4f-656b604c34e9
2019-02-14 14:15:12,284   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 14:15:12,764   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 14:15:12,901   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 14:15:13,008   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 14:15:13,009   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 14:15:13,009   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 14:15:13,010   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 14:15:13,010   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 14:15:14,474   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 12764.
2019-02-14 14:15:14,521   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 14:15:14,550   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 14:15:14,553   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 14:15:14,553   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 14:15:14,569   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-2e34fda1-c7e8-498e-ac64-1088071daba4
2019-02-14 14:15:14,608   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 14:15:14,627   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 14:15:14,745   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4531ms
2019-02-14 14:15:14,822   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 14:15:14,837   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4624ms
2019-02-14 14:15:14,869   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@18c5d94{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:15:14,869   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 14:15:14,897   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1ef51f2{/jobs,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,903   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@527389{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,904   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5a3466{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,905   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,906   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/stages,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,906   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,907   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,908   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,908   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,909   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,909   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/storage,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,910   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,911   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,912   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,913   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/environment,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,913   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,914   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/executors,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,914   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,914   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,915   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,922   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/static,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,922   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c10d84{/,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,923   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15fe372{/api,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,924   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,924   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6ef90e{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 14:15:14,926   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:15:15,061   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 14:15:15,122   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 12773.
2019-02-14 14:15:15,122   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:12773
2019-02-14 14:15:15,124   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 14:15:15,164   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 12773, None)
2019-02-14 14:15:15,167   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:12773 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 12773, None)
2019-02-14 14:15:15,172   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 12773, None)
2019-02-14 14:15:15,172   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 12773, None)
2019-02-14 14:15:15,415   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c612bd{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 14:15:15,912   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: takeSample at TakeSampleTest.java:20
2019-02-14 14:15:15,945   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (takeSample at TakeSampleTest.java:20) with 1 output partitions
2019-02-14 14:15:15,946   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (takeSample at TakeSampleTest.java:20)
2019-02-14 14:15:15,946   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:15:15,947   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:15:15,955   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19), which has no missing parents
2019-02-14 14:15:16,049   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 14:15:16,061   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 1048.8 MB)
2019-02-14 14:15:16,155   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1341.0 B, free 1048.8 MB)
2019-02-14 14:15:16,160   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:12773 (size: 1341.0 B, free: 1048.8 MB)
2019-02-14 14:15:16,163   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:15:16,181   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:15:16,183   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 14:15:16,274   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:15:16,292   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 14:15:16,620   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 795 bytes result sent to driver
2019-02-14 14:15:16,628   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 401 ms on localhost (executor driver) (1/1)
2019-02-14 14:15:16,631   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 14:15:16,637   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (takeSample at TakeSampleTest.java:20) finished in 0.658 s
2019-02-14 14:15:16,642   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: takeSample at TakeSampleTest.java:20, took 0.729703 s
2019-02-14 14:15:16,718   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: takeSample at TakeSampleTest.java:20
2019-02-14 14:15:16,719   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (takeSample at TakeSampleTest.java:20) with 1 output partitions
2019-02-14 14:15:16,719   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (takeSample at TakeSampleTest.java:20)
2019-02-14 14:15:16,720   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:15:16,720   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:15:16,720   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (PartitionwiseSampledRDD[1] at takeSample at TakeSampleTest.java:20), which has no missing parents
2019-02-14 14:15:16,723   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 19.2 KB, free 1048.8 MB)
2019-02-14 14:15:16,729   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1048.8 MB)
2019-02-14 14:15:16,731   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:12773 (size: 7.5 KB, free: 1048.8 MB)
2019-02-14 14:15:16,732   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:15:16,733   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (PartitionwiseSampledRDD[1] at takeSample at TakeSampleTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:15:16,733   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 14:15:16,735   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7556 bytes)
2019-02-14 14:15:16,735   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 14:15:16,747   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 983 bytes result sent to driver
2019-02-14 14:15:16,749   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 15 ms on localhost (executor driver) (1/1)
2019-02-14 14:15:16,749   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 14:15:16,750   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (takeSample at TakeSampleTest.java:20) finished in 0.028 s
2019-02-14 14:15:16,750   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: takeSample at TakeSampleTest.java:20, took 0.031597 s
2019-02-14 14:15:16,760   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@18c5d94{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:15:16,763   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:15:16,764   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-02-14 14:15:16,765   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-02-14 14:15:16,765   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-02-14 14:15:16,784   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on DESKTOP-Q1PPPMM:12773 in memory (size: 7.5 KB, free: 1048.8 MB)
2019-02-14 14:15:16,798   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 14:15:16,821   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 14:15:16,821   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 14:15:16,823   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 14:15:16,827   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 14:15:16,836   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 14:15:16,839   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 14:15:16,840   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-0b80c64a-e576-4287-8034-af8ebeb3d0a7
2019-02-14 14:17:21,946   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 14:17:22,513   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 14:17:22,666   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 14:17:22,788   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 14:17:22,789   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 14:17:22,790   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 14:17:22,791   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 14:17:22,791   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 14:17:24,441   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 13106.
2019-02-14 14:17:24,489   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 14:17:24,525   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 14:17:24,531   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 14:17:24,531   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 14:17:24,549   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-a61ae11c-d3fd-46e1-860e-f974196ee9cc
2019-02-14 14:17:24,589   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 14:17:24,613   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 14:17:24,737   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5075ms
2019-02-14 14:17:24,812   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 14:17:24,830   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5170ms
2019-02-14 14:17:24,855   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:17:24,856   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 14:17:24,882   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,882   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,883   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,883   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,884   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,884   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,884   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,885   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,886   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,887   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,888   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,889   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,889   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,889   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,890   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,891   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,891   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,892   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,892   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,893   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,899   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,900   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,902   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,902   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 14:17:24,904   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:17:25,067   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 14:17:25,129   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 13117.
2019-02-14 14:17:25,130   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:13117
2019-02-14 14:17:25,131   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 14:17:25,166   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 13117, None)
2019-02-14 14:17:25,172   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:13117 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 13117, None)
2019-02-14 14:17:25,176   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 13117, None)
2019-02-14 14:17:25,177   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 13117, None)
2019-02-14 14:17:25,416   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:25,901   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: takeSample at TakeSampleTest.java:20
2019-02-14 14:17:25,938   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (takeSample at TakeSampleTest.java:20) with 1 output partitions
2019-02-14 14:17:25,939   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (takeSample at TakeSampleTest.java:20)
2019-02-14 14:17:25,940   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:17:25,941   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:17:25,950   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19), which has no missing parents
2019-02-14 14:17:26,041   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 14:17:26,052   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 1048.8 MB)
2019-02-14 14:17:26,147   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1341.0 B, free 1048.8 MB)
2019-02-14 14:17:26,151   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:13117 (size: 1341.0 B, free: 1048.8 MB)
2019-02-14 14:17:26,154   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:17:26,171   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:17:26,172   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 14:17:26,264   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:17:26,279   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 14:17:26,540   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 795 bytes result sent to driver
2019-02-14 14:17:26,548   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 319 ms on localhost (executor driver) (1/1)
2019-02-14 14:17:26,550   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 14:17:26,555   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (takeSample at TakeSampleTest.java:20) finished in 0.583 s
2019-02-14 14:17:26,563   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: takeSample at TakeSampleTest.java:20, took 0.662392 s
2019-02-14 14:17:26,580   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: takeSample at TakeSampleTest.java:20
2019-02-14 14:17:26,581   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (takeSample at TakeSampleTest.java:20) with 1 output partitions
2019-02-14 14:17:26,581   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (takeSample at TakeSampleTest.java:20)
2019-02-14 14:17:26,581   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:17:26,581   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:17:26,582   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19), which has no missing parents
2019-02-14 14:17:26,585   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 2.2 KB, free 1048.8 MB)
2019-02-14 14:17:26,593   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1342.0 B, free 1048.8 MB)
2019-02-14 14:17:26,594   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:13117 (size: 1342.0 B, free: 1048.8 MB)
2019-02-14 14:17:26,594   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:17:26,595   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:17:26,595   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 14:17:26,597   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:17:26,597   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 14:17:26,602   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 752 bytes result sent to driver
2019-02-14 14:17:26,604   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 8 ms on localhost (executor driver) (1/1)
2019-02-14 14:17:26,604   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 14:17:26,604   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (takeSample at TakeSampleTest.java:20) finished in 0.021 s
2019-02-14 14:17:26,605   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: takeSample at TakeSampleTest.java:20, took 0.023739 s
2019-02-14 14:17:26,613   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:17:26,614   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:17:26,627   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 14:17:26,651   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 14:17:26,652   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 14:17:26,658   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 14:17:26,661   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 14:17:26,667   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 14:17:26,670   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 14:17:26,670   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-6e858027-6bfa-4024-9a1f-08b833f7321b
2019-02-14 14:17:42,760   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 14:17:43,200   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 14:17:43,335   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 14:17:43,439   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 14:17:43,440   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 14:17:43,440   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 14:17:43,441   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 14:17:43,441   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 14:17:44,910   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 13160.
2019-02-14 14:17:44,950   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 14:17:44,986   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 14:17:44,991   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 14:17:44,992   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 14:17:45,009   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-9efd0b5e-76e7-4f5f-b26f-255b9cf671f4
2019-02-14 14:17:45,068   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 14:17:45,097   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 14:17:45,217   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4240ms
2019-02-14 14:17:45,293   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 14:17:45,308   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4335ms
2019-02-14 14:17:45,334   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:17:45,335   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 14:17:45,360   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,364   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,364   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,365   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,366   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,366   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,366   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,367   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,368   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,368   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,369   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,369   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,370   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,370   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,371   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,372   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,373   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,373   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,374   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,374   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,381   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,381   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,382   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,383   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,383   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 14:17:45,385   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:17:45,536   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 14:17:45,603   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 13169.
2019-02-14 14:17:45,603   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:13169
2019-02-14 14:17:45,606   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 14:17:45,646   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 13169, None)
2019-02-14 14:17:45,650   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:13169 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 13169, None)
2019-02-14 14:17:45,653   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 13169, None)
2019-02-14 14:17:45,653   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 13169, None)
2019-02-14 14:17:45,896   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 14:17:46,374   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: takeSample at TakeSampleTest.java:20
2019-02-14 14:17:46,406   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (takeSample at TakeSampleTest.java:20) with 1 output partitions
2019-02-14 14:17:46,407   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (takeSample at TakeSampleTest.java:20)
2019-02-14 14:17:46,407   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:17:46,408   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:17:46,415   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19), which has no missing parents
2019-02-14 14:17:46,521   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 14:17:46,534   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 1048.8 MB)
2019-02-14 14:17:46,627   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1341.0 B, free 1048.8 MB)
2019-02-14 14:17:46,631   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:13169 (size: 1341.0 B, free: 1048.8 MB)
2019-02-14 14:17:46,633   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:17:46,651   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:17:46,652   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 14:17:46,739   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:17:46,753   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 14:17:47,086   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 795 bytes result sent to driver
2019-02-14 14:17:47,095   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 397 ms on localhost (executor driver) (1/1)
2019-02-14 14:17:47,100   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 14:17:47,108   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (takeSample at TakeSampleTest.java:20) finished in 0.668 s
2019-02-14 14:17:47,115   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: takeSample at TakeSampleTest.java:20, took 0.740727 s
2019-02-14 14:17:47,135   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: takeSample at TakeSampleTest.java:20
2019-02-14 14:17:47,139   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (takeSample at TakeSampleTest.java:20) with 1 output partitions
2019-02-14 14:17:47,139   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (takeSample at TakeSampleTest.java:20)
2019-02-14 14:17:47,139   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:17:47,139   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:17:47,140   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19), which has no missing parents
2019-02-14 14:17:47,143   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 2.2 KB, free 1048.8 MB)
2019-02-14 14:17:47,151   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1342.0 B, free 1048.8 MB)
2019-02-14 14:17:47,152   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:13169 (size: 1342.0 B, free: 1048.8 MB)
2019-02-14 14:17:47,153   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:17:47,155   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:17:47,155   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 14:17:47,157   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:17:47,157   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 14:17:47,164   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 752 bytes result sent to driver
2019-02-14 14:17:47,166   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 10 ms on localhost (executor driver) (1/1)
2019-02-14 14:17:47,166   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 14:17:47,166   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (takeSample at TakeSampleTest.java:20) finished in 0.025 s
2019-02-14 14:17:47,167   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: takeSample at TakeSampleTest.java:20, took 0.031571 s
2019-02-14 14:17:47,177   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:17:47,179   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:17:47,194   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 14:17:47,226   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 14:17:47,226   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 14:17:47,238   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 14:17:47,241   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 14:17:47,250   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 14:17:47,253   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 14:17:47,254   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-458093fc-ef24-4f4f-a5cb-8cdf2ba182b0
2019-02-14 14:22:55,884   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 14:22:56,323   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 14:22:56,469   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 14:22:56,570   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 14:22:56,571   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 14:22:56,571   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 14:22:56,572   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 14:22:56,572   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 14:22:58,296   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 13738.
2019-02-14 14:22:58,334   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 14:22:58,363   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 14:22:58,366   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 14:22:58,366   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 14:22:58,389   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-6d403938-9d0a-47ad-bdbd-33c8566468c5
2019-02-14 14:22:58,437   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 14:22:58,456   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 14:22:58,574   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4457ms
2019-02-14 14:22:58,650   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 14:22:58,666   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4550ms
2019-02-14 14:22:58,695   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:22:58,695   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 14:22:58,724   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,725   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,725   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,726   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,726   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,727   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,728   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,729   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,729   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,730   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,731   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,732   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,733   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,734   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,735   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,736   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,736   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,737   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,738   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,738   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,746   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,747   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,748   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,748   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,749   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 14:22:58,750   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:22:58,885   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 14:22:58,950   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 13749.
2019-02-14 14:22:58,951   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:13749
2019-02-14 14:22:58,953   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 14:22:58,996   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 13749, None)
2019-02-14 14:22:59,001   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:13749 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 13749, None)
2019-02-14 14:22:59,005   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 13749, None)
2019-02-14 14:22:59,006   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 13749, None)
2019-02-14 14:22:59,257   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 14:22:59,769   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: takeSample at TakeSampleTest.java:20
2019-02-14 14:22:59,804   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (takeSample at TakeSampleTest.java:20) with 1 output partitions
2019-02-14 14:22:59,805   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (takeSample at TakeSampleTest.java:20)
2019-02-14 14:22:59,805   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:22:59,806   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:22:59,817   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19), which has no missing parents
2019-02-14 14:22:59,908   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 14:22:59,926   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 1048.8 MB)
2019-02-14 14:23:00,045   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1341.0 B, free 1048.8 MB)
2019-02-14 14:23:00,049   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:13749 (size: 1341.0 B, free: 1048.8 MB)
2019-02-14 14:23:00,050   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:23:00,068   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:23:00,069   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 14:23:00,156   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:23:00,168   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 14:23:00,454   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 752 bytes result sent to driver
2019-02-14 14:23:00,461   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 345 ms on localhost (executor driver) (1/1)
2019-02-14 14:23:00,467   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 14:23:00,473   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (takeSample at TakeSampleTest.java:20) finished in 0.635 s
2019-02-14 14:23:00,478   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: takeSample at TakeSampleTest.java:20, took 0.708871 s
2019-02-14 14:23:00,498   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: takeSample at TakeSampleTest.java:20
2019-02-14 14:23:00,499   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (takeSample at TakeSampleTest.java:20) with 1 output partitions
2019-02-14 14:23:00,499   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (takeSample at TakeSampleTest.java:20)
2019-02-14 14:23:00,499   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:23:00,499   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:23:00,500   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19), which has no missing parents
2019-02-14 14:23:00,502   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 2.2 KB, free 1048.8 MB)
2019-02-14 14:23:00,510   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1342.0 B, free 1048.8 MB)
2019-02-14 14:23:00,511   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:13749 (size: 1342.0 B, free: 1048.8 MB)
2019-02-14 14:23:00,512   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:23:00,514   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:23:00,514   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 14:23:00,515   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:23:00,516   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 14:23:00,523   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 752 bytes result sent to driver
2019-02-14 14:23:00,525   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 9 ms on localhost (executor driver) (1/1)
2019-02-14 14:23:00,525   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 14:23:00,525   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (takeSample at TakeSampleTest.java:20) finished in 0.024 s
2019-02-14 14:23:00,526   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: takeSample at TakeSampleTest.java:20, took 0.027020 s
2019-02-14 14:23:00,538   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-02-14 14:23:00,538   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-02-14 14:23:00,538   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-02-14 14:23:00,539   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-02-14 14:23:00,539   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-02-14 14:23:00,539   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-02-14 14:23:00,539   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-02-14 14:23:00,539   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-02-14 14:23:00,539   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-02-14 14:23:00,539   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-02-14 14:23:00,539   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-02-14 14:23:00,539   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-02-14 14:23:00,540   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-02-14 14:23:00,540   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-02-14 14:23:00,541   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: takeSample at TakeSampleTest.java:21
2019-02-14 14:23:00,545   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 2 (takeSample at TakeSampleTest.java:21) with 1 output partitions
2019-02-14 14:23:00,545   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (takeSample at TakeSampleTest.java:21)
2019-02-14 14:23:00,545   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:23:00,545   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:23:00,546   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19), which has no missing parents
2019-02-14 14:23:00,548   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 2.1 KB, free 1048.8 MB)
2019-02-14 14:23:00,555   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 1341.0 B, free 1048.8 MB)
2019-02-14 14:23:00,561   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:13749 (size: 1341.0 B, free: 1048.8 MB)
2019-02-14 14:23:00,562   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:23:00,563   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[0] at parallelize at TakeSampleTest.java:19) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:23:00,563   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-02-14 14:23:00,565   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7447 bytes)
2019-02-14 14:23:00,565   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-02-14 14:23:00,571   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 623 bytes result sent to driver
2019-02-14 14:23:00,573   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 9 ms on localhost (executor driver) (1/1)
2019-02-14 14:23:00,573   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on DESKTOP-Q1PPPMM:13749 in memory (size: 1342.0 B, free: 1048.8 MB)
2019-02-14 14:23:00,574   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-02-14 14:23:00,579   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (takeSample at TakeSampleTest.java:21) finished in 0.032 s
2019-02-14 14:23:00,586   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 2 finished: takeSample at TakeSampleTest.java:21, took 0.041730 s
2019-02-14 14:23:00,593   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on DESKTOP-Q1PPPMM:13749 in memory (size: 1341.0 B, free: 1048.8 MB)
2019-02-14 14:23:00,595   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-02-14 14:23:00,596   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-02-14 14:23:00,596   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-02-14 14:23:00,596   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-02-14 14:23:00,596   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-02-14 14:23:00,596   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-02-14 14:23:00,596   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-02-14 14:23:00,597   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-02-14 14:23:00,597   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-02-14 14:23:00,597   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-02-14 14:23:00,597   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-02-14 14:23:00,648   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 71
2019-02-14 14:23:00,648   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 67
2019-02-14 14:23:00,649   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 50
2019-02-14 14:23:00,649   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 57
2019-02-14 14:23:00,649   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 51
2019-02-14 14:23:00,649   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 73
2019-02-14 14:23:00,649   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 65
2019-02-14 14:23:00,649   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 52
2019-02-14 14:23:00,649   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 72
2019-02-14 14:23:00,650   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 70
2019-02-14 14:23:00,650   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 69
2019-02-14 14:23:00,650   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 60
2019-02-14 14:23:00,650   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 64
2019-02-14 14:23:00,650   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 55
2019-02-14 14:23:00,650   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 63
2019-02-14 14:23:00,650   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 66
2019-02-14 14:23:00,651   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 54
2019-02-14 14:23:00,651   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 58
2019-02-14 14:23:00,651   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 62
2019-02-14 14:23:00,651   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 53
2019-02-14 14:23:00,651   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 74
2019-02-14 14:23:00,651   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 56
2019-02-14 14:23:00,651   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 68
2019-02-14 14:23:00,651   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 59
2019-02-14 14:23:00,651   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 61
2019-02-14 14:23:00,664   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: takeSample at TakeSampleTest.java:21
2019-02-14 14:23:00,665   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 3 (takeSample at TakeSampleTest.java:21) with 1 output partitions
2019-02-14 14:23:00,665   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 3 (takeSample at TakeSampleTest.java:21)
2019-02-14 14:23:00,665   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:23:00,666   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:23:00,666   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 3 (PartitionwiseSampledRDD[1] at takeSample at TakeSampleTest.java:21), which has no missing parents
2019-02-14 14:23:00,670   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 19.2 KB, free 1048.8 MB)
2019-02-14 14:23:00,681   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.5 KB, free 1048.8 MB)
2019-02-14 14:23:00,682   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on DESKTOP-Q1PPPMM:13749 (size: 7.5 KB, free: 1048.8 MB)
2019-02-14 14:23:00,683   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:23:00,684   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 3 (PartitionwiseSampledRDD[1] at takeSample at TakeSampleTest.java:21) (first 15 tasks are for partitions Vector(0))
2019-02-14 14:23:00,685   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 1 tasks
2019-02-14 14:23:00,688   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7556 bytes)
2019-02-14 14:23:00,688   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 3)
2019-02-14 14:23:00,701   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 3). 1008 bytes result sent to driver
2019-02-14 14:23:00,703   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 3) in 18 ms on localhost (executor driver) (1/1)
2019-02-14 14:23:00,704   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-02-14 14:23:00,704   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 3 (takeSample at TakeSampleTest.java:21) finished in 0.036 s
2019-02-14 14:23:00,705   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 3 finished: takeSample at TakeSampleTest.java:21, took 0.039861 s
2019-02-14 14:23:00,713   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:23:00,714   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:23:00,731   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 14:23:00,777   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 14:23:00,777   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 14:23:00,780   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 14:23:00,783   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 14:23:00,789   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 14:23:00,792   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 14:23:00,792   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-45440427-1aab-428b-a71a-cf091ce3d5fa
2019-02-14 14:38:34,412   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 14:38:34,828   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 14:38:34,966   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 14:38:35,092   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 14:38:35,092   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 14:38:35,093   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 14:38:35,094   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 14:38:35,095   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 14:38:36,560   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 15582.
2019-02-14 14:38:36,599   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 14:38:36,634   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 14:38:36,638   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 14:38:36,639   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 14:38:36,658   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-9898c40f-f9c1-4a99-aefd-dfd047f6fb16
2019-02-14 14:38:36,696   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 14:38:36,721   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 14:38:36,867   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4332ms
2019-02-14 14:38:36,951   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 14:38:36,970   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4436ms
2019-02-14 14:38:36,997   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:38:36,998   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 14:38:37,023   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,027   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,027   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,028   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,028   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,030   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,031   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,032   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,032   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,033   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,033   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,034   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,034   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,034   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,035   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,036   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,036   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,037   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,037   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,038   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,045   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,045   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,049   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,051   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,052   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 14:38:37,054   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:38:37,199   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 14:38:37,272   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 15593.
2019-02-14 14:38:37,273   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:15593
2019-02-14 14:38:37,275   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 14:38:37,319   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 15593, None)
2019-02-14 14:38:37,324   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:15593 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 15593, None)
2019-02-14 14:38:37,328   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 15593, None)
2019-02-14 14:38:37,329   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 15593, None)
2019-02-14 14:38:37,605   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 14:38:38,112   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreachPartition at AggregateTest.java:21
2019-02-14 14:38:38,147   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreachPartition at AggregateTest.java:21) with 2 output partitions
2019-02-14 14:38:38,148   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (foreachPartition at AggregateTest.java:21)
2019-02-14 14:38:38,148   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:38:38,150   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:38:38,158   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19), which has no missing parents
2019-02-14 14:38:38,255   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 14:38:38,274   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 1048.8 MB)
2019-02-14 14:38:38,376   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1586.0 B, free 1048.8 MB)
2019-02-14 14:38:38,380   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:15593 (size: 1586.0 B, free: 1048.8 MB)
2019-02-14 14:38:38,382   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:38:38,402   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:38:38,403   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-02-14 14:38:38,486   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7397 bytes)
2019-02-14 14:38:38,503   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 14:38:38,757   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 757 bytes result sent to driver
2019-02-14 14:38:38,764   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7397 bytes)
2019-02-14 14:38:38,765   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-02-14 14:38:38,768   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 320 ms on localhost (executor driver) (1/2)
2019-02-14 14:38:38,770   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 585 bytes result sent to driver
2019-02-14 14:38:38,774   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 10 ms on localhost (executor driver) (2/2)
2019-02-14 14:38:38,775   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 14:38:38,778   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (foreachPartition at AggregateTest.java:21) finished in 0.598 s
2019-02-14 14:38:38,785   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreachPartition at AggregateTest.java:21, took 0.672249 s
2019-02-14 14:38:38,804   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: aggregate at AggregateTest.java:22
2019-02-14 14:38:38,805   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (aggregate at AggregateTest.java:22) with 2 output partitions
2019-02-14 14:38:38,805   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (aggregate at AggregateTest.java:22)
2019-02-14 14:38:38,805   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:38:38,806   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:38:38,806   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19), which has no missing parents
2019-02-14 14:38:38,808   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 2.9 KB, free 1048.8 MB)
2019-02-14 14:38:38,816   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1733.0 B, free 1048.8 MB)
2019-02-14 14:38:38,817   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:15593 (size: 1733.0 B, free: 1048.8 MB)
2019-02-14 14:38:38,817   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:38:38,819   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:38:38,819   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-02-14 14:38:38,820   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7397 bytes)
2019-02-14 14:38:38,820   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-02-14 14:38:38,829   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 665 bytes result sent to driver
2019-02-14 14:38:38,831   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 7397 bytes)
2019-02-14 14:38:38,831   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-02-14 14:38:38,831   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 11 ms on localhost (executor driver) (1/2)
2019-02-14 14:38:38,835   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 665 bytes result sent to driver
2019-02-14 14:38:38,836   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 6 ms on localhost (executor driver) (2/2)
2019-02-14 14:38:38,837   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 14:38:38,837   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (aggregate at AggregateTest.java:22) finished in 0.030 s
2019-02-14 14:38:38,838   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: aggregate at AggregateTest.java:22, took 0.033368 s
2019-02-14 14:38:38,845   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:38:38,846   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:38:38,856   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 14:38:38,881   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 14:38:38,882   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 14:38:38,888   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 14:38:38,895   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 14:38:38,901   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 14:38:38,903   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 14:38:38,904   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-e255cd12-fc73-4e64-a844-f13b3cd1b0bd
2019-02-14 14:39:08,860   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 14:39:09,272   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 14:39:09,407   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 14:39:09,521   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 14:39:09,521   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 14:39:09,522   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 14:39:09,522   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 14:39:09,523   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 14:39:11,081   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 15665.
2019-02-14 14:39:11,113   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 14:39:11,143   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 14:39:11,146   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 14:39:11,147   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 14:39:11,162   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-fabc9888-02ec-4a82-ba0c-ef596dbdf5a0
2019-02-14 14:39:11,197   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 14:39:11,218   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 14:39:11,342   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4348ms
2019-02-14 14:39:11,418   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 14:39:11,433   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4440ms
2019-02-14 14:39:11,459   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:39:11,460   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 14:39:11,496   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,498   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,499   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,501   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,501   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,502   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,502   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,503   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,503   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,504   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,504   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,505   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,505   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,506   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,506   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,507   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,508   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,509   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,510   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,512   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,521   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,522   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,523   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,523   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,524   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 14:39:11,525   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:39:11,698   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 14:39:11,768   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 15674.
2019-02-14 14:39:11,768   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:15674
2019-02-14 14:39:11,770   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 14:39:11,813   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 15674, None)
2019-02-14 14:39:11,821   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:15674 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 15674, None)
2019-02-14 14:39:11,828   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 15674, None)
2019-02-14 14:39:11,829   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 15674, None)
2019-02-14 14:39:12,067   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:12,578   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreachPartition at AggregateTest.java:21
2019-02-14 14:39:12,614   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreachPartition at AggregateTest.java:21) with 2 output partitions
2019-02-14 14:39:12,615   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (foreachPartition at AggregateTest.java:21)
2019-02-14 14:39:12,615   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:39:12,619   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:39:12,629   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19), which has no missing parents
2019-02-14 14:39:12,744   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 14:39:12,763   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 1048.8 MB)
2019-02-14 14:39:12,877   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1586.0 B, free 1048.8 MB)
2019-02-14 14:39:12,880   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:15674 (size: 1586.0 B, free: 1048.8 MB)
2019-02-14 14:39:12,882   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:39:12,901   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:39:12,903   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-02-14 14:39:13,010   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7397 bytes)
2019-02-14 14:39:13,040   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 14:39:13,362   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 714 bytes result sent to driver
2019-02-14 14:39:13,365   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7397 bytes)
2019-02-14 14:39:13,366   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-02-14 14:39:13,369   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 418 ms on localhost (executor driver) (1/2)
2019-02-14 14:39:13,373   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 671 bytes result sent to driver
2019-02-14 14:39:13,377   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 13 ms on localhost (executor driver) (2/2)
2019-02-14 14:39:13,378   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 14:39:13,381   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (foreachPartition at AggregateTest.java:21) finished in 0.712 s
2019-02-14 14:39:13,389   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreachPartition at AggregateTest.java:21, took 0.811871 s
2019-02-14 14:39:13,404   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: aggregate at AggregateTest.java:22
2019-02-14 14:39:13,405   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (aggregate at AggregateTest.java:22) with 2 output partitions
2019-02-14 14:39:13,405   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (aggregate at AggregateTest.java:22)
2019-02-14 14:39:13,405   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:39:13,406   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:39:13,406   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19), which has no missing parents
2019-02-14 14:39:13,409   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 2.9 KB, free 1048.8 MB)
2019-02-14 14:39:13,418   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1728.0 B, free 1048.8 MB)
2019-02-14 14:39:13,419   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:15674 (size: 1728.0 B, free: 1048.8 MB)
2019-02-14 14:39:13,420   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:39:13,422   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:39:13,422   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-02-14 14:39:13,423   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7397 bytes)
2019-02-14 14:39:13,424   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-02-14 14:39:13,434   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 665 bytes result sent to driver
2019-02-14 14:39:13,436   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 7397 bytes)
2019-02-14 14:39:13,436   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-02-14 14:39:13,436   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 13 ms on localhost (executor driver) (1/2)
2019-02-14 14:39:13,441   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 665 bytes result sent to driver
2019-02-14 14:39:13,443   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 8 ms on localhost (executor driver) (2/2)
2019-02-14 14:39:13,443   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 14:39:13,444   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (aggregate at AggregateTest.java:22) finished in 0.037 s
2019-02-14 14:39:13,446   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: aggregate at AggregateTest.java:22, took 0.041102 s
2019-02-14 14:39:13,453   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:39:13,455   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:39:13,467   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 14:39:13,490   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 14:39:13,490   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 14:39:13,497   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 14:39:13,504   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 14:39:13,511   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 14:39:13,513   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 14:39:13,514   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-819975ce-80dc-4dab-820c-d508f0c8cfc5
2019-02-14 14:39:42,577   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 14:39:42,971   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 14:39:43,106   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 14:39:43,261   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 14:39:43,262   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 14:39:43,263   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 14:39:43,264   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 14:39:43,264   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 14:39:44,744   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 15747.
2019-02-14 14:39:44,780   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 14:39:44,808   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 14:39:44,812   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 14:39:44,812   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 14:39:44,830   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-f5d13c6a-b53c-4275-840c-c23e510fb499
2019-02-14 14:39:44,875   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 14:39:44,902   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 14:39:45,030   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4256ms
2019-02-14 14:39:45,105   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 14:39:45,119   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4346ms
2019-02-14 14:39:45,143   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@18c5d94{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:39:45,143   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 14:39:45,171   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1ef51f2{/jobs,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,171   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@527389{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,172   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5a3466{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,173   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,173   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/stages,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,173   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,174   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,175   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,175   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,176   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,176   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/storage,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,177   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,177   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,177   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,178   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/environment,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,179   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,181   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/executors,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,182   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,182   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,184   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,191   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/static,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,192   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c10d84{/,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,193   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15fe372{/api,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,193   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,194   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6ef90e{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 14:39:45,195   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:39:45,330   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 14:39:45,395   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 15756.
2019-02-14 14:39:45,396   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:15756
2019-02-14 14:39:45,398   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 14:39:45,437   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 15756, None)
2019-02-14 14:39:45,441   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:15756 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 15756, None)
2019-02-14 14:39:45,445   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 15756, None)
2019-02-14 14:39:45,446   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 15756, None)
2019-02-14 14:39:45,671   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c612bd{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 14:39:46,162   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreachPartition at AggregateTest.java:21
2019-02-14 14:39:46,201   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreachPartition at AggregateTest.java:21) with 2 output partitions
2019-02-14 14:39:46,201   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (foreachPartition at AggregateTest.java:21)
2019-02-14 14:39:46,202   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:39:46,203   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:39:46,210   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19), which has no missing parents
2019-02-14 14:39:46,303   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 14:39:46,321   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 1048.8 MB)
2019-02-14 14:39:46,440   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1586.0 B, free 1048.8 MB)
2019-02-14 14:39:46,443   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:15756 (size: 1586.0 B, free: 1048.8 MB)
2019-02-14 14:39:46,446   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:39:46,461   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:39:46,462   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-02-14 14:39:46,545   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:39:46,562   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 14:39:46,824   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 714 bytes result sent to driver
2019-02-14 14:39:46,828   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:39:46,828   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-02-14 14:39:46,831   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 318 ms on localhost (executor driver) (1/2)
2019-02-14 14:39:46,835   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 585 bytes result sent to driver
2019-02-14 14:39:46,838   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 11 ms on localhost (executor driver) (2/2)
2019-02-14 14:39:46,839   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 14:39:46,842   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (foreachPartition at AggregateTest.java:21) finished in 0.612 s
2019-02-14 14:39:46,847   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreachPartition at AggregateTest.java:21, took 0.685195 s
2019-02-14 14:39:46,862   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: aggregate at AggregateTest.java:22
2019-02-14 14:39:46,863   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (aggregate at AggregateTest.java:22) with 2 output partitions
2019-02-14 14:39:46,863   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (aggregate at AggregateTest.java:22)
2019-02-14 14:39:46,864   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:39:46,864   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:39:46,864   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19), which has no missing parents
2019-02-14 14:39:46,866   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 2.9 KB, free 1048.8 MB)
2019-02-14 14:39:46,874   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1728.0 B, free 1048.8 MB)
2019-02-14 14:39:46,875   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:15756 (size: 1728.0 B, free: 1048.8 MB)
2019-02-14 14:39:46,875   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:39:46,876   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:39:46,877   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-02-14 14:39:46,878   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-02-14 14:39:46,878   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-02-14 14:39:46,889   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 665 bytes result sent to driver
2019-02-14 14:39:46,891   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:39:46,893   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-02-14 14:39:46,893   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 16 ms on localhost (executor driver) (1/2)
2019-02-14 14:39:46,897   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 622 bytes result sent to driver
2019-02-14 14:39:46,898   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 8 ms on localhost (executor driver) (2/2)
2019-02-14 14:39:46,899   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 14:39:46,900   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (aggregate at AggregateTest.java:22) finished in 0.034 s
2019-02-14 14:39:46,900   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: aggregate at AggregateTest.java:22, took 0.037592 s
2019-02-14 14:39:46,908   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@18c5d94{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:39:46,910   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:39:46,921   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 14:39:46,943   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 14:39:46,944   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 14:39:46,951   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 14:39:46,959   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 14:39:46,967   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 14:39:46,970   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 14:39:46,970   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-780a5162-e5dc-42a6-8ba8-03724a67ee43
2019-02-14 14:40:52,686   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 14:40:53,098   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 14:40:53,228   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 14:40:53,368   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 14:40:53,368   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 14:40:53,369   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 14:40:53,369   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 14:40:53,370   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 14:40:54,984   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 15891.
2019-02-14 14:40:55,028   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 14:40:55,063   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 14:40:55,067   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 14:40:55,068   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 14:40:55,094   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-11ac99a1-1763-4788-a83a-4357fa803add
2019-02-14 14:40:55,145   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 14:40:55,171   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 14:40:55,293   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4385ms
2019-02-14 14:40:55,368   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 14:40:55,383   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4477ms
2019-02-14 14:40:55,421   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@18c5d94{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:40:55,421   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 14:40:55,442   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1ef51f2{/jobs,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,446   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@527389{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,447   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5a3466{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,447   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,448   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/stages,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,448   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,449   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,449   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,450   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,450   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,451   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/storage,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,452   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,453   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,453   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,454   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/environment,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,454   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,455   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/executors,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,455   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,456   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,456   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,463   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/static,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,464   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c10d84{/,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,466   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15fe372{/api,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,466   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,467   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6ef90e{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 14:40:55,468   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:40:55,601   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 14:40:55,667   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 15902.
2019-02-14 14:40:55,667   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:15902
2019-02-14 14:40:55,669   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 14:40:55,706   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 15902, None)
2019-02-14 14:40:55,713   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:15902 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 15902, None)
2019-02-14 14:40:55,719   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 15902, None)
2019-02-14 14:40:55,720   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 15902, None)
2019-02-14 14:40:55,957   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c612bd{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 14:40:56,430   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreachPartition at AggregateTest.java:21
2019-02-14 14:40:56,463   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreachPartition at AggregateTest.java:21) with 2 output partitions
2019-02-14 14:40:56,464   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (foreachPartition at AggregateTest.java:21)
2019-02-14 14:40:56,464   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:40:56,466   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:40:56,473   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19), which has no missing parents
2019-02-14 14:40:56,574   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 14:40:56,592   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 1048.8 MB)
2019-02-14 14:40:56,692   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1586.0 B, free 1048.8 MB)
2019-02-14 14:40:56,696   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:15902 (size: 1586.0 B, free: 1048.8 MB)
2019-02-14 14:40:56,698   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:40:56,713   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:40:56,717   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-02-14 14:40:56,793   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:40:56,809   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 14:40:57,075   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 714 bytes result sent to driver
2019-02-14 14:40:57,078   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:40:57,079   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-02-14 14:40:57,082   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 322 ms on localhost (executor driver) (1/2)
2019-02-14 14:40:57,085   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 585 bytes result sent to driver
2019-02-14 14:40:57,088   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 11 ms on localhost (executor driver) (2/2)
2019-02-14 14:40:57,089   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 14:40:57,093   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (foreachPartition at AggregateTest.java:21) finished in 0.598 s
2019-02-14 14:40:57,098   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreachPartition at AggregateTest.java:21, took 0.668175 s
2019-02-14 14:40:57,116   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: aggregate at AggregateTest.java:22
2019-02-14 14:40:57,117   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (aggregate at AggregateTest.java:22) with 2 output partitions
2019-02-14 14:40:57,117   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (aggregate at AggregateTest.java:22)
2019-02-14 14:40:57,118   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:40:57,118   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:40:57,118   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19), which has no missing parents
2019-02-14 14:40:57,121   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 2.9 KB, free 1048.8 MB)
2019-02-14 14:40:57,128   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1728.0 B, free 1048.8 MB)
2019-02-14 14:40:57,128   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:15902 (size: 1728.0 B, free: 1048.8 MB)
2019-02-14 14:40:57,129   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:40:57,130   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:40:57,130   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-02-14 14:40:57,132   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-02-14 14:40:57,132   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-02-14 14:40:57,142   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 665 bytes result sent to driver
2019-02-14 14:40:57,143   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:40:57,144   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-02-14 14:40:57,144   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 13 ms on localhost (executor driver) (1/2)
2019-02-14 14:40:57,148   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 622 bytes result sent to driver
2019-02-14 14:40:57,149   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 6 ms on localhost (executor driver) (2/2)
2019-02-14 14:40:57,149   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 14:40:57,150   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (aggregate at AggregateTest.java:22) finished in 0.031 s
2019-02-14 14:40:57,151   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: aggregate at AggregateTest.java:22, took 0.034025 s
2019-02-14 14:40:57,158   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@18c5d94{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:40:57,160   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:40:57,171   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 14:40:57,193   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 14:40:57,194   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 14:40:57,200   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 14:40:57,206   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 14:40:57,212   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 14:40:57,218   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 14:40:57,219   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-506a07ec-ee32-40fa-a78f-bb9958002f05
2019-02-14 14:41:53,808   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 14:41:54,214   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 14:41:54,341   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 14:41:54,442   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 14:41:54,442   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 14:41:54,443   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 14:41:54,443   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 14:41:54,444   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 14:41:55,935   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 16019.
2019-02-14 14:41:55,969   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 14:41:56,001   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 14:41:56,004   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 14:41:56,004   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 14:41:56,023   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-a72d96e8-1d7e-4605-8dc7-b42a21926858
2019-02-14 14:41:56,059   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 14:41:56,077   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 14:41:56,225   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4135ms
2019-02-14 14:41:56,318   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 14:41:56,342   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4254ms
2019-02-14 14:41:56,377   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:41:56,377   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 14:41:56,410   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,410   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,411   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,412   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,413   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,413   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,414   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,415   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,416   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,418   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,419   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,420   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,423   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,424   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,425   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,426   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,426   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,427   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,428   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,428   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,439   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,440   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,442   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,443   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,443   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 14:41:56,445   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:41:56,617   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 14:41:56,700   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 16032.
2019-02-14 14:41:56,701   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:16032
2019-02-14 14:41:56,704   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 14:41:56,752   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 16032, None)
2019-02-14 14:41:56,757   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:16032 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 16032, None)
2019-02-14 14:41:56,761   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 16032, None)
2019-02-14 14:41:56,762   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 16032, None)
2019-02-14 14:41:57,066   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 14:41:57,549   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreachPartition at AggregateTest.java:21
2019-02-14 14:41:57,586   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreachPartition at AggregateTest.java:21) with 2 output partitions
2019-02-14 14:41:57,587   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (foreachPartition at AggregateTest.java:21)
2019-02-14 14:41:57,587   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:41:57,589   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:41:57,595   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19), which has no missing parents
2019-02-14 14:41:57,686   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 14:41:57,701   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 1048.8 MB)
2019-02-14 14:41:57,794   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1586.0 B, free 1048.8 MB)
2019-02-14 14:41:57,798   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:16032 (size: 1586.0 B, free: 1048.8 MB)
2019-02-14 14:41:57,800   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:41:57,816   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:41:57,817   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-02-14 14:41:57,898   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:41:57,913   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 14:41:58,173   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 757 bytes result sent to driver
2019-02-14 14:41:58,176   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:41:58,177   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-02-14 14:41:58,180   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 316 ms on localhost (executor driver) (1/2)
2019-02-14 14:41:58,185   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 628 bytes result sent to driver
2019-02-14 14:41:58,189   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 13 ms on localhost (executor driver) (2/2)
2019-02-14 14:41:58,190   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 14:41:58,193   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (foreachPartition at AggregateTest.java:21) finished in 0.578 s
2019-02-14 14:41:58,201   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreachPartition at AggregateTest.java:21, took 0.651996 s
2019-02-14 14:41:58,217   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: aggregate at AggregateTest.java:22
2019-02-14 14:41:58,218   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (aggregate at AggregateTest.java:22) with 2 output partitions
2019-02-14 14:41:58,218   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (aggregate at AggregateTest.java:22)
2019-02-14 14:41:58,218   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:41:58,218   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:41:58,218   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19), which has no missing parents
2019-02-14 14:41:58,220   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 2.9 KB, free 1048.8 MB)
2019-02-14 14:41:58,227   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1728.0 B, free 1048.8 MB)
2019-02-14 14:41:58,229   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:16032 (size: 1728.0 B, free: 1048.8 MB)
2019-02-14 14:41:58,232   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:41:58,233   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:19) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:41:58,233   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-02-14 14:41:58,235   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-02-14 14:41:58,235   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-02-14 14:41:58,244   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 622 bytes result sent to driver
2019-02-14 14:41:58,246   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:41:58,246   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-02-14 14:41:58,247   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 13 ms on localhost (executor driver) (1/2)
2019-02-14 14:41:58,251   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 622 bytes result sent to driver
2019-02-14 14:41:58,253   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 8 ms on localhost (executor driver) (2/2)
2019-02-14 14:41:58,253   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 14:41:58,253   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (aggregate at AggregateTest.java:22) finished in 0.034 s
2019-02-14 14:41:58,254   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: aggregate at AggregateTest.java:22, took 0.037065 s
2019-02-14 14:41:58,260   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:41:58,263   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:41:58,274   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 14:41:58,295   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 14:41:58,295   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 14:41:58,303   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 14:41:58,310   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 14:41:58,316   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 14:41:58,318   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 14:41:58,318   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-3aa956f3-ee9f-4c18-b713-cd00cc4de238
2019-02-14 14:43:52,617   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 14:43:53,023   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 14:43:53,155   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 14:43:53,353   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 14:43:53,353   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 14:43:53,354   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 14:43:53,355   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 14:43:53,355   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 14:43:55,050   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 16400.
2019-02-14 14:43:55,097   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 14:43:55,125   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 14:43:55,128   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 14:43:55,128   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 14:43:55,143   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-d42ecd04-8742-47cc-a049-5baa1fd8cebd
2019-02-14 14:43:55,179   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 14:43:55,199   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 14:43:55,320   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4772ms
2019-02-14 14:43:55,400   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 14:43:55,416   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4868ms
2019-02-14 14:43:55,441   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:43:55,442   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 14:43:55,466   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,470   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,470   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,471   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,471   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,472   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,473   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,475   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,476   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,476   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,477   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,478   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,478   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,479   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,480   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,480   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,481   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,482   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,482   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,483   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,490   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,491   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,492   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,492   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,493   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 14:43:55,494   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:43:55,631   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 14:43:55,695   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 16411.
2019-02-14 14:43:55,696   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:16411
2019-02-14 14:43:55,697   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 14:43:55,734   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 16411, None)
2019-02-14 14:43:55,739   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:16411 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 16411, None)
2019-02-14 14:43:55,743   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 16411, None)
2019-02-14 14:43:55,743   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 16411, None)
2019-02-14 14:43:55,989   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 14:43:56,461   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreachPartition at AggregateTest.java:22
2019-02-14 14:43:56,497   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreachPartition at AggregateTest.java:22) with 2 output partitions
2019-02-14 14:43:56,497   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (foreachPartition at AggregateTest.java:22)
2019-02-14 14:43:56,498   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:43:56,499   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:43:56,505   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:20), which has no missing parents
2019-02-14 14:43:56,598   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 14:43:56,615   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 1048.8 MB)
2019-02-14 14:43:56,710   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1586.0 B, free 1048.8 MB)
2019-02-14 14:43:56,713   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:16411 (size: 1586.0 B, free: 1048.8 MB)
2019-02-14 14:43:56,715   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:43:56,733   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:20) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:43:56,733   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-02-14 14:43:56,810   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:43:56,828   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 14:43:57,107   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 757 bytes result sent to driver
2019-02-14 14:43:57,110   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:43:57,111   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-02-14 14:43:57,113   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 337 ms on localhost (executor driver) (1/2)
2019-02-14 14:43:57,116   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 585 bytes result sent to driver
2019-02-14 14:43:57,120   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 11 ms on localhost (executor driver) (2/2)
2019-02-14 14:43:57,121   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 14:43:57,124   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (foreachPartition at AggregateTest.java:22) finished in 0.597 s
2019-02-14 14:43:57,130   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreachPartition at AggregateTest.java:22, took 0.668261 s
2019-02-14 14:43:57,146   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: aggregate at AggregateTest.java:23
2019-02-14 14:43:57,147   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (aggregate at AggregateTest.java:23) with 2 output partitions
2019-02-14 14:43:57,147   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (aggregate at AggregateTest.java:23)
2019-02-14 14:43:57,147   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:43:57,148   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:43:57,148   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:20), which has no missing parents
2019-02-14 14:43:57,150   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 2.9 KB, free 1048.8 MB)
2019-02-14 14:43:57,157   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1728.0 B, free 1048.8 MB)
2019-02-14 14:43:57,158   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:16411 (size: 1728.0 B, free: 1048.8 MB)
2019-02-14 14:43:57,159   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:43:57,160   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:20) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:43:57,160   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-02-14 14:43:57,161   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-02-14 14:43:57,162   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-02-14 14:43:57,172   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 665 bytes result sent to driver
2019-02-14 14:43:57,174   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:43:57,174   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-02-14 14:43:57,174   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 13 ms on localhost (executor driver) (1/2)
2019-02-14 14:43:57,178   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 665 bytes result sent to driver
2019-02-14 14:43:57,179   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 6 ms on localhost (executor driver) (2/2)
2019-02-14 14:43:57,180   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 14:43:57,180   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (aggregate at AggregateTest.java:23) finished in 0.031 s
2019-02-14 14:43:57,181   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: aggregate at AggregateTest.java:23, took 0.034399 s
2019-02-14 14:43:57,191   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:43:57,194   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:43:57,204   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 14:43:57,227   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 14:43:57,228   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 14:43:57,236   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 14:43:57,242   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 14:43:57,249   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 14:43:57,252   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 14:43:57,253   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-d8718bbd-3274-4d76-b890-d993a34bff93
2019-02-14 14:46:04,999   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 14:46:05,503   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 14:46:05,635   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 14:46:05,745   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 14:46:05,747   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 14:46:05,747   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 14:46:05,748   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 14:46:05,748   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 14:46:07,257   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 16657.
2019-02-14 14:46:07,291   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 14:46:07,322   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 14:46:07,324   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 14:46:07,325   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 14:46:07,345   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-afd08f4d-45b6-4bd0-9f7b-509fd5546fa3
2019-02-14 14:46:07,386   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 14:46:07,406   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 14:46:07,522   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4602ms
2019-02-14 14:46:07,600   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 14:46:07,615   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4696ms
2019-02-14 14:46:07,641   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:46:07,642   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 14:46:07,668   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,672   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,673   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,674   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,674   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,675   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,675   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,676   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,677   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,677   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,678   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,679   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,679   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,680   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,680   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,681   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,681   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,682   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,682   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,683   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,690   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,691   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,693   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,694   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,695   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 14:46:07,696   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:46:07,835   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 14:46:07,899   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 16668.
2019-02-14 14:46:07,900   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:16668
2019-02-14 14:46:07,902   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 14:46:07,943   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 16668, None)
2019-02-14 14:46:07,947   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:16668 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 16668, None)
2019-02-14 14:46:07,953   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 16668, None)
2019-02-14 14:46:07,954   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 16668, None)
2019-02-14 14:46:08,211   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 14:46:08,697   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreachPartition at AggregateTest.java:22
2019-02-14 14:46:08,729   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreachPartition at AggregateTest.java:22) with 2 output partitions
2019-02-14 14:46:08,730   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (foreachPartition at AggregateTest.java:22)
2019-02-14 14:46:08,730   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:46:08,731   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:46:08,738   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:20), which has no missing parents
2019-02-14 14:46:08,835   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 14:46:08,853   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 1048.8 MB)
2019-02-14 14:46:08,949   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1586.0 B, free 1048.8 MB)
2019-02-14 14:46:08,953   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:16668 (size: 1586.0 B, free: 1048.8 MB)
2019-02-14 14:46:08,955   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:46:08,972   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:20) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:46:08,974   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-02-14 14:46:09,052   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:46:09,067   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 14:46:09,344   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 757 bytes result sent to driver
2019-02-14 14:46:09,348   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:46:09,350   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-02-14 14:46:09,354   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 336 ms on localhost (executor driver) (1/2)
2019-02-14 14:46:09,358   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 628 bytes result sent to driver
2019-02-14 14:46:09,363   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 16 ms on localhost (executor driver) (2/2)
2019-02-14 14:46:09,364   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 14:46:09,368   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (foreachPartition at AggregateTest.java:22) finished in 0.602 s
2019-02-14 14:46:09,375   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreachPartition at AggregateTest.java:22, took 0.678089 s
2019-02-14 14:46:09,391   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: aggregate at AggregateTest.java:23
2019-02-14 14:46:09,392   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (aggregate at AggregateTest.java:23) with 2 output partitions
2019-02-14 14:46:09,392   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (aggregate at AggregateTest.java:23)
2019-02-14 14:46:09,392   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:46:09,393   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:46:09,393   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:20), which has no missing parents
2019-02-14 14:46:09,395   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 2.9 KB, free 1048.8 MB)
2019-02-14 14:46:09,401   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1733.0 B, free 1048.8 MB)
2019-02-14 14:46:09,402   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:16668 (size: 1733.0 B, free: 1048.8 MB)
2019-02-14 14:46:09,403   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:46:09,404   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:20) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:46:09,404   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-02-14 14:46:09,406   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-02-14 14:46:09,406   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-02-14 14:46:09,416   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 622 bytes result sent to driver
2019-02-14 14:46:09,418   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:46:09,418   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-02-14 14:46:09,418   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 13 ms on localhost (executor driver) (1/2)
2019-02-14 14:46:09,425   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 622 bytes result sent to driver
2019-02-14 14:46:09,426   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 9 ms on localhost (executor driver) (2/2)
2019-02-14 14:46:09,426   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 14:46:09,427   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (aggregate at AggregateTest.java:23) finished in 0.033 s
2019-02-14 14:46:09,427   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: aggregate at AggregateTest.java:23, took 0.036378 s
2019-02-14 14:46:09,435   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:46:09,436   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:46:09,446   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 14:46:09,474   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 14:46:09,474   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 14:46:09,481   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 14:46:09,488   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 14:46:09,494   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 14:46:09,497   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 14:46:09,497   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-2f8b8a79-377d-423e-8b5e-bad55da4a50c
2019-02-14 14:54:52,066   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 14:54:52,608   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 14:54:52,759   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 14:54:52,858   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 14:54:52,858   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 14:54:52,859   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 14:54:52,859   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 14:54:52,860   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 14:54:54,472   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 17688.
2019-02-14 14:54:54,528   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 14:54:54,571   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 14:54:54,574   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 14:54:54,575   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 14:54:54,601   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-4d58fbd2-1d8b-4ffe-98ba-5dd64e941f2e
2019-02-14 14:54:54,658   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 14:54:54,702   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 14:54:54,887   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4817ms
2019-02-14 14:54:54,976   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 14:54:54,994   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4925ms
2019-02-14 14:54:55,021   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:54:55,022   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 14:54:55,050   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,051   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,051   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,052   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,053   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,053   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,053   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,054   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,055   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,055   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,057   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,059   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,059   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,060   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,060   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,061   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,061   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,062   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,068   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,069   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,070   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,070   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,071   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 14:54:55,072   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:54:55,227   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 14:54:55,292   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 17699.
2019-02-14 14:54:55,293   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:17699
2019-02-14 14:54:55,294   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 14:54:55,334   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 17699, None)
2019-02-14 14:54:55,338   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:17699 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 17699, None)
2019-02-14 14:54:55,343   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 17699, None)
2019-02-14 14:54:55,344   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 17699, None)
2019-02-14 14:54:55,596   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 14:54:56,225   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreachPartition at AggregateTest.java:23
2019-02-14 14:54:56,266   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreachPartition at AggregateTest.java:23) with 2 output partitions
2019-02-14 14:54:56,266   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (foreachPartition at AggregateTest.java:23)
2019-02-14 14:54:56,267   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:54:56,268   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:54:56,274   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:21), which has no missing parents
2019-02-14 14:54:56,385   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 14:54:56,402   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 1048.8 MB)
2019-02-14 14:54:56,505   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1586.0 B, free 1048.8 MB)
2019-02-14 14:54:56,509   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:17699 (size: 1586.0 B, free: 1048.8 MB)
2019-02-14 14:54:56,513   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:54:56,531   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:21) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:54:56,532   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-02-14 14:54:56,611   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:54:56,631   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 14:54:56,898   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 757 bytes result sent to driver
2019-02-14 14:54:56,902   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:54:56,902   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-02-14 14:54:56,905   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 326 ms on localhost (executor driver) (1/2)
2019-02-14 14:54:56,909   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 585 bytes result sent to driver
2019-02-14 14:54:56,912   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 11 ms on localhost (executor driver) (2/2)
2019-02-14 14:54:56,913   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 14:54:56,915   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (foreachPartition at AggregateTest.java:23) finished in 0.610 s
2019-02-14 14:54:56,923   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreachPartition at AggregateTest.java:23, took 0.697653 s
2019-02-14 14:54:56,943   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: aggregate at AggregateTest.java:24
2019-02-14 14:54:56,944   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (aggregate at AggregateTest.java:24) with 2 output partitions
2019-02-14 14:54:56,944   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (aggregate at AggregateTest.java:24)
2019-02-14 14:54:56,944   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 14:54:56,945   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 14:54:56,945   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:21), which has no missing parents
2019-02-14 14:54:56,948   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 2.9 KB, free 1048.8 MB)
2019-02-14 14:54:56,956   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1733.0 B, free 1048.8 MB)
2019-02-14 14:54:56,957   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:17699 (size: 1733.0 B, free: 1048.8 MB)
2019-02-14 14:54:56,957   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 14:54:56,958   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:21) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 14:54:56,958   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-02-14 14:54:56,959   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-02-14 14:54:56,960   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-02-14 14:54:56,969   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 665 bytes result sent to driver
2019-02-14 14:54:56,970   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 14:54:56,971   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-02-14 14:54:56,971   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 12 ms on localhost (executor driver) (1/2)
2019-02-14 14:54:56,978   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 622 bytes result sent to driver
2019-02-14 14:54:56,979   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 9 ms on localhost (executor driver) (2/2)
2019-02-14 14:54:56,979   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 14:54:56,980   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (aggregate at AggregateTest.java:24) finished in 0.034 s
2019-02-14 14:54:56,981   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: aggregate at AggregateTest.java:24, took 0.037897 s
2019-02-14 14:54:56,988   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 14:54:56,989   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 14:54:57,001   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 14:54:57,023   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 14:54:57,024   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 14:54:57,034   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 14:54:57,040   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 14:54:57,050   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 14:54:57,052   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 14:54:57,053   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-6a4b80d9-4147-4b2e-b2fe-370c80023a7d
2019-02-14 15:04:32,451   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 15:04:32,894   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 15:04:33,052   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 15:04:33,167   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 15:04:33,168   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 15:04:33,168   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 15:04:33,169   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 15:04:33,169   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 15:04:34,788   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 18928.
2019-02-14 15:04:34,832   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 15:04:34,881   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 15:04:34,884   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 15:04:34,887   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 15:04:34,905   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-7065e405-bf95-4c57-a390-3c6106d1b583
2019-02-14 15:04:34,952   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 15:04:34,976   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 15:04:35,110   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4667ms
2019-02-14 15:04:35,186   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 15:04:35,200   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4760ms
2019-02-14 15:04:35,231   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 15:04:35,231   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 15:04:35,258   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,262   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,263   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,264   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,264   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,265   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,265   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,266   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,266   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,268   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,268   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,269   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,269   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,270   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,270   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,271   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,271   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,272   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,272   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,279   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,280   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,281   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,282   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,282   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 15:04:35,284   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 15:04:35,427   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 15:04:35,490   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 18940.
2019-02-14 15:04:35,491   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:18940
2019-02-14 15:04:35,493   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 15:04:35,532   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 18940, None)
2019-02-14 15:04:35,535   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:18940 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 18940, None)
2019-02-14 15:04:35,540   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 18940, None)
2019-02-14 15:04:35,540   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 18940, None)
2019-02-14 15:04:35,805   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 15:04:36,326   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreachPartition at AggregateTest.java:23
2019-02-14 15:04:36,363   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreachPartition at AggregateTest.java:23) with 2 output partitions
2019-02-14 15:04:36,364   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (foreachPartition at AggregateTest.java:23)
2019-02-14 15:04:36,364   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:04:36,366   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:04:36,373   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:21), which has no missing parents
2019-02-14 15:04:36,463   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 15:04:36,479   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 1048.8 MB)
2019-02-14 15:04:36,573   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1586.0 B, free 1048.8 MB)
2019-02-14 15:04:36,576   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:18940 (size: 1586.0 B, free: 1048.8 MB)
2019-02-14 15:04:36,579   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:04:36,596   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:21) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 15:04:36,597   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-02-14 15:04:36,679   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7367 bytes)
2019-02-14 15:04:36,695   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 15:04:37,017   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 714 bytes result sent to driver
2019-02-14 15:04:37,020   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 15:04:37,020   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-02-14 15:04:37,023   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 381 ms on localhost (executor driver) (1/2)
2019-02-14 15:04:37,027   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 628 bytes result sent to driver
2019-02-14 15:04:37,030   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 11 ms on localhost (executor driver) (2/2)
2019-02-14 15:04:37,031   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 15:04:37,035   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (foreachPartition at AggregateTest.java:23) finished in 0.638 s
2019-02-14 15:04:37,040   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreachPartition at AggregateTest.java:23, took 0.713777 s
2019-02-14 15:04:37,063   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: aggregate at AggregateTest.java:24
2019-02-14 15:04:37,064   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (aggregate at AggregateTest.java:24) with 2 output partitions
2019-02-14 15:04:37,064   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (aggregate at AggregateTest.java:24)
2019-02-14 15:04:37,064   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:04:37,065   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:04:37,065   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:21), which has no missing parents
2019-02-14 15:04:37,067   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 2.9 KB, free 1048.8 MB)
2019-02-14 15:04:37,079   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1733.0 B, free 1048.8 MB)
2019-02-14 15:04:37,080   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:18940 (size: 1733.0 B, free: 1048.8 MB)
2019-02-14 15:04:37,081   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:04:37,083   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:21) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 15:04:37,083   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-02-14 15:04:37,085   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-02-14 15:04:37,085   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-02-14 15:04:37,100   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 665 bytes result sent to driver
2019-02-14 15:04:37,101   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 15:04:37,102   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-02-14 15:04:37,110   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 26 ms on localhost (executor driver) (1/2)
2019-02-14 15:04:37,112   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 665 bytes result sent to driver
2019-02-14 15:04:37,128   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 27 ms on localhost (executor driver) (2/2)
2019-02-14 15:04:37,134   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (aggregate at AggregateTest.java:24) finished in 0.068 s
2019-02-14 15:04:37,137   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: aggregate at AggregateTest.java:24, took 0.073856 s
2019-02-14 15:04:37,138   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 15:04:37,153   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: fold at AggregateTest.java:25
2019-02-14 15:04:37,156   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 2 (fold at AggregateTest.java:25) with 2 output partitions
2019-02-14 15:04:37,157   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (fold at AggregateTest.java:25)
2019-02-14 15:04:37,157   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:04:37,158   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:04:37,160   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:21), which has no missing parents
2019-02-14 15:04:37,165   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 2.8 KB, free 1048.8 MB)
2019-02-14 15:04:37,200   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-02-14 15:04:37,200   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-02-14 15:04:37,200   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-02-14 15:04:37,203   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-02-14 15:04:37,204   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-02-14 15:04:37,204   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-02-14 15:04:37,212   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 1662.0 B, free 1048.8 MB)
2019-02-14 15:04:37,233   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:18940 (size: 1662.0 B, free: 1048.8 MB)
2019-02-14 15:04:37,235   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:04:37,236   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 2 (ParallelCollectionRDD[0] at parallelize at AggregateTest.java:21) (first 15 tasks are for partitions Vector(0, 1))
2019-02-14 15:04:37,236   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 2 tasks
2019-02-14 15:04:37,238   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-02-14 15:04:37,239   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 4)
2019-02-14 15:04:37,249   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on DESKTOP-Q1PPPMM:18940 in memory (size: 1733.0 B, free: 1048.8 MB)
2019-02-14 15:04:37,252   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 4). 622 bytes result sent to driver
2019-02-14 15:04:37,253   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 7367 bytes)
2019-02-14 15:04:37,254   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 4) in 16 ms on localhost (executor driver) (1/2)
2019-02-14 15:04:37,255   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 2.0 (TID 5)
2019-02-14 15:04:37,263   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-02-14 15:04:37,263   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 2.0 (TID 5). 665 bytes result sent to driver
2019-02-14 15:04:37,263   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-02-14 15:04:37,263   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-02-14 15:04:37,264   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-02-14 15:04:37,264   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-02-14 15:04:37,264   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-02-14 15:04:37,264   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-02-14 15:04:37,264   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-02-14 15:04:37,264   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 2.0 (TID 5) in 11 ms on localhost (executor driver) (2/2)
2019-02-14 15:04:37,264   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-02-14 15:04:37,264   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-02-14 15:04:37,264   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-02-14 15:04:37,265   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-02-14 15:04:37,265   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-02-14 15:04:37,265   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-02-14 15:04:37,265   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (fold at AggregateTest.java:25) finished in 0.103 s
2019-02-14 15:04:37,265   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-02-14 15:04:37,265   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-02-14 15:04:37,266   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-02-14 15:04:37,266   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-02-14 15:04:37,266   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 2 finished: fold at AggregateTest.java:25, took 0.111527 s
2019-02-14 15:04:37,266   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-02-14 15:04:37,266   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-02-14 15:04:37,274   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 15:04:37,276   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 15:04:37,294   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 15:04:37,331   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 15:04:37,332   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 15:04:37,333   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 15:04:37,336   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 15:04:37,343   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 15:04:37,346   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 15:04:37,346   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-c3264a5b-72a2-427b-a8ee-eb2de78340cf
2019-02-14 15:15:31,861   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 15:15:32,355   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 15:15:32,528   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 15:15:32,663   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 15:15:32,664   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 15:15:32,665   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 15:15:32,665   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 15:15:32,666   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 15:15:34,489   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 20285.
2019-02-14 15:15:34,530   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 15:15:34,585   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 15:15:34,590   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 15:15:34,591   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 15:15:34,615   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-5ae6c4f9-e14f-4cee-8fc4-3da402290543
2019-02-14 15:15:34,660   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 15:15:34,680   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 15:15:34,813   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5179ms
2019-02-14 15:15:34,923   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 15:15:34,952   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5319ms
2019-02-14 15:15:34,990   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@61a60e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 15:15:34,990   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 15:15:35,019   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@50eab8{/jobs,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,023   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5a3466{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,024   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,024   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,025   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/stages,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,025   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,026   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,027   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,027   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,028   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,028   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/storage,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,030   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,031   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,032   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,033   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/environment,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,033   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,034   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/executors,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,034   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,034   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,035   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,043   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/static,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,044   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15fe372{/,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,045   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/api,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,045   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6ef90e{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,046   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 15:15:35,047   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 15:15:35,189   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 15:15:35,281   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 20294.
2019-02-14 15:15:35,283   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:20294
2019-02-14 15:15:35,288   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 15:15:35,365   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 20294, None)
2019-02-14 15:15:35,371   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:20294 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 20294, None)
2019-02-14 15:15:35,376   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 20294, None)
2019-02-14 15:15:35,377   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 20294, None)
2019-02-14 15:15:35,821   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f41c66{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 15:15:36,729   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: countByKey at CountByKeyTest.java:25
2019-02-14 15:15:36,766   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 1 (countByKey at CountByKeyTest.java:25)
2019-02-14 15:15:36,772   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (countByKey at CountByKeyTest.java:25) with 1 output partitions
2019-02-14 15:15:36,772   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (countByKey at CountByKeyTest.java:25)
2019-02-14 15:15:36,773   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-02-14 15:15:36,775   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-02-14 15:15:36,788   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[1] at countByKey at CountByKeyTest.java:25), which has no missing parents
2019-02-14 15:15:36,888   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 15:15:36,903   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.5 KB, free 1048.8 MB)
2019-02-14 15:15:37,042   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1048.8 MB)
2019-02-14 15:15:37,045   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:20294 (size: 2.1 KB, free: 1048.8 MB)
2019-02-14 15:15:37,049   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:15:37,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[1] at countByKey at CountByKeyTest.java:25) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:15:37,130   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 15:15:37,290   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7513 bytes)
2019-02-14 15:15:37,306   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 15:15:37,522   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1076 bytes result sent to driver
2019-02-14 15:15:37,531   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 287 ms on localhost (executor driver) (1/1)
2019-02-14 15:15:37,535   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 15:15:37,542   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (countByKey at CountByKeyTest.java:25) finished in 0.723 s
2019-02-14 15:15:37,542   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-02-14 15:15:37,543   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-02-14 15:15:37,543   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-02-14 15:15:37,544   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-02-14 15:15:37,547   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[2] at countByKey at CountByKeyTest.java:25), which has no missing parents
2019-02-14 15:15:37,572   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 1048.8 MB)
2019-02-14 15:15:37,577   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1048.8 MB)
2019-02-14 15:15:37,580   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:20294 (size: 2.2 KB, free: 1048.8 MB)
2019-02-14 15:15:37,580   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:15:37,583   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[2] at countByKey at CountByKeyTest.java:25) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:15:37,583   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 15:15:37,590   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-02-14 15:15:37,591   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 15:15:37,818   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-02-14 15:15:37,821   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 8 ms
2019-02-14 15:15:37,860   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1413 bytes result sent to driver
2019-02-14 15:15:37,863   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 275 ms on localhost (executor driver) (1/1)
2019-02-14 15:15:37,864   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 15:15:37,865   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (countByKey at CountByKeyTest.java:25) finished in 0.294 s
2019-02-14 15:15:37,871   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: countByKey at CountByKeyTest.java:25, took 1.141674 s
2019-02-14 15:15:37,881   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@61a60e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 15:15:37,882   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 15:15:37,894   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 15:15:37,930   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 15:15:37,931   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 15:15:37,941   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 15:15:37,943   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 15:15:37,949   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 15:15:37,953   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 15:15:37,956   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-93f1aaa5-2dda-4b33-b65d-2dc88fe0ac5b
2019-02-14 15:39:22,680   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 15:39:23,160   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 15:39:23,305   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 15:39:23,430   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 15:39:23,430   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 15:39:23,431   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 15:39:23,431   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 15:39:23,432   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 15:39:24,969   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 23155.
2019-02-14 15:39:25,010   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 15:39:25,053   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 15:39:25,056   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 15:39:25,057   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 15:39:25,076   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-c9d2dace-38b1-460e-80de-51b9b814e13b
2019-02-14 15:39:25,115   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 15:39:25,138   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 15:39:25,283   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4772ms
2019-02-14 15:39:25,367   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 15:39:25,385   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4876ms
2019-02-14 15:39:25,415   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 15:39:25,415   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 15:39:25,444   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,445   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,445   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,446   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,446   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,447   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,447   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,448   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,448   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,449   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,450   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,450   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,451   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,452   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,453   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,453   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,453   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,454   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,455   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,455   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,462   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,463   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,465   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,466   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,466   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 15:39:25,468   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 15:39:25,609   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 15:39:25,678   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23164.
2019-02-14 15:39:25,679   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:23164
2019-02-14 15:39:25,681   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 15:39:25,727   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 23164, None)
2019-02-14 15:39:25,731   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:23164 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 23164, None)
2019-02-14 15:39:25,736   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 23164, None)
2019-02-14 15:39:25,737   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 23164, None)
2019-02-14 15:39:26,014   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 15:39:26,598   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: count at NumberOperateTest.java:19
2019-02-14 15:39:26,631   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (count at NumberOperateTest.java:19) with 1 output partitions
2019-02-14 15:39:26,631   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (count at NumberOperateTest.java:19)
2019-02-14 15:39:26,632   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:39:26,634   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:39:26,640   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[1] at parallelizeDoubles at NumberOperateTest.java:18), which has no missing parents
2019-02-14 15:39:26,739   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 15:39:26,754   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.1 KB, free 1048.8 MB)
2019-02-14 15:39:26,821   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1830.0 B, free 1048.8 MB)
2019-02-14 15:39:26,824   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:23164 (size: 1830.0 B, free: 1048.8 MB)
2019-02-14 15:39:26,826   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:39:26,845   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parallelizeDoubles at NumberOperateTest.java:18) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:39:26,846   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 15:39:26,961   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:39:26,978   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 15:39:27,318   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 795 bytes result sent to driver
2019-02-14 15:39:27,328   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 415 ms on localhost (executor driver) (1/1)
2019-02-14 15:39:27,331   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 15:39:27,338   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (count at NumberOperateTest.java:19) finished in 0.674 s
2019-02-14 15:39:27,344   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: count at NumberOperateTest.java:19, took 0.746101 s
2019-02-14 15:39:27,364   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: mean at NumberOperateTest.java:20
2019-02-14 15:39:27,366   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (mean at NumberOperateTest.java:20) with 1 output partitions
2019-02-14 15:39:27,366   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (mean at NumberOperateTest.java:20)
2019-02-14 15:39:27,366   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:39:27,367   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:39:27,367   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[2] at mean at NumberOperateTest.java:20), which has no missing parents
2019-02-14 15:39:27,372   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 1048.8 MB)
2019-02-14 15:39:27,382   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1850.0 B, free 1048.8 MB)
2019-02-14 15:39:27,383   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:23164 (size: 1850.0 B, free: 1048.8 MB)
2019-02-14 15:39:27,384   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:39:27,386   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at mean at NumberOperateTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:39:27,386   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 15:39:27,388   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:39:27,388   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 15:39:27,404   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 797 bytes result sent to driver
2019-02-14 15:39:27,407   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 19 ms on localhost (executor driver) (1/1)
2019-02-14 15:39:27,407   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 15:39:27,408   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (mean at NumberOperateTest.java:20) finished in 0.038 s
2019-02-14 15:39:27,408   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: mean at NumberOperateTest.java:20, took 0.042243 s
2019-02-14 15:39:27,423   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: sum at NumberOperateTest.java:21
2019-02-14 15:39:27,424   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 2 (sum at NumberOperateTest.java:21) with 1 output partitions
2019-02-14 15:39:27,425   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (sum at NumberOperateTest.java:21)
2019-02-14 15:39:27,425   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:39:27,425   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:39:27,426   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (ParallelCollectionRDD[0] at parallelizeDoubles at NumberOperateTest.java:18), which has no missing parents
2019-02-14 15:39:27,430   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-14 15:39:27,436   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 1426.0 B, free 1048.8 MB)
2019-02-14 15:39:27,442   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:23164 (size: 1426.0 B, free: 1048.8 MB)
2019-02-14 15:39:27,442   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:39:27,443   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[0] at parallelizeDoubles at NumberOperateTest.java:18) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:39:27,444   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-02-14 15:39:27,445   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:39:27,446   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-02-14 15:39:27,451   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on DESKTOP-Q1PPPMM:23164 in memory (size: 1850.0 B, free: 1048.8 MB)
2019-02-14 15:39:27,453   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 668 bytes result sent to driver
2019-02-14 15:39:27,454   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 9 ms on localhost (executor driver) (1/1)
2019-02-14 15:39:27,454   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-02-14 15:39:27,455   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (sum at NumberOperateTest.java:21) finished in 0.028 s
2019-02-14 15:39:27,456   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 2 finished: sum at NumberOperateTest.java:21, took 0.032226 s
2019-02-14 15:39:27,477   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: max at NumberOperateTest.java:22
2019-02-14 15:39:27,478   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 3 (max at NumberOperateTest.java:22) with 1 output partitions
2019-02-14 15:39:27,478   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 3 (max at NumberOperateTest.java:22)
2019-02-14 15:39:27,479   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:39:27,479   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:39:27,479   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 3 (MapPartitionsRDD[1] at parallelizeDoubles at NumberOperateTest.java:18), which has no missing parents
2019-02-14 15:39:27,483   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.5 KB, free 1048.8 MB)
2019-02-14 15:39:27,489   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2038.0 B, free 1048.8 MB)
2019-02-14 15:39:27,490   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 59
2019-02-14 15:39:27,490   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 50
2019-02-14 15:39:27,490   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 51
2019-02-14 15:39:27,490   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 54
2019-02-14 15:39:27,490   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 71
2019-02-14 15:39:27,490   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on DESKTOP-Q1PPPMM:23164 (size: 2038.0 B, free: 1048.8 MB)
2019-02-14 15:39:27,490   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 74
2019-02-14 15:39:27,491   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 67
2019-02-14 15:39:27,491   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 56
2019-02-14 15:39:27,491   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 68
2019-02-14 15:39:27,491   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:39:27,491   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 64
2019-02-14 15:39:27,491   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 62
2019-02-14 15:39:27,491   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 66
2019-02-14 15:39:27,491   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 60
2019-02-14 15:39:27,492   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 55
2019-02-14 15:39:27,492   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 63
2019-02-14 15:39:27,492   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 72
2019-02-14 15:39:27,492   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[1] at parallelizeDoubles at NumberOperateTest.java:18) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:39:27,493   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 1 tasks
2019-02-14 15:39:27,494   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on DESKTOP-Q1PPPMM:23164 in memory (size: 1426.0 B, free: 1048.8 MB)
2019-02-14 15:39:27,494   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:39:27,494   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 3)
2019-02-14 15:39:27,497   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 52
2019-02-14 15:39:27,497   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 70
2019-02-14 15:39:27,497   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 58
2019-02-14 15:39:27,497   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 53
2019-02-14 15:39:27,497   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 61
2019-02-14 15:39:27,497   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 65
2019-02-14 15:39:27,498   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 73
2019-02-14 15:39:27,498   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 69
2019-02-14 15:39:27,498   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 57
2019-02-14 15:39:27,501   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 3). 751 bytes result sent to driver
2019-02-14 15:39:27,503   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 3) in 9 ms on localhost (executor driver) (1/1)
2019-02-14 15:39:27,503   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-02-14 15:39:27,503   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 3 (max at NumberOperateTest.java:22) finished in 0.021 s
2019-02-14 15:39:27,504   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 3 finished: max at NumberOperateTest.java:22, took 0.026289 s
2019-02-14 15:39:27,513   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 98
2019-02-14 15:39:27,513   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 83
2019-02-14 15:39:27,513   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 89
2019-02-14 15:39:27,513   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 97
2019-02-14 15:39:27,514   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 87
2019-02-14 15:39:27,514   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 91
2019-02-14 15:39:27,514   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 77
2019-02-14 15:39:27,514   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 79
2019-02-14 15:39:27,514   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 99
2019-02-14 15:39:27,514   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 78
2019-02-14 15:39:27,515   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: variance at NumberOperateTest.java:23
2019-02-14 15:39:27,515   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 86
2019-02-14 15:39:27,515   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 84
2019-02-14 15:39:27,515   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 95
2019-02-14 15:39:27,515   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 85
2019-02-14 15:39:27,515   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 96
2019-02-14 15:39:27,515   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 92
2019-02-14 15:39:27,515   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 82
2019-02-14 15:39:27,516   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 80
2019-02-14 15:39:27,516   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 76
2019-02-14 15:39:27,516   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 4 (variance at NumberOperateTest.java:23) with 1 output partitions
2019-02-14 15:39:27,516   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 81
2019-02-14 15:39:27,516   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 4 (variance at NumberOperateTest.java:23)
2019-02-14 15:39:27,516   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:39:27,516   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:39:27,517   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 4 (MapPartitionsRDD[3] at variance at NumberOperateTest.java:23), which has no missing parents
2019-02-14 15:39:27,518   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_3_piece0 on DESKTOP-Q1PPPMM:23164 in memory (size: 2038.0 B, free: 1048.8 MB)
2019-02-14 15:39:27,519   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 1048.8 MB)
2019-02-14 15:39:27,520   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 75
2019-02-14 15:39:27,520   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 88
2019-02-14 15:39:27,521   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 90
2019-02-14 15:39:27,521   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 94
2019-02-14 15:39:27,521   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 93
2019-02-14 15:39:27,522   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 1850.0 B, free 1048.8 MB)
2019-02-14 15:39:27,524   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on DESKTOP-Q1PPPMM:23164 (size: 1850.0 B, free: 1048.8 MB)
2019-02-14 15:39:27,525   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:39:27,526   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[3] at variance at NumberOperateTest.java:23) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:39:27,526   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 4.0 with 1 tasks
2019-02-14 15:39:27,527   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:39:27,528   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 4.0 (TID 4)
2019-02-14 15:39:27,532   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 4.0 (TID 4). 754 bytes result sent to driver
2019-02-14 15:39:27,533   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 4.0 (TID 4) in 6 ms on localhost (executor driver) (1/1)
2019-02-14 15:39:27,533   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2019-02-14 15:39:27,534   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 4 (variance at NumberOperateTest.java:23) finished in 0.016 s
2019-02-14 15:39:27,534   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 4 finished: variance at NumberOperateTest.java:23, took 0.019176 s
2019-02-14 15:39:27,542   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 122
2019-02-14 15:39:27,542   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 111
2019-02-14 15:39:27,542   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 100
2019-02-14 15:39:27,542   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 116
2019-02-14 15:39:27,543   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 115
2019-02-14 15:39:27,543   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 124
2019-02-14 15:39:27,544   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 101
2019-02-14 15:39:27,544   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 105
2019-02-14 15:39:27,544   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 113
2019-02-14 15:39:27,544   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 109
2019-02-14 15:39:27,544   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 112
2019-02-14 15:39:27,544   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 110
2019-02-14 15:39:27,544   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: sampleVariance at NumberOperateTest.java:24
2019-02-14 15:39:27,544   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 107
2019-02-14 15:39:27,545   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 119
2019-02-14 15:39:27,545   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 102
2019-02-14 15:39:27,545   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 103
2019-02-14 15:39:27,545   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 104
2019-02-14 15:39:27,545   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 121
2019-02-14 15:39:27,545   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 5 (sampleVariance at NumberOperateTest.java:24) with 1 output partitions
2019-02-14 15:39:27,546   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 5 (sampleVariance at NumberOperateTest.java:24)
2019-02-14 15:39:27,546   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:39:27,546   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:39:27,546   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 5 (MapPartitionsRDD[4] at sampleVariance at NumberOperateTest.java:24), which has no missing parents
2019-02-14 15:39:27,547   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_4_piece0 on DESKTOP-Q1PPPMM:23164 in memory (size: 1850.0 B, free: 1048.8 MB)
2019-02-14 15:39:27,549   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 1048.8 MB)
2019-02-14 15:39:27,552   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 106
2019-02-14 15:39:27,552   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 120
2019-02-14 15:39:27,552   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 108
2019-02-14 15:39:27,553   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 114
2019-02-14 15:39:27,553   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 118
2019-02-14 15:39:27,553   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 117
2019-02-14 15:39:27,553   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 123
2019-02-14 15:39:27,554   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5_piece0 stored as bytes in memory (estimated size 1850.0 B, free 1048.8 MB)
2019-02-14 15:39:27,559   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_5_piece0 in memory on DESKTOP-Q1PPPMM:23164 (size: 1850.0 B, free: 1048.8 MB)
2019-02-14 15:39:27,560   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:39:27,560   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[4] at sampleVariance at NumberOperateTest.java:24) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:39:27,561   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 5.0 with 1 tasks
2019-02-14 15:39:27,562   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:39:27,562   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 5.0 (TID 5)
2019-02-14 15:39:27,566   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 5.0 (TID 5). 754 bytes result sent to driver
2019-02-14 15:39:27,567   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 5.0 (TID 5) in 6 ms on localhost (executor driver) (1/1)
2019-02-14 15:39:27,567   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2019-02-14 15:39:27,567   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 5 (sampleVariance at NumberOperateTest.java:24) finished in 0.020 s
2019-02-14 15:39:27,568   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 5 finished: sampleVariance at NumberOperateTest.java:24, took 0.022938 s
2019-02-14 15:39:27,577   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 126
2019-02-14 15:39:27,578   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 143
2019-02-14 15:39:27,578   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 132
2019-02-14 15:39:27,578   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 137
2019-02-14 15:39:27,578   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 128
2019-02-14 15:39:27,578   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 129
2019-02-14 15:39:27,578   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 139
2019-02-14 15:39:27,578   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 146
2019-02-14 15:39:27,579   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 138
2019-02-14 15:39:27,579   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 130
2019-02-14 15:39:27,579   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 141
2019-02-14 15:39:27,579   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 142
2019-02-14 15:39:27,579   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 133
2019-02-14 15:39:27,579   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 148
2019-02-14 15:39:27,580   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 136
2019-02-14 15:39:27,580   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 145
2019-02-14 15:39:27,580   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 135
2019-02-14 15:39:27,580   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 144
2019-02-14 15:39:27,580   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 140
2019-02-14 15:39:27,580   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 147
2019-02-14 15:39:27,580   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 131
2019-02-14 15:39:27,580   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 127
2019-02-14 15:39:27,580   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 149
2019-02-14 15:39:27,582   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: stdev at NumberOperateTest.java:25
2019-02-14 15:39:27,585   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 6 (stdev at NumberOperateTest.java:25) with 1 output partitions
2019-02-14 15:39:27,588   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_5_piece0 on DESKTOP-Q1PPPMM:23164 in memory (size: 1850.0 B, free: 1048.8 MB)
2019-02-14 15:39:27,590   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 6 (stdev at NumberOperateTest.java:25)
2019-02-14 15:39:27,590   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:39:27,590   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:39:27,591   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 6 (MapPartitionsRDD[5] at stdev at NumberOperateTest.java:25), which has no missing parents
2019-02-14 15:39:27,591   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 125
2019-02-14 15:39:27,591   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 134
2019-02-14 15:39:27,593   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 1048.8 MB)
2019-02-14 15:39:27,597   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_6_piece0 stored as bytes in memory (estimated size 1848.0 B, free 1048.8 MB)
2019-02-14 15:39:27,599   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_6_piece0 in memory on DESKTOP-Q1PPPMM:23164 (size: 1848.0 B, free: 1048.8 MB)
2019-02-14 15:39:27,600   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 6 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:39:27,601   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[5] at stdev at NumberOperateTest.java:25) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:39:27,601   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 6.0 with 1 tasks
2019-02-14 15:39:27,602   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:39:27,603   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 6.0 (TID 6)
2019-02-14 15:39:27,608   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 6.0 (TID 6). 797 bytes result sent to driver
2019-02-14 15:39:27,609   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 6.0 (TID 6) in 7 ms on localhost (executor driver) (1/1)
2019-02-14 15:39:27,609   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 6.0, whose tasks have all completed, from pool 
2019-02-14 15:39:27,610   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 6 (stdev at NumberOperateTest.java:25) finished in 0.018 s
2019-02-14 15:39:27,610   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 6 finished: stdev at NumberOperateTest.java:25, took 0.027936 s
2019-02-14 15:39:27,616   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 155
2019-02-14 15:39:27,616   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 170
2019-02-14 15:39:27,616   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 162
2019-02-14 15:39:27,616   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 153
2019-02-14 15:39:27,616   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 165
2019-02-14 15:39:27,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 167
2019-02-14 15:39:27,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 152
2019-02-14 15:39:27,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 151
2019-02-14 15:39:27,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 172
2019-02-14 15:39:27,619   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_6_piece0 on DESKTOP-Q1PPPMM:23164 in memory (size: 1848.0 B, free: 1048.8 MB)
2019-02-14 15:39:27,621   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: sampleStdev at NumberOperateTest.java:26
2019-02-14 15:39:27,622   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 7 (sampleStdev at NumberOperateTest.java:26) with 1 output partitions
2019-02-14 15:39:27,622   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 156
2019-02-14 15:39:27,622   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 7 (sampleStdev at NumberOperateTest.java:26)
2019-02-14 15:39:27,622   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 154
2019-02-14 15:39:27,623   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:39:27,623   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 168
2019-02-14 15:39:27,623   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 164
2019-02-14 15:39:27,623   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:39:27,623   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 161
2019-02-14 15:39:27,623   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 159
2019-02-14 15:39:27,623   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 160
2019-02-14 15:39:27,623   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 174
2019-02-14 15:39:27,623   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 166
2019-02-14 15:39:27,624   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 157
2019-02-14 15:39:27,624   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 169
2019-02-14 15:39:27,625   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 150
2019-02-14 15:39:27,625   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 173
2019-02-14 15:39:27,625   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 158
2019-02-14 15:39:27,625   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 163
2019-02-14 15:39:27,623   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 7 (MapPartitionsRDD[6] at sampleStdev at NumberOperateTest.java:26), which has no missing parents
2019-02-14 15:39:27,625   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 171
2019-02-14 15:39:27,627   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_7 stored as values in memory (estimated size 3.3 KB, free 1048.8 MB)
2019-02-14 15:39:27,630   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_7_piece0 stored as bytes in memory (estimated size 1851.0 B, free 1048.8 MB)
2019-02-14 15:39:27,632   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_7_piece0 in memory on DESKTOP-Q1PPPMM:23164 (size: 1851.0 B, free: 1048.8 MB)
2019-02-14 15:39:27,632   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 7 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:39:27,633   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[6] at sampleStdev at NumberOperateTest.java:26) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:39:27,633   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 7.0 with 1 tasks
2019-02-14 15:39:27,635   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:39:27,636   INFO --- [Executor task launch worker for task 7]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 7.0 (TID 7)
2019-02-14 15:39:27,641   INFO --- [Executor task launch worker for task 7]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 7.0 (TID 7). 754 bytes result sent to driver
2019-02-14 15:39:27,646   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 7.0 (TID 7) in 11 ms on localhost (executor driver) (1/1)
2019-02-14 15:39:27,647   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2019-02-14 15:39:27,648   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 7 (sampleStdev at NumberOperateTest.java:26) finished in 0.022 s
2019-02-14 15:39:27,649   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 7 finished: sampleStdev at NumberOperateTest.java:26, took 0.027128 s
2019-02-14 15:39:27,660   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 15:39:27,662   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 15:39:27,677   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 15:39:27,819   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 15:39:27,820   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 15:39:27,825   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 15:39:27,837   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 15:39:27,849   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 15:39:27,855   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 15:39:27,857   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-43307ecd-12e0-4816-a351-ff63f3993088
2019-02-14 15:40:26,087   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 15:40:26,649   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 15:40:26,817   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 15:40:26,916   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 15:40:26,916   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 15:40:26,917   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 15:40:26,917   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 15:40:26,918   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 15:40:28,582   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 23296.
2019-02-14 15:40:28,616   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 15:40:28,647   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 15:40:28,649   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 15:40:28,650   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 15:40:28,666   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-fd25c69e-7240-418b-a12b-d250175d5e05
2019-02-14 15:40:28,706   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 15:40:28,727   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 15:40:28,854   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5334ms
2019-02-14 15:40:28,952   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 15:40:28,970   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5453ms
2019-02-14 15:40:28,996   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@3b9204{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 15:40:28,997   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 15:40:29,035   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@50eab8{/jobs,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,036   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5a3466{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,036   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,037   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,038   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/stages,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,039   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,039   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,041   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,041   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,042   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,043   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/storage,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,044   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,044   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,045   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,046   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/environment,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,047   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,049   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/executors,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,050   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,051   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,052   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,062   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/static,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,063   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15fe372{/,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,069   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/api,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,071   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6ef90e{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,072   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 15:40:29,078   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 15:40:29,278   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 15:40:29,359   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 23307.
2019-02-14 15:40:29,360   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:23307
2019-02-14 15:40:29,362   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 15:40:29,398   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 23307, None)
2019-02-14 15:40:29,402   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:23307 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 23307, None)
2019-02-14 15:40:29,407   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 23307, None)
2019-02-14 15:40:29,407   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 23307, None)
2019-02-14 15:40:29,652   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f41c66{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 15:40:30,151   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: count at NumberOperateTest.java:19
2019-02-14 15:40:30,182   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (count at NumberOperateTest.java:19) with 1 output partitions
2019-02-14 15:40:30,183   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (count at NumberOperateTest.java:19)
2019-02-14 15:40:30,184   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:40:30,185   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:40:30,192   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[1] at parallelizeDoubles at NumberOperateTest.java:18), which has no missing parents
2019-02-14 15:40:30,310   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 15:40:30,331   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.1 KB, free 1048.8 MB)
2019-02-14 15:40:30,410   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1830.0 B, free 1048.8 MB)
2019-02-14 15:40:30,414   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:23307 (size: 1830.0 B, free: 1048.8 MB)
2019-02-14 15:40:30,416   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:40:30,439   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parallelizeDoubles at NumberOperateTest.java:18) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:40:30,442   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 15:40:30,527   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:40:30,543   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 15:40:30,851   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 752 bytes result sent to driver
2019-02-14 15:40:30,857   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 362 ms on localhost (executor driver) (1/1)
2019-02-14 15:40:30,861   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 15:40:30,867   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (count at NumberOperateTest.java:19) finished in 0.649 s
2019-02-14 15:40:30,875   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: count at NumberOperateTest.java:19, took 0.723652 s
2019-02-14 15:40:30,893   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: mean at NumberOperateTest.java:20
2019-02-14 15:40:30,894   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (mean at NumberOperateTest.java:20) with 1 output partitions
2019-02-14 15:40:30,894   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (mean at NumberOperateTest.java:20)
2019-02-14 15:40:30,894   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:40:30,894   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:40:30,895   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[2] at mean at NumberOperateTest.java:20), which has no missing parents
2019-02-14 15:40:30,898   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 1048.8 MB)
2019-02-14 15:40:30,905   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1850.0 B, free 1048.8 MB)
2019-02-14 15:40:30,906   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:23307 (size: 1850.0 B, free: 1048.8 MB)
2019-02-14 15:40:30,906   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:40:30,907   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at mean at NumberOperateTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:40:30,908   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 15:40:30,909   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:40:30,909   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 15:40:30,924   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 840 bytes result sent to driver
2019-02-14 15:40:30,925   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 17 ms on localhost (executor driver) (1/1)
2019-02-14 15:40:30,926   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 15:40:30,926   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (mean at NumberOperateTest.java:20) finished in 0.030 s
2019-02-14 15:40:30,927   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: mean at NumberOperateTest.java:20, took 0.033497 s
2019-02-14 15:40:30,939   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: sum at NumberOperateTest.java:21
2019-02-14 15:40:30,940   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 2 (sum at NumberOperateTest.java:21) with 1 output partitions
2019-02-14 15:40:30,940   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (sum at NumberOperateTest.java:21)
2019-02-14 15:40:30,941   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:40:30,941   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:40:30,941   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (ParallelCollectionRDD[0] at parallelizeDoubles at NumberOperateTest.java:18), which has no missing parents
2019-02-14 15:40:30,943   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 2.3 KB, free 1048.8 MB)
2019-02-14 15:40:30,947   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 1426.0 B, free 1048.8 MB)
2019-02-14 15:40:30,955   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:23307 (size: 1426.0 B, free: 1048.8 MB)
2019-02-14 15:40:30,955   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:40:30,957   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (ParallelCollectionRDD[0] at parallelizeDoubles at NumberOperateTest.java:18) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:40:30,957   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-02-14 15:40:30,958   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:40:30,959   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-02-14 15:40:30,963   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on DESKTOP-Q1PPPMM:23307 in memory (size: 1850.0 B, free: 1048.8 MB)
2019-02-14 15:40:30,968   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 668 bytes result sent to driver
2019-02-14 15:40:30,969   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 11 ms on localhost (executor driver) (1/1)
2019-02-14 15:40:30,970   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-02-14 15:40:30,971   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (sum at NumberOperateTest.java:21) finished in 0.028 s
2019-02-14 15:40:30,971   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 2 finished: sum at NumberOperateTest.java:21, took 0.031409 s
2019-02-14 15:40:30,990   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: min at NumberOperateTest.java:22
2019-02-14 15:40:30,991   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 3 (min at NumberOperateTest.java:22) with 1 output partitions
2019-02-14 15:40:30,991   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 3 (min at NumberOperateTest.java:22)
2019-02-14 15:40:30,991   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:40:30,991   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:40:30,992   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 3 (MapPartitionsRDD[1] at parallelizeDoubles at NumberOperateTest.java:18), which has no missing parents
2019-02-14 15:40:30,994   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.5 KB, free 1048.8 MB)
2019-02-14 15:40:31,005   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2038.0 B, free 1048.8 MB)
2019-02-14 15:40:31,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 62
2019-02-14 15:40:31,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 64
2019-02-14 15:40:31,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 53
2019-02-14 15:40:31,006   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on DESKTOP-Q1PPPMM:23307 (size: 2038.0 B, free: 1048.8 MB)
2019-02-14 15:40:31,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 61
2019-02-14 15:40:31,007   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 59
2019-02-14 15:40:31,007   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 73
2019-02-14 15:40:31,007   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:40:31,007   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 70
2019-02-14 15:40:31,007   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 69
2019-02-14 15:40:31,007   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 72
2019-02-14 15:40:31,007   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 50
2019-02-14 15:40:31,008   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 65
2019-02-14 15:40:31,008   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 67
2019-02-14 15:40:31,008   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 63
2019-02-14 15:40:31,008   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 74
2019-02-14 15:40:31,008   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 55
2019-02-14 15:40:31,008   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 57
2019-02-14 15:40:31,008   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[1] at parallelizeDoubles at NumberOperateTest.java:18) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:40:31,008   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 54
2019-02-14 15:40:31,009   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 1 tasks
2019-02-14 15:40:31,009   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 56
2019-02-14 15:40:31,009   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 51
2019-02-14 15:40:31,009   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 66
2019-02-14 15:40:31,009   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 71
2019-02-14 15:40:31,009   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 58
2019-02-14 15:40:31,010   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:40:31,010   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 3)
2019-02-14 15:40:31,013   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on DESKTOP-Q1PPPMM:23307 in memory (size: 1426.0 B, free: 1048.8 MB)
2019-02-14 15:40:31,015   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 68
2019-02-14 15:40:31,015   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 52
2019-02-14 15:40:31,015   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 60
2019-02-14 15:40:31,017   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 3). 751 bytes result sent to driver
2019-02-14 15:40:31,019   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 3) in 9 ms on localhost (executor driver) (1/1)
2019-02-14 15:40:31,019   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-02-14 15:40:31,019   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 3 (min at NumberOperateTest.java:22) finished in 0.026 s
2019-02-14 15:40:31,020   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 3 finished: min at NumberOperateTest.java:22, took 0.029712 s
2019-02-14 15:40:31,028   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 78
2019-02-14 15:40:31,028   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 80
2019-02-14 15:40:31,028   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 77
2019-02-14 15:40:31,029   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 96
2019-02-14 15:40:31,029   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 94
2019-02-14 15:40:31,029   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 92
2019-02-14 15:40:31,029   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 85
2019-02-14 15:40:31,029   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 98
2019-02-14 15:40:31,029   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 87
2019-02-14 15:40:31,029   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 83
2019-02-14 15:40:31,029   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 88
2019-02-14 15:40:31,029   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 82
2019-02-14 15:40:31,029   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 97
2019-02-14 15:40:31,029   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 79
2019-02-14 15:40:31,030   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 75
2019-02-14 15:40:31,030   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 84
2019-02-14 15:40:31,030   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 90
2019-02-14 15:40:31,031   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: max at NumberOperateTest.java:23
2019-02-14 15:40:31,031   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_3_piece0 on DESKTOP-Q1PPPMM:23307 in memory (size: 2038.0 B, free: 1048.8 MB)
2019-02-14 15:40:31,032   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 4 (max at NumberOperateTest.java:23) with 1 output partitions
2019-02-14 15:40:31,032   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 4 (max at NumberOperateTest.java:23)
2019-02-14 15:40:31,032   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:40:31,032   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:40:31,033   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 4 (MapPartitionsRDD[1] at parallelizeDoubles at NumberOperateTest.java:18), which has no missing parents
2019-02-14 15:40:31,033   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 86
2019-02-14 15:40:31,033   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 76
2019-02-14 15:40:31,033   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 93
2019-02-14 15:40:31,034   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 91
2019-02-14 15:40:31,034   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 89
2019-02-14 15:40:31,034   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 95
2019-02-14 15:40:31,034   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 81
2019-02-14 15:40:31,034   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 99
2019-02-14 15:40:31,035   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 1048.8 MB)
2019-02-14 15:40:31,038   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2038.0 B, free 1048.8 MB)
2019-02-14 15:40:31,040   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on DESKTOP-Q1PPPMM:23307 (size: 2038.0 B, free: 1048.8 MB)
2019-02-14 15:40:31,040   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:40:31,041   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[1] at parallelizeDoubles at NumberOperateTest.java:18) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:40:31,041   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 4.0 with 1 tasks
2019-02-14 15:40:31,042   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:40:31,043   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 4.0 (TID 4)
2019-02-14 15:40:31,050   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 4.0 (TID 4). 708 bytes result sent to driver
2019-02-14 15:40:31,051   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 4.0 (TID 4) in 9 ms on localhost (executor driver) (1/1)
2019-02-14 15:40:31,052   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2019-02-14 15:40:31,053   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 4 (max at NumberOperateTest.java:23) finished in 0.019 s
2019-02-14 15:40:31,053   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 4 finished: max at NumberOperateTest.java:23, took 0.022086 s
2019-02-14 15:40:31,061   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 105
2019-02-14 15:40:31,061   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 116
2019-02-14 15:40:31,062   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 117
2019-02-14 15:40:31,062   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 109
2019-02-14 15:40:31,062   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 112
2019-02-14 15:40:31,063   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 123
2019-02-14 15:40:31,063   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 101
2019-02-14 15:40:31,063   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 120
2019-02-14 15:40:31,064   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 119
2019-02-14 15:40:31,064   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 104
2019-02-14 15:40:31,064   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 114
2019-02-14 15:40:31,064   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 121
2019-02-14 15:40:31,064   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 118
2019-02-14 15:40:31,065   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 124
2019-02-14 15:40:31,065   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 115
2019-02-14 15:40:31,065   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 106
2019-02-14 15:40:31,066   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 102
2019-02-14 15:40:31,066   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 100
2019-02-14 15:40:31,066   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 122
2019-02-14 15:40:31,066   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 110
2019-02-14 15:40:31,066   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 111
2019-02-14 15:40:31,066   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 108
2019-02-14 15:40:31,066   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 107
2019-02-14 15:40:31,066   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 113
2019-02-14 15:40:31,067   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 103
2019-02-14 15:40:31,068   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: variance at NumberOperateTest.java:24
2019-02-14 15:40:31,069   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 5 (variance at NumberOperateTest.java:24) with 1 output partitions
2019-02-14 15:40:31,069   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 5 (variance at NumberOperateTest.java:24)
2019-02-14 15:40:31,069   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:40:31,069   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:40:31,070   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 5 (MapPartitionsRDD[3] at variance at NumberOperateTest.java:24), which has no missing parents
2019-02-14 15:40:31,071   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_4_piece0 on DESKTOP-Q1PPPMM:23307 in memory (size: 2038.0 B, free: 1048.8 MB)
2019-02-14 15:40:31,073   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 1048.8 MB)
2019-02-14 15:40:31,079   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5_piece0 stored as bytes in memory (estimated size 1850.0 B, free 1048.8 MB)
2019-02-14 15:40:31,082   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_5_piece0 in memory on DESKTOP-Q1PPPMM:23307 (size: 1850.0 B, free: 1048.8 MB)
2019-02-14 15:40:31,083   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:40:31,085   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[3] at variance at NumberOperateTest.java:24) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:40:31,085   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 5.0 with 1 tasks
2019-02-14 15:40:31,091   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:40:31,092   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 5.0 (TID 5)
2019-02-14 15:40:31,097   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 5.0 (TID 5). 754 bytes result sent to driver
2019-02-14 15:40:31,098   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 5.0 (TID 5) in 7 ms on localhost (executor driver) (1/1)
2019-02-14 15:40:31,098   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2019-02-14 15:40:31,099   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 5 (variance at NumberOperateTest.java:24) finished in 0.028 s
2019-02-14 15:40:31,099   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 5 finished: variance at NumberOperateTest.java:24, took 0.030916 s
2019-02-14 15:40:31,109   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: sampleVariance at NumberOperateTest.java:25
2019-02-14 15:40:31,110   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 6 (sampleVariance at NumberOperateTest.java:25) with 1 output partitions
2019-02-14 15:40:31,110   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 6 (sampleVariance at NumberOperateTest.java:25)
2019-02-14 15:40:31,110   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:40:31,110   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:40:31,111   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 6 (MapPartitionsRDD[4] at sampleVariance at NumberOperateTest.java:25), which has no missing parents
2019-02-14 15:40:31,114   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 1048.8 MB)
2019-02-14 15:40:31,120   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_6_piece0 stored as bytes in memory (estimated size 1850.0 B, free 1048.8 MB)
2019-02-14 15:40:31,121   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 142
2019-02-14 15:40:31,121   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 144
2019-02-14 15:40:31,121   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 135
2019-02-14 15:40:31,121   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 131
2019-02-14 15:40:31,123   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_6_piece0 in memory on DESKTOP-Q1PPPMM:23307 (size: 1850.0 B, free: 1048.8 MB)
2019-02-14 15:40:31,124   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 6 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:40:31,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[4] at sampleVariance at NumberOperateTest.java:25) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:40:31,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 6.0 with 1 tasks
2019-02-14 15:40:31,127   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 129
2019-02-14 15:40:31,127   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 145
2019-02-14 15:40:31,128   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 143
2019-02-14 15:40:31,128   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 138
2019-02-14 15:40:31,128   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 128
2019-02-14 15:40:31,128   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 136
2019-02-14 15:40:31,128   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 139
2019-02-14 15:40:31,128   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 141
2019-02-14 15:40:31,130   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:40:31,130   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 6.0 (TID 6)
2019-02-14 15:40:31,132   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_5_piece0 on DESKTOP-Q1PPPMM:23307 in memory (size: 1850.0 B, free: 1048.8 MB)
2019-02-14 15:40:31,137   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 6.0 (TID 6). 840 bytes result sent to driver
2019-02-14 15:40:31,138   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 6.0 (TID 6) in 9 ms on localhost (executor driver) (1/1)
2019-02-14 15:40:31,138   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 6.0, whose tasks have all completed, from pool 
2019-02-14 15:40:31,139   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 6 (sampleVariance at NumberOperateTest.java:25) finished in 0.027 s
2019-02-14 15:40:31,140   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 6 finished: sampleVariance at NumberOperateTest.java:25, took 0.030853 s
2019-02-14 15:40:31,142   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 126
2019-02-14 15:40:31,142   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 130
2019-02-14 15:40:31,143   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 149
2019-02-14 15:40:31,143   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 146
2019-02-14 15:40:31,143   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 132
2019-02-14 15:40:31,143   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 140
2019-02-14 15:40:31,143   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 127
2019-02-14 15:40:31,143   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 147
2019-02-14 15:40:31,143   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 148
2019-02-14 15:40:31,144   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 133
2019-02-14 15:40:31,144   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 125
2019-02-14 15:40:31,144   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 134
2019-02-14 15:40:31,144   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 137
2019-02-14 15:40:31,152   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 157
2019-02-14 15:40:31,152   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 170
2019-02-14 15:40:31,152   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 156
2019-02-14 15:40:31,153   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: stdev at NumberOperateTest.java:26
2019-02-14 15:40:31,154   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 7 (stdev at NumberOperateTest.java:26) with 1 output partitions
2019-02-14 15:40:31,154   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 7 (stdev at NumberOperateTest.java:26)
2019-02-14 15:40:31,154   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:40:31,154   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:40:31,154   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_6_piece0 on DESKTOP-Q1PPPMM:23307 in memory (size: 1850.0 B, free: 1048.8 MB)
2019-02-14 15:40:31,155   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 7 (MapPartitionsRDD[5] at stdev at NumberOperateTest.java:26), which has no missing parents
2019-02-14 15:40:31,157   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_7 stored as values in memory (estimated size 3.3 KB, free 1048.8 MB)
2019-02-14 15:40:31,158   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 166
2019-02-14 15:40:31,158   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 160
2019-02-14 15:40:31,158   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 162
2019-02-14 15:40:31,158   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 155
2019-02-14 15:40:31,158   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 159
2019-02-14 15:40:31,158   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 161
2019-02-14 15:40:31,158   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 169
2019-02-14 15:40:31,159   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 152
2019-02-14 15:40:31,159   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 164
2019-02-14 15:40:31,159   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 163
2019-02-14 15:40:31,159   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 167
2019-02-14 15:40:31,159   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 154
2019-02-14 15:40:31,159   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 165
2019-02-14 15:40:31,159   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 171
2019-02-14 15:40:31,160   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 158
2019-02-14 15:40:31,160   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 172
2019-02-14 15:40:31,160   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_7_piece0 stored as bytes in memory (estimated size 1848.0 B, free 1048.8 MB)
2019-02-14 15:40:31,160   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 150
2019-02-14 15:40:31,160   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 174
2019-02-14 15:40:31,160   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 151
2019-02-14 15:40:31,160   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 173
2019-02-14 15:40:31,160   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 168
2019-02-14 15:40:31,160   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 153
2019-02-14 15:40:31,162   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_7_piece0 in memory on DESKTOP-Q1PPPMM:23307 (size: 1848.0 B, free: 1048.8 MB)
2019-02-14 15:40:31,162   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 7 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:40:31,163   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[5] at stdev at NumberOperateTest.java:26) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:40:31,164   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 7.0 with 1 tasks
2019-02-14 15:40:31,165   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:40:31,165   INFO --- [Executor task launch worker for task 7]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 7.0 (TID 7)
2019-02-14 15:40:31,168   INFO --- [Executor task launch worker for task 7]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 7.0 (TID 7). 754 bytes result sent to driver
2019-02-14 15:40:31,169   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 7.0 (TID 7) in 5 ms on localhost (executor driver) (1/1)
2019-02-14 15:40:31,169   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2019-02-14 15:40:31,170   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 7 (stdev at NumberOperateTest.java:26) finished in 0.015 s
2019-02-14 15:40:31,170   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 7 finished: stdev at NumberOperateTest.java:26, took 0.017517 s
2019-02-14 15:40:31,180   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 191
2019-02-14 15:40:31,181   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 194
2019-02-14 15:40:31,181   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 179
2019-02-14 15:40:31,181   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 197
2019-02-14 15:40:31,181   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 184
2019-02-14 15:40:31,181   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 185
2019-02-14 15:40:31,181   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 186
2019-02-14 15:40:31,181   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 187
2019-02-14 15:40:31,181   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: sampleStdev at NumberOperateTest.java:27
2019-02-14 15:40:31,181   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 182
2019-02-14 15:40:31,182   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 190
2019-02-14 15:40:31,183   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 192
2019-02-14 15:40:31,184   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 8 (sampleStdev at NumberOperateTest.java:27) with 1 output partitions
2019-02-14 15:40:31,184   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 8 (sampleStdev at NumberOperateTest.java:27)
2019-02-14 15:40:31,184   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 15:40:31,184   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 15:40:31,185   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 8 (MapPartitionsRDD[6] at sampleStdev at NumberOperateTest.java:27), which has no missing parents
2019-02-14 15:40:31,187   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_7_piece0 on DESKTOP-Q1PPPMM:23307 in memory (size: 1848.0 B, free: 1048.8 MB)
2019-02-14 15:40:31,187   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_8 stored as values in memory (estimated size 3.3 KB, free 1048.8 MB)
2019-02-14 15:40:31,192   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_8_piece0 stored as bytes in memory (estimated size 1851.0 B, free 1048.8 MB)
2019-02-14 15:40:31,193   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 175
2019-02-14 15:40:31,193   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 177
2019-02-14 15:40:31,193   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 196
2019-02-14 15:40:31,193   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 180
2019-02-14 15:40:31,193   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 193
2019-02-14 15:40:31,193   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 181
2019-02-14 15:40:31,193   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 188
2019-02-14 15:40:31,194   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 178
2019-02-14 15:40:31,194   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 176
2019-02-14 15:40:31,194   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 189
2019-02-14 15:40:31,194   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 195
2019-02-14 15:40:31,194   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 199
2019-02-14 15:40:31,194   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_8_piece0 in memory on DESKTOP-Q1PPPMM:23307 (size: 1851.0 B, free: 1048.8 MB)
2019-02-14 15:40:31,194   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 198
2019-02-14 15:40:31,194   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 183
2019-02-14 15:40:31,195   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 8 from broadcast at DAGScheduler.scala:1161
2019-02-14 15:40:31,196   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[6] at sampleStdev at NumberOperateTest.java:27) (first 15 tasks are for partitions Vector(0))
2019-02-14 15:40:31,196   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 8.0 with 1 tasks
2019-02-14 15:40:31,197   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
2019-02-14 15:40:31,198   INFO --- [Executor task launch worker for task 8]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 8.0 (TID 8)
2019-02-14 15:40:31,214   INFO --- [Executor task launch worker for task 8]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 8.0 (TID 8). 754 bytes result sent to driver
2019-02-14 15:40:31,215   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 8.0 (TID 8) in 18 ms on localhost (executor driver) (1/1)
2019-02-14 15:40:31,216   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 8.0, whose tasks have all completed, from pool 
2019-02-14 15:40:31,216   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 8 (sampleStdev at NumberOperateTest.java:27) finished in 0.031 s
2019-02-14 15:40:31,217   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 8 finished: sampleStdev at NumberOperateTest.java:27, took 0.034118 s
2019-02-14 15:40:31,224   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@3b9204{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 15:40:31,225   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 15:40:31,241   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 15:40:31,348   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 15:40:31,349   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 15:40:31,350   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 15:40:31,353   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 15:40:31,359   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 15:40:31,361   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 15:40:31,362   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-e0a758f4-19d1-4763-a2fe-dc87785a6576
2019-02-14 17:15:18,950   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 17:15:19,456   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 17:15:19,614   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 17:15:19,757   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 17:15:19,758   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 17:15:19,758   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 17:15:19,758   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 17:15:19,759   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 17:15:21,468   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 34492.
2019-02-14 17:15:21,515   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 17:15:21,549   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 17:15:21,556   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 17:15:21,557   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 17:15:21,579   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-f9d947a4-2784-4df7-9c83-30f2f6c6ade8
2019-02-14 17:15:21,628   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 17:15:21,657   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 17:15:21,810   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4818ms
2019-02-14 17:15:21,909   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 17:15:21,928   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4937ms
2019-02-14 17:15:21,962   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 17:15:21,962   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 17:15:21,997   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-14 17:15:21,998   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 17:15:21,999   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 17:15:21,999   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,000   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,000   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,001   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,002   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,003   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,003   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,004   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,004   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,005   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,005   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,006   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,006   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,007   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,007   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,008   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,008   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,017   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,018   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,019   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,020   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,021   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 17:15:22,022   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 17:15:22,185   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 17:15:22,267   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34505.
2019-02-14 17:15:22,268   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:34505
2019-02-14 17:15:22,270   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 17:15:22,319   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 34505, None)
2019-02-14 17:15:22,326   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:34505 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 34505, None)
2019-02-14 17:15:22,330   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 34505, None)
2019-02-14 17:15:22,331   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 34505, None)
2019-02-14 17:15:22,605   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 17:15:23,321   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at CheckPointTest.java:23
2019-02-14 17:15:23,356   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at CheckPointTest.java:23) with 1 output partitions
2019-02-14 17:15:23,356   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at CheckPointTest.java:23)
2019-02-14 17:15:23,357   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 17:15:23,359   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 17:15:23,368   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[1] at map at CheckPointTest.java:20), which has no missing parents
2019-02-14 17:15:23,455   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 17:15:23,474   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 1048.8 MB)
2019-02-14 17:15:23,591   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1933.0 B, free 1048.8 MB)
2019-02-14 17:15:23,595   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:34505 (size: 1933.0 B, free: 1048.8 MB)
2019-02-14 17:15:23,597   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 17:15:23,617   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at CheckPointTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 17:15:23,618   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 17:15:23,709   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7359 bytes)
2019-02-14 17:15:23,725   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 17:15:24,025   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 765 bytes result sent to driver
2019-02-14 17:15:24,033   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 362 ms on localhost (executor driver) (1/1)
2019-02-14 17:15:24,036   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 17:15:24,041   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at CheckPointTest.java:23) finished in 0.645 s
2019-02-14 17:15:24,051   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at CheckPointTest.java:23, took 0.728995 s
2019-02-14 17:15:24,070   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at CheckPointTest.java:24
2019-02-14 17:15:24,071   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at CheckPointTest.java:24) with 1 output partitions
2019-02-14 17:15:24,071   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at CheckPointTest.java:24)
2019-02-14 17:15:24,071   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 17:15:24,072   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 17:15:24,072   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[1] at map at CheckPointTest.java:20), which has no missing parents
2019-02-14 17:15:24,076   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 3.4 KB, free 1048.8 MB)
2019-02-14 17:15:24,080   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1933.0 B, free 1048.8 MB)
2019-02-14 17:15:24,081   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:34505 (size: 1933.0 B, free: 1048.8 MB)
2019-02-14 17:15:24,081   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-14 17:15:24,082   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[1] at map at CheckPointTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 17:15:24,082   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-14 17:15:24,084   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7359 bytes)
2019-02-14 17:15:24,084   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-14 17:15:24,089   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 636 bytes result sent to driver
2019-02-14 17:15:24,091   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 8 ms on localhost (executor driver) (1/1)
2019-02-14 17:15:24,091   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-14 17:15:24,092   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at CheckPointTest.java:24) finished in 0.017 s
2019-02-14 17:15:24,092   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: collect at CheckPointTest.java:24, took 0.021769 s
2019-02-14 17:15:24,104   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-02-14 17:15:24,104   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-02-14 17:15:24,104   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-02-14 17:15:24,104   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-02-14 17:15:24,104   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at CheckPointTest.java:25
2019-02-14 17:15:24,104   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-02-14 17:15:24,105   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-02-14 17:15:24,105   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-02-14 17:15:24,105   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-02-14 17:15:24,105   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-02-14 17:15:24,105   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-02-14 17:15:24,105   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-02-14 17:15:24,105   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-02-14 17:15:24,105   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 2 (collect at CheckPointTest.java:25) with 1 output partitions
2019-02-14 17:15:24,105   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-02-14 17:15:24,105   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at CheckPointTest.java:25)
2019-02-14 17:15:24,106   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-02-14 17:15:24,106   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 17:15:24,106   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-02-14 17:15:24,106   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 17:15:24,106   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-02-14 17:15:24,106   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-02-14 17:15:24,106   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-02-14 17:15:24,107   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[1] at map at CheckPointTest.java:20), which has no missing parents
2019-02-14 17:15:24,107   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-02-14 17:15:24,107   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-02-14 17:15:24,107   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-02-14 17:15:24,107   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-02-14 17:15:24,107   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-02-14 17:15:24,107   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-02-14 17:15:24,111   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 3.4 KB, free 1048.8 MB)
2019-02-14 17:15:24,114   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 1933.0 B, free 1048.8 MB)
2019-02-14 17:15:24,120   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:34505 (size: 1933.0 B, free: 1048.8 MB)
2019-02-14 17:15:24,120   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-14 17:15:24,122   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[1] at map at CheckPointTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 17:15:24,122   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-02-14 17:15:24,124   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7359 bytes)
2019-02-14 17:15:24,124   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-02-14 17:15:24,130   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 636 bytes result sent to driver
2019-02-14 17:15:24,131   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on DESKTOP-Q1PPPMM:34505 in memory (size: 1933.0 B, free: 1048.8 MB)
2019-02-14 17:15:24,131   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 8 ms on localhost (executor driver) (1/1)
2019-02-14 17:15:24,132   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-02-14 17:15:24,133   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at CheckPointTest.java:25) finished in 0.024 s
2019-02-14 17:15:24,133   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 2 finished: collect at CheckPointTest.java:25, took 0.028567 s
2019-02-14 17:15:24,138   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 60
2019-02-14 17:15:24,139   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 63
2019-02-14 17:15:24,139   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 53
2019-02-14 17:15:24,139   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 68
2019-02-14 17:15:24,139   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 62
2019-02-14 17:15:24,139   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 59
2019-02-14 17:15:24,139   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 72
2019-02-14 17:15:24,139   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 66
2019-02-14 17:15:24,140   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 57
2019-02-14 17:15:24,140   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 50
2019-02-14 17:15:24,140   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 67
2019-02-14 17:15:24,140   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 73
2019-02-14 17:15:24,140   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 61
2019-02-14 17:15:24,140   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 64
2019-02-14 17:15:24,140   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 56
2019-02-14 17:15:24,140   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 65
2019-02-14 17:15:24,142   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on DESKTOP-Q1PPPMM:34505 in memory (size: 1933.0 B, free: 1048.8 MB)
2019-02-14 17:15:24,144   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 51
2019-02-14 17:15:24,144   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 54
2019-02-14 17:15:24,145   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 52
2019-02-14 17:15:24,145   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 58
2019-02-14 17:15:24,145   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 69
2019-02-14 17:15:24,145   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 70
2019-02-14 17:15:24,145   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 74
2019-02-14 17:15:24,145   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 55
2019-02-14 17:15:24,145   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 71
2019-02-14 17:15:24,145   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-02-14 17:15:24,172   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at CheckPointTest.java:29
2019-02-14 17:15:24,173   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 3 (collect at CheckPointTest.java:29) with 1 output partitions
2019-02-14 17:15:24,173   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 3 (collect at CheckPointTest.java:29)
2019-02-14 17:15:24,173   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 17:15:24,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 17:15:24,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 3 (MapPartitionsRDD[2] at map at CheckPointTest.java:21), which has no missing parents
2019-02-14 17:15:24,176   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.6 KB, free 1048.8 MB)
2019-02-14 17:15:24,182   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1048.8 MB)
2019-02-14 17:15:24,183   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on DESKTOP-Q1PPPMM:34505 (size: 2.6 KB, free: 1048.8 MB)
2019-02-14 17:15:24,184   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-02-14 17:15:24,185   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[2] at map at CheckPointTest.java:21) (first 15 tasks are for partitions Vector(0))
2019-02-14 17:15:24,185   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 1 tasks
2019-02-14 17:15:24,186   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7359 bytes)
2019-02-14 17:15:24,186   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 3)
2019-02-14 17:15:24,192   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 3). 636 bytes result sent to driver
2019-02-14 17:15:24,193   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 3) in 7 ms on localhost (executor driver) (1/1)
2019-02-14 17:15:24,193   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-02-14 17:15:24,194   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 3 (collect at CheckPointTest.java:29) finished in 0.019 s
2019-02-14 17:15:24,194   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 3 finished: collect at CheckPointTest.java:29, took 0.021798 s
2019-02-14 17:15:24,220   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 192.7 KB, free 1048.6 MB)
2019-02-14 17:15:24,245   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 95
2019-02-14 17:15:24,246   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 98
2019-02-14 17:15:24,246   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 77
2019-02-14 17:15:24,247   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 94
2019-02-14 17:15:24,247   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 75
2019-02-14 17:15:24,247   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 88
2019-02-14 17:15:24,247   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 76
2019-02-14 17:15:24,247   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 81
2019-02-14 17:15:24,247   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 83
2019-02-14 17:15:24,247   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 80
2019-02-14 17:15:24,247   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 97
2019-02-14 17:15:24,247   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 78
2019-02-14 17:15:24,247   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 96
2019-02-14 17:15:24,250   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_3_piece0 on DESKTOP-Q1PPPMM:34505 in memory (size: 2.6 KB, free: 1048.8 MB)
2019-02-14 17:15:24,254   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 87
2019-02-14 17:15:24,254   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 91
2019-02-14 17:15:24,254   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 89
2019-02-14 17:15:24,254   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 84
2019-02-14 17:15:24,254   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 93
2019-02-14 17:15:24,254   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 85
2019-02-14 17:15:24,255   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 90
2019-02-14 17:15:24,255   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 86
2019-02-14 17:15:24,255   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 82
2019-02-14 17:15:24,255   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 79
2019-02-14 17:15:24,255   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 99
2019-02-14 17:15:24,255   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 92
2019-02-14 17:15:24,260   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1048.6 MB)
2019-02-14 17:15:24,269   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on DESKTOP-Q1PPPMM:34505 (size: 20.4 KB, free: 1048.8 MB)
2019-02-14 17:15:24,270   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from collect at CheckPointTest.java:29
2019-02-14 17:15:24,274   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at CheckPointTest.java:29
2019-02-14 17:15:24,276   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 4 (collect at CheckPointTest.java:29) with 1 output partitions
2019-02-14 17:15:24,276   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 4 (collect at CheckPointTest.java:29)
2019-02-14 17:15:24,276   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 17:15:24,276   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 17:15:24,277   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 4 (MapPartitionsRDD[2] at map at CheckPointTest.java:21), which has no missing parents
2019-02-14 17:15:24,280   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5 stored as values in memory (estimated size 5.2 KB, free 1048.6 MB)
2019-02-14 17:15:24,288   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1048.6 MB)
2019-02-14 17:15:24,290   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_5_piece0 in memory on DESKTOP-Q1PPPMM:34505 (size: 2.9 KB, free: 1048.8 MB)
2019-02-14 17:15:24,291   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2019-02-14 17:15:24,292   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[2] at map at CheckPointTest.java:21) (first 15 tasks are for partitions Vector(0))
2019-02-14 17:15:24,292   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 4.0 with 1 tasks
2019-02-14 17:15:24,293   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7359 bytes)
2019-02-14 17:15:24,294   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 4.0 (TID 4)
2019-02-14 17:15:24,891   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 4.0 (TID 4). 714 bytes result sent to driver
2019-02-14 17:15:24,892   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 4.0 (TID 4) in 599 ms on localhost (executor driver) (1/1)
2019-02-14 17:15:24,893   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2019-02-14 17:15:24,893   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 4 (collect at CheckPointTest.java:29) finished in 0.614 s
2019-02-14 17:15:24,893   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 4 finished: collect at CheckPointTest.java:29, took 0.619047 s
2019-02-14 17:15:24,894   INFO --- [main]  org.apache.spark.rdd.ReliableCheckpointRDD(line:54) : Checkpointing took 696 ms.
2019-02-14 17:15:24,900   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_6 stored as values in memory (estimated size 192.8 KB, free 1048.4 MB)
2019-02-14 17:15:24,920   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_5_piece0 on DESKTOP-Q1PPPMM:34505 in memory (size: 2.9 KB, free: 1048.8 MB)
2019-02-14 17:15:24,921   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 108
2019-02-14 17:15:24,922   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 116
2019-02-14 17:15:24,922   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 113
2019-02-14 17:15:24,922   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 119
2019-02-14 17:15:24,922   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 114
2019-02-14 17:15:24,922   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 120
2019-02-14 17:15:24,922   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 109
2019-02-14 17:15:24,922   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 123
2019-02-14 17:15:24,923   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 106
2019-02-14 17:15:24,923   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 104
2019-02-14 17:15:24,923   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 117
2019-02-14 17:15:24,923   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 102
2019-02-14 17:15:24,923   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 101
2019-02-14 17:15:24,923   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 118
2019-02-14 17:15:24,923   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 124
2019-02-14 17:15:24,923   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 105
2019-02-14 17:15:24,923   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 110
2019-02-14 17:15:24,924   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 100
2019-02-14 17:15:24,924   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 112
2019-02-14 17:15:24,924   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 122
2019-02-14 17:15:24,924   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 103
2019-02-14 17:15:24,924   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 115
2019-02-14 17:15:24,924   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 111
2019-02-14 17:15:24,924   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 121
2019-02-14 17:15:24,924   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 107
2019-02-14 17:15:24,924   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_6_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1048.4 MB)
2019-02-14 17:15:24,926   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_6_piece0 in memory on DESKTOP-Q1PPPMM:34505 (size: 20.4 KB, free: 1048.8 MB)
2019-02-14 17:15:24,927   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 6 from collect at CheckPointTest.java:29
2019-02-14 17:15:24,938   INFO --- [main]  org.apache.spark.rdd.ReliableRDDCheckpointData(line:54) : Done checkpointing RDD 2 to file:/C:/Users/10160/eclipse-workspace/spark-vlearn/checkDir/b25be71c-8ed0-4a8f-afa9-09ea46cfe4bf/rdd-2, new parent is RDD 3
2019-02-14 17:15:24,953   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at CheckPointTest.java:30
2019-02-14 17:15:24,953   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_4_piece0 on DESKTOP-Q1PPPMM:34505 in memory (size: 20.4 KB, free: 1048.8 MB)
2019-02-14 17:15:24,955   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 5 (collect at CheckPointTest.java:30) with 1 output partitions
2019-02-14 17:15:24,955   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 5 (collect at CheckPointTest.java:30)
2019-02-14 17:15:24,955   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 17:15:24,956   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 17:15:24,956   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 5 (MapPartitionsRDD[2] at map at CheckPointTest.java:21), which has no missing parents
2019-02-14 17:15:24,961   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_7 stored as values in memory (estimated size 5.1 KB, free 1048.6 MB)
2019-02-14 17:15:24,965   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1048.6 MB)
2019-02-14 17:15:24,971   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_7_piece0 in memory on DESKTOP-Q1PPPMM:34505 (size: 2.9 KB, free: 1048.8 MB)
2019-02-14 17:15:24,971   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 7 from broadcast at DAGScheduler.scala:1161
2019-02-14 17:15:24,972   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[2] at map at CheckPointTest.java:21) (first 15 tasks are for partitions Vector(0))
2019-02-14 17:15:24,973   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 5.0 with 1 tasks
2019-02-14 17:15:24,974   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 7133 bytes)
2019-02-14 17:15:24,975   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 5.0 (TID 5)
2019-02-14 17:15:24,989   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 5.0 (TID 5). 679 bytes result sent to driver
2019-02-14 17:15:24,989   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 5.0 (TID 5) in 15 ms on localhost (executor driver) (1/1)
2019-02-14 17:15:24,990   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2019-02-14 17:15:24,990   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 5 (collect at CheckPointTest.java:30) finished in 0.030 s
2019-02-14 17:15:24,990   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 5 finished: collect at CheckPointTest.java:30, took 0.036495 s
2019-02-14 17:15:24,998   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 130
2019-02-14 17:15:24,999   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 133
2019-02-14 17:15:24,999   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 143
2019-02-14 17:15:24,999   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 132
2019-02-14 17:15:24,999   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 125
2019-02-14 17:15:24,999   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 146
2019-02-14 17:15:25,001   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_7_piece0 on DESKTOP-Q1PPPMM:34505 in memory (size: 2.9 KB, free: 1048.8 MB)
2019-02-14 17:15:25,002   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at CheckPointTest.java:31
2019-02-14 17:15:25,004   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 6 (collect at CheckPointTest.java:31) with 1 output partitions
2019-02-14 17:15:25,004   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 6 (collect at CheckPointTest.java:31)
2019-02-14 17:15:25,004   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 142
2019-02-14 17:15:25,004   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 17:15:25,004   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 144
2019-02-14 17:15:25,004   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 141
2019-02-14 17:15:25,004   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 17:15:25,005   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 126
2019-02-14 17:15:25,005   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 136
2019-02-14 17:15:25,005   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 138
2019-02-14 17:15:25,005   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 127
2019-02-14 17:15:25,005   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 6 (MapPartitionsRDD[2] at map at CheckPointTest.java:21), which has no missing parents
2019-02-14 17:15:25,005   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 134
2019-02-14 17:15:25,005   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 147
2019-02-14 17:15:25,005   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 135
2019-02-14 17:15:25,005   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 139
2019-02-14 17:15:25,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 129
2019-02-14 17:15:25,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 145
2019-02-14 17:15:25,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 149
2019-02-14 17:15:25,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 128
2019-02-14 17:15:25,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 131
2019-02-14 17:15:25,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 140
2019-02-14 17:15:25,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 148
2019-02-14 17:15:25,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 137
2019-02-14 17:15:25,009   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_8 stored as values in memory (estimated size 5.1 KB, free 1048.6 MB)
2019-02-14 17:15:25,013   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1048.6 MB)
2019-02-14 17:15:25,015   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_8_piece0 in memory on DESKTOP-Q1PPPMM:34505 (size: 2.9 KB, free: 1048.8 MB)
2019-02-14 17:15:25,016   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 8 from broadcast at DAGScheduler.scala:1161
2019-02-14 17:15:25,016   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[2] at map at CheckPointTest.java:21) (first 15 tasks are for partitions Vector(0))
2019-02-14 17:15:25,016   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 6.0 with 1 tasks
2019-02-14 17:15:25,017   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7133 bytes)
2019-02-14 17:15:25,018   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 6.0 (TID 6)
2019-02-14 17:15:25,025   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 6.0 (TID 6). 636 bytes result sent to driver
2019-02-14 17:15:25,025   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 6.0 (TID 6) in 8 ms on localhost (executor driver) (1/1)
2019-02-14 17:15:25,026   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 6.0, whose tasks have all completed, from pool 
2019-02-14 17:15:25,026   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 6 (collect at CheckPointTest.java:31) finished in 0.019 s
2019-02-14 17:15:25,028   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 6 finished: collect at CheckPointTest.java:31, took 0.024767 s
2019-02-14 17:15:25,035   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 17:15:25,036   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-14 17:15:25,053   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-14 17:15:25,145   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-14 17:15:25,146   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-14 17:15:25,149   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-14 17:15:25,154   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-14 17:15:25,160   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-14 17:15:25,162   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-14 17:15:25,163   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-5ea68462-9f5b-4f74-8c60-7f9bb5a2092b
2019-02-14 17:17:37,505   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-14 17:17:37,927   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-14 17:17:38,072   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-14 17:17:38,176   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-14 17:17:38,176   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-14 17:17:38,177   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-14 17:17:38,177   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-14 17:17:38,178   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-14 17:17:39,653   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 34766.
2019-02-14 17:17:39,687   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-14 17:17:39,719   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-14 17:17:39,722   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-14 17:17:39,723   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-14 17:17:39,739   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-bed30d82-74ad-41e5-a7f2-7cc27c6763cd
2019-02-14 17:17:39,778   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-14 17:17:39,798   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-14 17:17:39,931   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4317ms
2019-02-14 17:17:40,019   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-14 17:17:40,038   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4426ms
2019-02-14 17:17:40,070   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@18c5d94{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-14 17:17:40,070   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-14 17:17:40,098   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1ef51f2{/jobs,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,099   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@527389{/jobs/json,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,100   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5a3466{/jobs/job,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,101   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,101   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/stages,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,101   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/stages/json,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,102   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages/stage,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,103   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,103   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/pool,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,104   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,104   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/storage,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,105   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/storage/json,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,105   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,106   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,106   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/environment,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,107   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/environment/json,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,108   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/executors,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/executors/json,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,110   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,116   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/static,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,117   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c10d84{/,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,118   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15fe372{/api,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,118   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,119   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6ef90e{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-14 17:17:40,120   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-14 17:17:40,273   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-14 17:17:40,338   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34777.
2019-02-14 17:17:40,340   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:34777
2019-02-14 17:17:40,342   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-14 17:17:40,380   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 34777, None)
2019-02-14 17:17:40,384   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:34777 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 34777, None)
2019-02-14 17:17:40,389   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 34777, None)
2019-02-14 17:17:40,390   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 34777, None)
2019-02-14 17:17:40,700   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c612bd{/metrics/json,null,AVAILABLE,@Spark}
2019-02-14 17:17:41,609   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at CheckPointTest.java:22
2019-02-14 17:17:41,641   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at CheckPointTest.java:22) with 1 output partitions
2019-02-14 17:17:41,642   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at CheckPointTest.java:22)
2019-02-14 17:17:41,642   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-14 17:17:41,644   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-14 17:17:41,650   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[1] at map at CheckPointTest.java:20), which has no missing parents
2019-02-14 17:17:41,734   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-14 17:17:41,748   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 1048.8 MB)
2019-02-14 17:17:41,851   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1933.0 B, free 1048.8 MB)
2019-02-14 17:17:41,856   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:34777 (size: 1933.0 B, free: 1048.8 MB)
2019-02-14 17:17:41,858   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-14 17:17:41,876   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at CheckPointTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-14 17:17:41,877   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-14 17:17:41,955   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7359 bytes)
2019-02-14 17:17:41,974   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-14 17:17:42,263   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 768 bytes result sent to driver
2019-02-14 17:17:42,269   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 346 ms on localhost (executor driver) (1/1)
2019-02-14 17:17:42,273   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-14 17:17:42,279   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at CheckPointTest.java:22) finished in 0.598 s
2019-02-14 17:17:42,284   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at CheckPointTest.java:22, took 0.674459 s
2019-02-14 17:17:42,307   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at CheckPointTest.java:24
2019-02-14 17:17:42,310   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at CheckPointTest.java:24) with 1 output partitions
