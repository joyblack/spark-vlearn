2019-02-15 14:19:58,857   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-15 14:19:59,327   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-15 14:19:59,603   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-15 14:19:59,769   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-15 14:19:59,769   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-15 14:19:59,770   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-15 14:19:59,775   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-15 14:19:59,776   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-15 14:20:01,766   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 21105.
2019-02-15 14:20:01,813   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-15 14:20:01,843   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-15 14:20:01,847   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-15 14:20:01,847   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-15 14:20:01,863   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-b291b980-ee8c-4772-8dac-1dd5d6742660
2019-02-15 14:20:01,913   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-15 14:20:01,939   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-15 14:20:02,105   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5321ms
2019-02-15 14:20:02,216   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-15 14:20:02,236   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5452ms
2019-02-15 14:20:02,265   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 14:20:02,266   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-15 14:20:02,297   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,297   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,298   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,298   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,299   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,299   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,300   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,301   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,302   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,302   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,304   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,304   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,306   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,306   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,308   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,318   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,318   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,319   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,320   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,321   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-15 14:20:02,323   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-15 14:20:02,465   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-15 14:20:02,536   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 21115.
2019-02-15 14:20:02,537   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:21115
2019-02-15 14:20:02,539   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-15 14:20:02,580   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 21115, None)
2019-02-15 14:20:02,586   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:21115 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 21115, None)
2019-02-15 14:20:02,591   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 21115, None)
2019-02-15 14:20:02,592   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 21115, None)
2019-02-15 14:20:02,921   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-15 14:20:03,682   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at AccumulatorTest.java:18
2019-02-15 14:20:03,728   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at AccumulatorTest.java:18) with 1 output partitions
2019-02-15 14:20:03,732   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (foreach at AccumulatorTest.java:18)
2019-02-15 14:20:03,733   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 14:20:03,735   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 14:20:03,749   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AccumulatorTest.java:18), which has no missing parents
2019-02-15 14:20:03,956   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-15 14:20:03,973   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.1 KB, free 1048.8 MB)
2019-02-15 14:20:04,091   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1841.0 B, free 1048.8 MB)
2019-02-15 14:20:04,095   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:21115 (size: 1841.0 B, free: 1048.8 MB)
2019-02-15 14:20:04,097   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-15 14:20:04,112   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at AccumulatorTest.java:18) (first 15 tasks are for partitions Vector(0))
2019-02-15 14:20:04,113   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-15 14:20:04,201   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7397 bytes)
2019-02-15 14:20:04,217   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-15 14:20:04,489   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 831 bytes result sent to driver
2019-02-15 14:20:04,498   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 335 ms on localhost (executor driver) (1/1)
2019-02-15 14:20:04,502   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-15 14:20:04,507   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (foreach at AccumulatorTest.java:18) finished in 0.708 s
2019-02-15 14:20:04,514   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreach at AccumulatorTest.java:18, took 0.832078 s
2019-02-15 14:20:04,518   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-15 14:20:04,526   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 14:20:04,532   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-15 14:20:04,544   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-15 14:20:04,565   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-15 14:20:04,566   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-15 14:20:04,574   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-15 14:20:04,577   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-15 14:20:04,586   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-15 14:20:04,587   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-15 14:20:04,588   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-713867f5-d9c7-45c9-b693-5422b23d6fab
2019-02-15 14:52:24,738   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-15 14:52:25,161   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-15 14:52:25,297   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-15 14:52:25,421   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-15 14:52:25,422   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-15 14:52:25,422   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-15 14:52:25,423   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-15 14:52:25,424   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-15 14:52:27,384   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 21776.
2019-02-15 14:52:27,419   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-15 14:52:27,447   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-15 14:52:27,450   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-15 14:52:27,451   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-15 14:52:27,469   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-86be9966-2a60-45d7-b169-2b4c20e75255
2019-02-15 14:52:27,504   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-15 14:52:27,525   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-15 14:52:27,656   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4803ms
2019-02-15 14:52:27,756   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-15 14:52:27,776   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4925ms
2019-02-15 14:52:27,804   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 14:52:27,805   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-15 14:52:27,839   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,840   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,841   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,842   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,843   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,845   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,846   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,847   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,847   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,848   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,849   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,849   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,850   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,850   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,851   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,851   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,852   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,852   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,854   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,864   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,865   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,866   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,867   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,869   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-15 14:52:27,871   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-15 14:52:28,024   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-15 14:52:28,089   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 21785.
2019-02-15 14:52:28,090   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:21785
2019-02-15 14:52:28,092   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-15 14:52:28,127   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 21785, None)
2019-02-15 14:52:28,131   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:21785 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 21785, None)
2019-02-15 14:52:28,136   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 21785, None)
2019-02-15 14:52:28,136   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 21785, None)
2019-02-15 14:52:28,382   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-15 14:52:28,953   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at MyAccumulatorTest.java:23
2019-02-15 14:52:28,987   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at MyAccumulatorTest.java:23) with 1 output partitions
2019-02-15 14:52:28,988   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (foreach at MyAccumulatorTest.java:23)
2019-02-15 14:52:28,988   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 14:52:28,989   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 14:52:28,997   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at MyAccumulatorTest.java:23), which has no missing parents
2019-02-15 14:52:29,086   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-15 14:52:29,102   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 3.1 KB, free 1048.8 MB)
2019-02-15 14:52:29,207   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 1860.0 B, free 1048.8 MB)
2019-02-15 14:52:29,211   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:21785 (size: 1860.0 B, free: 1048.8 MB)
2019-02-15 14:52:29,213   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-15 14:52:29,230   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at MyAccumulatorTest.java:23) (first 15 tasks are for partitions Vector(0))
2019-02-15 14:52:29,231   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-15 14:52:29,317   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7386 bytes)
2019-02-15 14:52:29,334   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-15 14:52:29,614   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 950 bytes result sent to driver
2019-02-15 14:52:29,622   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 339 ms on localhost (executor driver) (1/1)
2019-02-15 14:52:29,627   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-15 14:52:29,635   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (foreach at MyAccumulatorTest.java:23) finished in 0.617 s
2019-02-15 14:52:29,641   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreach at MyAccumulatorTest.java:23, took 0.688537 s
2019-02-15 14:52:29,646   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-15 14:52:29,656   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 14:52:29,658   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-15 14:52:29,672   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-15 14:52:29,693   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-15 14:52:29,693   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-15 14:52:29,701   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-15 14:52:29,705   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-15 14:52:29,712   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-15 14:52:29,713   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-15 14:52:29,713   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-940e69a9-6977-4bd6-9545-8165bfa20708
2019-02-15 16:28:40,356   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-15 16:28:41,052   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-15 16:28:41,243   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-15 16:28:41,406   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-15 16:28:41,407   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-15 16:28:41,407   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-15 16:28:41,408   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-15 16:28:41,409   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-15 16:28:43,567   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 24689.
2019-02-15 16:28:43,637   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-15 16:28:43,680   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-15 16:28:43,687   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-15 16:28:43,688   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-15 16:28:43,708   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-0735a544-4d0d-4241-a602-a71955b9153e
2019-02-15 16:28:43,773   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-15 16:28:43,819   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-15 16:28:44,041   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @7209ms
2019-02-15 16:28:44,128   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-15 16:28:44,147   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @7315ms
2019-02-15 16:28:44,191   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 16:28:44,191   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-15 16:28:44,237   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,238   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,238   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,239   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,240   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,240   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,241   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,242   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,242   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,243   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,244   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,245   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,245   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,246   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,247   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,250   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,251   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,252   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,254   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,254   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,263   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,265   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,268   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,269   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,270   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-15 16:28:44,272   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-15 16:28:44,439   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-15 16:28:44,523   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 24699.
2019-02-15 16:28:44,524   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:24699
2019-02-15 16:28:44,527   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-15 16:28:44,592   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 24699, None)
2019-02-15 16:28:44,597   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:24699 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 24699, None)
2019-02-15 16:28:44,605   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 24699, None)
2019-02-15 16:28:44,606   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 24699, None)
2019-02-15 16:28:44,911   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-15 16:28:45,629   WARN --- [main]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-15 16:28:45,709   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 192.7 KB, free 1048.6 MB)
2019-02-15 16:28:45,899   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1048.6 MB)
2019-02-15 16:28:45,905   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:24699 (size: 20.4 KB, free: 1048.8 MB)
2019-02-15 16:28:45,910   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at TextFileInputTest.java:17
2019-02-15 16:28:46,117   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 193.2 KB, free 1048.4 MB)
2019-02-15 16:28:46,145   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.5 KB, free 1048.4 MB)
2019-02-15 16:28:46,146   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:24699 (size: 20.5 KB, free: 1048.8 MB)
2019-02-15 16:28:46,147   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from wholeTextFiles at TextFileInputTest.java:20
2019-02-15 16:28:47,245   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 2
2019-02-15 16:28:47,331   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at TextFileInputTest.java:22
2019-02-15 16:28:47,365   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at TextFileInputTest.java:22) with 2 output partitions
2019-02-15 16:28:47,365   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at TextFileInputTest.java:22)
2019-02-15 16:28:47,366   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 16:28:47,367   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 16:28:47,376   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (input MapPartitionsRDD[1] at textFile at TextFileInputTest.java:17), which has no missing parents
2019-02-15 16:28:47,428   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.0 KB, free 1048.4 MB)
2019-02-15 16:28:47,445   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1048.4 MB)
2019-02-15 16:28:47,448   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:24699 (size: 2.3 KB, free: 1048.8 MB)
2019-02-15 16:28:47,449   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-15 16:28:47,471   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (input MapPartitionsRDD[1] at textFile at TextFileInputTest.java:17) (first 15 tasks are for partitions Vector(0, 1))
2019-02-15 16:28:47,475   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-02-15 16:28:47,556   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7406 bytes)
2019-02-15 16:28:47,570   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-15 16:28:47,906   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/C:/Users/10160/eclipse-workspace/spark-vlearn/input/file1.txt:0+42
2019-02-15 16:28:47,958   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 846 bytes result sent to driver
2019-02-15 16:28:47,962   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7406 bytes)
2019-02-15 16:28:47,963   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-02-15 16:28:47,967   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 432 ms on localhost (executor driver) (1/2)
2019-02-15 16:28:47,969   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/C:/Users/10160/eclipse-workspace/spark-vlearn/input/file2.txt:0+42
2019-02-15 16:28:47,977   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 760 bytes result sent to driver
2019-02-15 16:28:47,981   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 19 ms on localhost (executor driver) (2/2)
2019-02-15 16:28:47,982   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-15 16:28:47,983   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at TextFileInputTest.java:22) finished in 0.558 s
2019-02-15 16:28:47,992   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at TextFileInputTest.java:22, took 0.659500 s
2019-02-15 16:28:48,329   INFO --- [main]  org.apache.hadoop.mapreduce.lib.input.FileInputFormat(line:281) : Total input paths to process : 2
2019-02-15 16:28:48,631   INFO --- [main]  org.apache.hadoop.mapreduce.lib.input.FileInputFormat(line:281) : Total input paths to process : 2
2019-02-15 16:28:48,654   INFO --- [main]  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat(line:413) : DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2019-02-15 16:28:48,665   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at TextFileInputTest.java:24
2019-02-15 16:28:48,666   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at TextFileInputTest.java:24) with 1 output partitions
2019-02-15 16:28:48,666   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at TextFileInputTest.java:24)
2019-02-15 16:28:48,666   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 16:28:48,666   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 16:28:48,669   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (input MapPartitionsRDD[3] at wholeTextFiles at TextFileInputTest.java:20), which has no missing parents
2019-02-15 16:28:48,675   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.6 KB, free 1048.4 MB)
2019-02-15 16:28:48,685   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1048.4 MB)
2019-02-15 16:28:48,687   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on DESKTOP-Q1PPPMM:24699 (size: 2.1 KB, free: 1048.8 MB)
2019-02-15 16:28:48,688   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-02-15 16:28:48,690   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (input MapPartitionsRDD[3] at wholeTextFiles at TextFileInputTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-15 16:28:48,690   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-15 16:28:48,692   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7569 bytes)
2019-02-15 16:28:48,693   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-02-15 16:28:48,703   INFO --- [Executor task launch worker for task 2]  org.apache.spark.rdd.WholeTextFileRDD(line:54) : Input split: Paths:/C:/Users/10160/eclipse-workspace/spark-vlearn/input/file1.txt:0+42,/C:/Users/10160/eclipse-workspace/spark-vlearn/input/file2.txt:0+42
2019-02-15 16:28:48,783   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1057 bytes result sent to driver
2019-02-15 16:28:48,785   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 94 ms on localhost (executor driver) (1/1)
2019-02-15 16:28:48,785   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-15 16:28:48,786   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at TextFileInputTest.java:24) finished in 0.112 s
2019-02-15 16:28:48,786   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: collect at TextFileInputTest.java:24, took 0.121171 s
2019-02-15 16:28:48,791   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-15 16:28:48,796   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 16:28:48,801   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-15 16:28:48,814   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-15 16:28:48,875   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-15 16:28:48,876   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-15 16:28:48,885   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-15 16:28:48,889   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-15 16:28:48,897   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-15 16:28:48,898   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-15 16:28:48,901   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-a6228dd0-1187-40a0-a270-433e62672e41
2019-02-15 16:30:02,050   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-15 16:30:02,764   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-15 16:30:03,040   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-15 16:30:03,196   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-15 16:30:03,198   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-15 16:30:03,201   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-15 16:30:03,202   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-15 16:30:03,203   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-15 16:30:05,401   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 24736.
2019-02-15 16:30:05,445   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-15 16:30:05,535   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-15 16:30:05,544   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-15 16:30:05,549   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-15 16:30:05,595   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-fb2f52f6-e13a-464c-bd90-acd3c1433ed5
2019-02-15 16:30:05,649   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-15 16:30:05,673   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-15 16:30:05,819   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @6786ms
2019-02-15 16:30:05,911   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-15 16:30:05,931   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @6900ms
2019-02-15 16:30:05,961   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 16:30:05,962   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-15 16:30:05,997   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-15 16:30:05,998   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-15 16:30:05,999   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,000   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,001   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,001   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,002   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,003   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,003   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,004   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,004   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,005   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,005   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,006   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,007   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,007   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,009   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,010   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,010   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,011   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,023   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,023   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,026   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,026   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,027   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-15 16:30:06,028   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-15 16:30:06,183   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-15 16:30:06,257   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 24745.
2019-02-15 16:30:06,257   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:24745
2019-02-15 16:30:06,259   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-15 16:30:06,302   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 24745, None)
2019-02-15 16:30:06,308   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:24745 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 24745, None)
2019-02-15 16:30:06,312   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 24745, None)
2019-02-15 16:30:06,312   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 24745, None)
2019-02-15 16:30:06,640   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-15 16:30:07,405   WARN --- [main]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-15 16:30:07,469   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 192.7 KB, free 1048.6 MB)
2019-02-15 16:30:07,622   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1048.6 MB)
2019-02-15 16:30:07,636   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:24745 (size: 20.4 KB, free: 1048.8 MB)
2019-02-15 16:30:07,644   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at TextFileInputTest.java:17
2019-02-15 16:30:07,776   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 193.2 KB, free 1048.4 MB)
2019-02-15 16:30:07,800   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.5 KB, free 1048.4 MB)
2019-02-15 16:30:07,801   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:24745 (size: 20.5 KB, free: 1048.8 MB)
2019-02-15 16:30:07,802   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from wholeTextFiles at TextFileInputTest.java:20
2019-02-15 16:30:08,246   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 2
2019-02-15 16:30:08,328   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at TextFileInputTest.java:23
2019-02-15 16:30:08,367   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at TextFileInputTest.java:23) with 2 output partitions
2019-02-15 16:30:08,367   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at TextFileInputTest.java:23)
2019-02-15 16:30:08,369   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 16:30:08,372   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 16:30:08,392   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (input MapPartitionsRDD[1] at textFile at TextFileInputTest.java:17), which has no missing parents
2019-02-15 16:30:08,472   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.0 KB, free 1048.4 MB)
2019-02-15 16:30:08,478   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1048.4 MB)
2019-02-15 16:30:08,484   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:24745 (size: 2.3 KB, free: 1048.8 MB)
2019-02-15 16:30:08,485   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-15 16:30:08,499   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (input MapPartitionsRDD[1] at textFile at TextFileInputTest.java:17) (first 15 tasks are for partitions Vector(0, 1))
2019-02-15 16:30:08,500   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-02-15 16:30:08,581   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7406 bytes)
2019-02-15 16:30:08,595   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-15 16:30:08,921   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/C:/Users/10160/eclipse-workspace/spark-vlearn/input/file1.txt:0+42
2019-02-15 16:30:08,984   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 846 bytes result sent to driver
2019-02-15 16:30:08,988   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7406 bytes)
2019-02-15 16:30:08,988   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-02-15 16:30:08,993   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 432 ms on localhost (executor driver) (1/2)
2019-02-15 16:30:08,994   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/C:/Users/10160/eclipse-workspace/spark-vlearn/input/file2.txt:0+42
2019-02-15 16:30:09,004   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 760 bytes result sent to driver
2019-02-15 16:30:09,009   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 22 ms on localhost (executor driver) (2/2)
2019-02-15 16:30:09,012   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-15 16:30:09,012   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at TextFileInputTest.java:23) finished in 0.543 s
2019-02-15 16:30:09,020   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at TextFileInputTest.java:23, took 0.691342 s
2019-02-15 16:30:09,391   INFO --- [main]  org.apache.hadoop.mapreduce.lib.input.FileInputFormat(line:281) : Total input paths to process : 2
2019-02-15 16:30:09,680   INFO --- [main]  org.apache.hadoop.mapreduce.lib.input.FileInputFormat(line:281) : Total input paths to process : 2
2019-02-15 16:30:09,698   INFO --- [main]  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat(line:413) : DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2019-02-15 16:30:09,714   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at TextFileInputTest.java:26
2019-02-15 16:30:09,714   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at TextFileInputTest.java:26) with 1 output partitions
2019-02-15 16:30:09,715   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at TextFileInputTest.java:26)
2019-02-15 16:30:09,715   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 16:30:09,715   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 16:30:09,715   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (input MapPartitionsRDD[3] at wholeTextFiles at TextFileInputTest.java:20), which has no missing parents
2019-02-15 16:30:09,719   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.6 KB, free 1048.4 MB)
2019-02-15 16:30:09,727   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1048.4 MB)
2019-02-15 16:30:09,728   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on DESKTOP-Q1PPPMM:24745 (size: 2.1 KB, free: 1048.8 MB)
2019-02-15 16:30:09,729   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-02-15 16:30:09,731   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (input MapPartitionsRDD[3] at wholeTextFiles at TextFileInputTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-15 16:30:09,731   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-15 16:30:09,733   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7569 bytes)
2019-02-15 16:30:09,734   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-02-15 16:30:09,747   INFO --- [Executor task launch worker for task 2]  org.apache.spark.rdd.WholeTextFileRDD(line:54) : Input split: Paths:/C:/Users/10160/eclipse-workspace/spark-vlearn/input/file1.txt:0+42,/C:/Users/10160/eclipse-workspace/spark-vlearn/input/file2.txt:0+42
2019-02-15 16:30:09,822   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1100 bytes result sent to driver
2019-02-15 16:30:09,823   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 91 ms on localhost (executor driver) (1/1)
2019-02-15 16:30:09,824   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-15 16:30:09,825   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at TextFileInputTest.java:26) finished in 0.107 s
2019-02-15 16:30:09,825   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: collect at TextFileInputTest.java:26, took 0.111457 s
2019-02-15 16:30:09,828   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-15 16:30:09,835   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 16:30:09,840   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-15 16:30:09,855   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-15 16:30:09,906   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-15 16:30:09,907   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-15 16:30:09,914   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-15 16:30:09,918   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-15 16:30:09,931   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-15 16:30:09,932   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-15 16:30:09,933   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-b8619370-2573-4b54-93fc-5c2f937711fe
2019-02-15 16:32:05,664   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-15 16:32:06,195   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-15 16:32:06,369   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-15 16:32:06,510   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-15 16:32:06,511   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-15 16:32:06,512   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-15 16:32:06,512   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-15 16:32:06,513   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-15 16:32:08,388   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 24801.
2019-02-15 16:32:08,429   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-15 16:32:08,461   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-15 16:32:08,464   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-15 16:32:08,465   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-15 16:32:08,487   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-035e5feb-dc03-48a1-87a0-a4802dfdf7b6
2019-02-15 16:32:08,541   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-15 16:32:08,564   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-15 16:32:08,699   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5299ms
2019-02-15 16:32:08,787   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-15 16:32:08,803   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5406ms
2019-02-15 16:32:08,832   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 16:32:08,833   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-15 16:32:08,864   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f5a68a{/jobs,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,864   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1748410{/jobs/json,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,865   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@143529a{/jobs/job,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,866   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,866   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@d10c1a{/stages,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,867   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/stages/json,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,867   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages/stage,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,868   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f7d241{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,868   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@335fa8{/stages/pool,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,870   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,871   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/storage,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,872   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/storage/json,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,873   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,876   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,876   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/environment,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,877   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/environment/json,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,878   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/executors,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,878   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/executors/json,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,879   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,879   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,886   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/static,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,889   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e10ff8{/,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,891   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@17a1e2d{/api,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,892   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01298{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,892   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e75d13{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-15 16:32:08,894   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-15 16:32:09,043   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-15 16:32:09,118   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 24810.
2019-02-15 16:32:09,118   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:24810
2019-02-15 16:32:09,122   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-15 16:32:09,166   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 24810, None)
2019-02-15 16:32:09,174   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:24810 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 24810, None)
2019-02-15 16:32:09,183   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 24810, None)
2019-02-15 16:32:09,183   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 24810, None)
2019-02-15 16:32:09,452   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1739528{/metrics/json,null,AVAILABLE,@Spark}
2019-02-15 16:32:10,028   WARN --- [main]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-15 16:32:10,088   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 192.7 KB, free 1048.6 MB)
2019-02-15 16:32:10,236   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1048.6 MB)
2019-02-15 16:32:10,241   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:24810 (size: 20.4 KB, free: 1048.8 MB)
2019-02-15 16:32:10,246   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at TextFileInputTest.java:17
2019-02-15 16:32:10,357   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 193.2 KB, free 1048.4 MB)
2019-02-15 16:32:10,379   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.5 KB, free 1048.4 MB)
2019-02-15 16:32:10,380   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:24810 (size: 20.5 KB, free: 1048.8 MB)
2019-02-15 16:32:10,382   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from wholeTextFiles at TextFileInputTest.java:19
2019-02-15 16:32:10,461   INFO --- [main]  org.apache.hadoop.conf.Configuration.deprecation(line:1129) : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2019-02-15 16:32:10,469   INFO --- [main]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-02-15 16:32:10,825   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 2
2019-02-15 16:32:10,859   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: runJob at SparkHadoopWriter.scala:78
2019-02-15 16:32:10,886   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions
2019-02-15 16:32:10,886   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (runJob at SparkHadoopWriter.scala:78)
2019-02-15 16:32:10,887   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 16:32:10,889   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 16:32:10,895   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at saveAsTextFile at TextFileInputTest.java:21), which has no missing parents
2019-02-15 16:32:10,970   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 66.4 KB, free 1048.3 MB)
2019-02-15 16:32:10,979   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.0 KB, free 1048.3 MB)
2019-02-15 16:32:10,985   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:24810 (size: 24.0 KB, free: 1048.7 MB)
2019-02-15 16:32:10,985   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-15 16:32:11,002   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at saveAsTextFile at TextFileInputTest.java:21) (first 15 tasks are for partitions Vector(0, 1))
2019-02-15 16:32:11,004   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-02-15 16:32:11,084   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7406 bytes)
2019-02-15 16:32:11,096   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-15 16:32:11,425   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/C:/Users/10160/eclipse-workspace/spark-vlearn/input/file1.txt:0+42
2019-02-15 16:32:11,450   INFO --- [Executor task launch worker for task 0]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-02-15 16:32:11,835   INFO --- [Executor task launch worker for task 0]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190215163210_0004_m_000000_0' to file:/C:/Users/10160/eclipse-workspace/spark-vlearn/out1/_temporary/0/task_20190215163210_0004_m_000000
2019-02-15 16:32:11,836   INFO --- [Executor task launch worker for task 0]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190215163210_0004_m_000000_0: Committed
2019-02-15 16:32:11,858   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1164 bytes result sent to driver
2019-02-15 16:32:11,863   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7406 bytes)
2019-02-15 16:32:11,864   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-02-15 16:32:11,867   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 799 ms on localhost (executor driver) (1/2)
2019-02-15 16:32:11,893   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/C:/Users/10160/eclipse-workspace/spark-vlearn/input/file2.txt:0+42
2019-02-15 16:32:11,897   INFO --- [Executor task launch worker for task 1]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-02-15 16:32:12,281   INFO --- [Executor task launch worker for task 1]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190215163210_0004_m_000001_0' to file:/C:/Users/10160/eclipse-workspace/spark-vlearn/out1/_temporary/0/task_20190215163210_0004_m_000001
2019-02-15 16:32:12,281   INFO --- [Executor task launch worker for task 1]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190215163210_0004_m_000001_0: Committed
2019-02-15 16:32:12,282   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1078 bytes result sent to driver
2019-02-15 16:32:12,296   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 434 ms on localhost (executor driver) (2/2)
2019-02-15 16:32:12,299   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-15 16:32:12,300   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (runJob at SparkHadoopWriter.scala:78) finished in 1.359 s
2019-02-15 16:32:12,311   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 1.451675 s
2019-02-15 16:32:12,720   INFO --- [main]  org.apache.spark.internal.io.SparkHadoopWriter(line:54) : Job job_20190215163210_0004 committed.
2019-02-15 16:32:12,729   INFO --- [main]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-02-15 16:32:13,068   INFO --- [main]  org.apache.hadoop.mapreduce.lib.input.FileInputFormat(line:281) : Total input paths to process : 2
2019-02-15 16:32:13,359   INFO --- [main]  org.apache.hadoop.mapreduce.lib.input.FileInputFormat(line:281) : Total input paths to process : 2
2019-02-15 16:32:13,378   INFO --- [main]  org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat(line:413) : DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
2019-02-15 16:32:13,397   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: runJob at SparkHadoopWriter.scala:78
2019-02-15 16:32:13,398   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2019-02-15 16:32:13,398   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:78)
2019-02-15 16:32:13,398   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 16:32:13,398   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 16:32:13,399   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[5] at saveAsTextFile at TextFileInputTest.java:22), which has no missing parents
2019-02-15 16:32:13,413   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 66.1 KB, free 1048.2 MB)
2019-02-15 16:32:13,418   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.8 KB, free 1048.2 MB)
2019-02-15 16:32:13,419   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on DESKTOP-Q1PPPMM:24810 (size: 23.8 KB, free: 1048.7 MB)
2019-02-15 16:32:13,420   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-02-15 16:32:13,421   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at saveAsTextFile at TextFileInputTest.java:22) (first 15 tasks are for partitions Vector(0))
2019-02-15 16:32:13,422   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-15 16:32:13,424   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7569 bytes)
2019-02-15 16:32:13,425   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-02-15 16:32:13,472   INFO --- [Executor task launch worker for task 2]  org.apache.spark.rdd.WholeTextFileRDD(line:54) : Input split: Paths:/C:/Users/10160/eclipse-workspace/spark-vlearn/input/file1.txt:0+42,/C:/Users/10160/eclipse-workspace/spark-vlearn/input/file2.txt:0+42
2019-02-15 16:32:13,474   INFO --- [Executor task launch worker for task 2]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-02-15 16:32:13,923   INFO --- [Executor task launch worker for task 2]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190215163212_0005_m_000000_0' to file:/C:/Users/10160/eclipse-workspace/spark-vlearn/out2/_temporary/0/task_20190215163212_0005_m_000000
2019-02-15 16:32:13,923   INFO --- [Executor task launch worker for task 2]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190215163212_0005_m_000000_0: Committed
2019-02-15 16:32:13,924   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1121 bytes result sent to driver
2019-02-15 16:32:13,926   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 504 ms on localhost (executor driver) (1/1)
2019-02-15 16:32:13,926   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-15 16:32:13,928   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (runJob at SparkHadoopWriter.scala:78) finished in 0.526 s
2019-02-15 16:32:13,929   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: runJob at SparkHadoopWriter.scala:78, took 0.531864 s
2019-02-15 16:32:14,522   INFO --- [main]  org.apache.spark.internal.io.SparkHadoopWriter(line:54) : Job job_20190215163212_0005 committed.
2019-02-15 16:32:14,526   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-15 16:32:14,535   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c3c1e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 16:32:14,536   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-15 16:32:14,549   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-15 16:32:14,604   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-15 16:32:14,604   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-15 16:32:14,615   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-15 16:32:14,618   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-15 16:32:14,626   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-15 16:32:14,626   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-15 16:32:14,627   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-a0219aec-d485-4dc2-a2c6-8b0d3683eebb
2019-02-15 16:48:49,383   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-15 16:48:49,819   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-15 16:48:49,958   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-15 16:48:50,088   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-15 16:48:50,090   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-15 16:48:50,094   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-15 16:48:50,095   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-15 16:48:50,096   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-15 16:48:51,794   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 25492.
2019-02-15 16:48:51,828   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-15 16:48:51,857   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-15 16:48:51,860   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-15 16:48:51,861   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-15 16:48:51,877   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-c0b92cf8-844f-474c-94e7-b9d531b368af
2019-02-15 16:48:51,911   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-15 16:48:51,935   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-15 16:48:52,065   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4871ms
2019-02-15 16:48:52,138   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-15 16:48:52,159   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4965ms
2019-02-15 16:48:52,187   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 16:48:52,188   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-15 16:48:52,217   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@12c9ba6{/jobs,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,218   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/jobs/json,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,219   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,219   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,220   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,220   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/json,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,221   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/stage,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,221   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,222   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/stages/pool,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,222   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,223   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,224   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/json,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,224   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,225   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,226   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/environment,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,226   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/environment/json,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,226   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,227   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/json,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,228   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,229   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34ace1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,235   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fe6067{/static,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,236   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@a4842b{/,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,237   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/api,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,237   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5b0b86{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,238   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13c354a{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-15 16:48:52,240   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-15 16:48:52,377   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-15 16:48:52,454   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 25501.
2019-02-15 16:48:52,456   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:25501
2019-02-15 16:48:52,458   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-15 16:48:52,498   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 25501, None)
2019-02-15 16:48:52,502   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:25501 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 25501, None)
2019-02-15 16:48:52,505   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 25501, None)
2019-02-15 16:48:52,506   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 25501, None)
2019-02-15 16:48:52,829   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1de5768{/metrics/json,null,AVAILABLE,@Spark}
2019-02-15 16:48:53,498   WARN --- [main]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-15 16:48:53,560   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 192.7 KB, free 1048.6 MB)
2019-02-15 16:48:53,699   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1048.6 MB)
2019-02-15 16:48:53,707   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:25501 (size: 20.4 KB, free: 1048.8 MB)
2019-02-15 16:48:53,711   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at JsonFileTest.java:17
2019-02-15 16:48:53,839   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-02-15 16:48:53,907   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at JsonFileTest.java:21
2019-02-15 16:48:53,930   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at JsonFileTest.java:21) with 1 output partitions
2019-02-15 16:48:53,931   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at JsonFileTest.java:21)
2019-02-15 16:48:53,932   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 16:48:53,933   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 16:48:53,938   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[2] at map at JsonFileTest.java:19), which has no missing parents
2019-02-15 16:48:53,981   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 1048.6 MB)
2019-02-15 16:48:53,987   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.7 KB, free 1048.6 MB)
2019-02-15 16:48:53,988   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:25501 (size: 2.7 KB, free: 1048.8 MB)
2019-02-15 16:48:53,989   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-15 16:48:54,009   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at JsonFileTest.java:19) (first 15 tasks are for partitions Vector(0))
2019-02-15 16:48:54,010   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-15 16:48:54,072   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7400 bytes)
2019-02-15 16:48:54,084   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-15 16:48:54,356   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/C:/Users/10160/eclipse-workspace/spark-vlearn/user.json:0+51
2019-02-15 16:48:54,536   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1007 bytes result sent to driver
2019-02-15 16:48:54,542   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 484 ms on localhost (executor driver) (1/1)
2019-02-15 16:48:54,547   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-15 16:48:54,555   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at JsonFileTest.java:21) finished in 0.574 s
2019-02-15 16:48:54,561   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at JsonFileTest.java:21, took 0.653227 s
2019-02-15 16:48:54,567   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-15 16:48:54,573   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 16:48:54,575   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-15 16:48:54,585   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-15 16:48:54,612   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-15 16:48:54,612   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-15 16:48:54,618   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-15 16:48:54,621   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-15 16:48:54,633   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-15 16:48:54,634   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-15 16:48:54,635   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-de4bd933-11a0-4be5-af46-7e538dff5c73
2019-02-15 17:26:29,935   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-15 17:26:30,489   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-15 17:26:30,632   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-15 17:26:30,752   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-15 17:26:30,753   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-15 17:26:30,754   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-15 17:26:30,754   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-15 17:26:30,755   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-15 17:26:32,514   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 26696.
2019-02-15 17:26:32,565   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-15 17:26:32,599   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-15 17:26:32,602   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-15 17:26:32,603   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-15 17:26:32,622   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-45d0608a-13eb-494b-b50e-8adb22635ec8
2019-02-15 17:26:32,656   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-15 17:26:32,678   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-15 17:26:32,797   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4991ms
2019-02-15 17:26:32,885   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-15 17:26:32,907   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5102ms
2019-02-15 17:26:32,938   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 17:26:32,939   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-15 17:26:32,970   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@12c9ba6{/jobs,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,971   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/jobs/json,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,971   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,972   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,972   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,973   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/json,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,973   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/stage,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,974   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,975   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/stages/pool,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,976   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,976   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,977   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/json,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,977   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,978   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,978   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/environment,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,979   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/environment/json,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,979   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,980   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/json,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,981   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,981   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34ace1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,988   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fe6067{/static,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,989   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@a4842b{/,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,990   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/api,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,990   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5b0b86{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,991   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13c354a{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-15 17:26:32,992   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-15 17:26:33,123   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-15 17:26:33,208   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 26705.
2019-02-15 17:26:33,209   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:26705
2019-02-15 17:26:33,212   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-15 17:26:33,265   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 26705, None)
2019-02-15 17:26:33,274   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:26705 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 26705, None)
2019-02-15 17:26:33,281   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 26705, None)
2019-02-15 17:26:33,282   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 26705, None)
2019-02-15 17:26:33,533   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1de5768{/metrics/json,null,AVAILABLE,@Spark}
2019-02-15 17:26:34,027   INFO --- [main]  org.apache.spark.rdd.SequenceFileRDDFunctions(line:54) : Saving as sequence file of type (NullWritable,BytesWritable)
2019-02-15 17:26:34,162   INFO --- [main]  org.apache.hadoop.conf.Configuration.deprecation(line:1129) : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2019-02-15 17:26:34,169   INFO --- [main]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-02-15 17:26:34,230   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: runJob at SparkHadoopWriter.scala:78
2019-02-15 17:26:34,269   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2019-02-15 17:26:34,270   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (runJob at SparkHadoopWriter.scala:78)
2019-02-15 17:26:34,271   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 17:26:34,274   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 17:26:34,281   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[2] at saveAsObjectFile at ObjectFileTest.java:21), which has no missing parents
2019-02-15 17:26:34,392   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-15 17:26:34,403   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 65.6 KB, free 1048.7 MB)
2019-02-15 17:26:34,501   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.5 KB, free 1048.7 MB)
2019-02-15 17:26:34,505   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:26705 (size: 23.5 KB, free: 1048.8 MB)
2019-02-15 17:26:34,507   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-15 17:26:34,527   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at saveAsObjectFile at ObjectFileTest.java:21) (first 15 tasks are for partitions Vector(0))
2019-02-15 17:26:34,529   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-15 17:26:34,627   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7355 bytes)
2019-02-15 17:26:34,643   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-15 17:26:34,903   INFO --- [Executor task launch worker for task 0]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-02-15 17:26:35,456   INFO --- [Executor task launch worker for task 0]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190215172634_0002_m_000000_0' to file:/C:/Users/10160/eclipse-workspace/spark-vlearn/out/_temporary/0/task_20190215172634_0002_m_000000
2019-02-15 17:26:35,458   INFO --- [Executor task launch worker for task 0]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190215172634_0002_m_000000_0: Committed
2019-02-15 17:26:35,490   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1078 bytes result sent to driver
2019-02-15 17:26:35,500   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 917 ms on localhost (executor driver) (1/1)
2019-02-15 17:26:35,511   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-15 17:26:35,518   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (runJob at SparkHadoopWriter.scala:78) finished in 1.213 s
2019-02-15 17:26:35,528   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 1.297832 s
2019-02-15 17:26:36,119   INFO --- [main]  org.apache.spark.internal.io.SparkHadoopWriter(line:54) : Job job_20190215172634_0002 committed.
2019-02-15 17:26:36,386   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 192.8 KB, free 1048.5 MB)
2019-02-15 17:26:36,456   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1048.5 MB)
2019-02-15 17:26:36,458   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:26705 (size: 20.4 KB, free: 1048.8 MB)
2019-02-15 17:26:36,459   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from objectFile at ObjectFileTest.java:23
2019-02-15 17:26:37,507   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-02-15 17:26:37,533   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at ObjectFileTest.java:25
2019-02-15 17:26:37,534   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (foreach at ObjectFileTest.java:25) with 1 output partitions
2019-02-15 17:26:37,534   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (foreach at ObjectFileTest.java:25)
2019-02-15 17:26:37,534   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 17:26:37,535   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 17:26:37,535   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[4] at objectFile at ObjectFileTest.java:23), which has no missing parents
2019-02-15 17:26:37,556   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.7 KB, free 1048.5 MB)
2019-02-15 17:26:37,566   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1048.5 MB)
2019-02-15 17:26:37,571   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:26705 (size: 2.6 KB, free: 1048.8 MB)
2019-02-15 17:26:37,572   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-15 17:26:37,573   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at objectFile at ObjectFileTest.java:23) (first 15 tasks are for partitions Vector(0))
2019-02-15 17:26:37,574   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-15 17:26:37,583   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7405 bytes)
2019-02-15 17:26:37,584   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-15 17:26:37,710   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/C:/Users/10160/eclipse-workspace/spark-vlearn/out/part-00000:0+159
2019-02-15 17:26:37,744   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 800 bytes result sent to driver
2019-02-15 17:26:37,746   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 171 ms on localhost (executor driver) (1/1)
2019-02-15 17:26:37,747   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-15 17:26:37,747   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (foreach at ObjectFileTest.java:25) finished in 0.196 s
2019-02-15 17:26:37,748   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: foreach at ObjectFileTest.java:25, took 0.215324 s
2019-02-15 17:26:37,751   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-15 17:26:37,759   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 17:26:37,762   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-15 17:26:37,779   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-15 17:26:37,836   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-15 17:26:37,838   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-15 17:26:37,848   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-15 17:26:37,851   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-15 17:26:37,862   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-15 17:26:37,862   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-15 17:26:37,863   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-fecda187-4053-498c-81b1-3a559c580377
2019-02-15 17:30:28,949   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-15 17:30:29,353   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-15 17:30:29,483   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-15 17:30:29,584   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-15 17:30:29,585   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-15 17:30:29,585   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-15 17:30:29,586   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-15 17:30:29,587   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-15 17:30:31,329   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 26801.
2019-02-15 17:30:31,402   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-15 17:30:31,467   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-15 17:30:31,471   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-15 17:30:31,472   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-15 17:30:31,500   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-3e297ebe-ecc4-4640-aa9b-dea151d5d12d
2019-02-15 17:30:31,539   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-15 17:30:31,559   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-15 17:30:31,685   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4590ms
2019-02-15 17:30:31,763   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-15 17:30:31,779   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4686ms
2019-02-15 17:30:31,804   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 17:30:31,805   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-15 17:30:31,835   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@12c9ba6{/jobs,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,837   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/jobs/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,838   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,839   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,840   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,840   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,841   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/stage,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,841   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,842   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/stages/pool,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,842   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,843   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,843   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,845   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,846   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/environment,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,846   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/environment/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,847   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,847   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,848   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,848   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34ace1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,855   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fe6067{/static,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,855   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@a4842b{/,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,857   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/api,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,858   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5b0b86{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,859   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13c354a{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-15 17:30:31,860   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-15 17:30:31,990   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-15 17:30:32,053   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 26810.
2019-02-15 17:30:32,054   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:26810
2019-02-15 17:30:32,056   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-15 17:30:32,092   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 26810, None)
2019-02-15 17:30:32,096   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:26810 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 26810, None)
2019-02-15 17:30:32,099   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 26810, None)
2019-02-15 17:30:32,100   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 26810, None)
2019-02-15 17:30:32,335   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1de5768{/metrics/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:32,839   INFO --- [main]  org.apache.spark.rdd.SequenceFileRDDFunctions(line:54) : Saving as sequence file of type (NullWritable,BytesWritable)
2019-02-15 17:30:32,964   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-15 17:30:32,971   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 17:30:32,972   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-15 17:30:32,989   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-15 17:30:33,001   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-15 17:30:33,001   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-15 17:30:33,008   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-15 17:30:33,012   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-15 17:30:33,020   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-15 17:30:33,021   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-15 17:30:33,021   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-d30b8463-462c-4f1a-b1bc-42d018ae72c7
2019-02-15 17:30:50,388   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-15 17:30:50,850   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-15 17:30:51,001   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-15 17:30:51,107   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-15 17:30:51,107   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-15 17:30:51,108   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-15 17:30:51,108   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-15 17:30:51,109   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-15 17:30:52,739   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 26831.
2019-02-15 17:30:52,774   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-15 17:30:52,821   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-15 17:30:52,826   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-15 17:30:52,827   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-15 17:30:52,856   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-de79b84c-4244-4245-994f-1c75eec4cac3
2019-02-15 17:30:52,912   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-15 17:30:52,937   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-15 17:30:53,070   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4519ms
2019-02-15 17:30:53,168   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-15 17:30:53,192   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4643ms
2019-02-15 17:30:53,220   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 17:30:53,221   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-15 17:30:53,257   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@12c9ba6{/jobs,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,258   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/jobs/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,259   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,260   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,262   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,263   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,264   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/stage,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,265   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,266   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/stages/pool,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,268   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,269   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,270   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,271   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,272   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/environment,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,273   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/environment/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,274   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,274   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,276   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,276   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34ace1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fe6067{/static,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,291   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@a4842b{/,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,292   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/api,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,294   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5b0b86{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,295   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13c354a{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-15 17:30:53,300   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-15 17:30:53,473   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-15 17:30:53,549   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 26841.
2019-02-15 17:30:53,550   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:26841
2019-02-15 17:30:53,552   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-15 17:30:53,603   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 26841, None)
2019-02-15 17:30:53,608   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:26841 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 26841, None)
2019-02-15 17:30:53,613   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 26841, None)
2019-02-15 17:30:53,614   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 26841, None)
2019-02-15 17:30:53,863   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1de5768{/metrics/json,null,AVAILABLE,@Spark}
2019-02-15 17:30:54,383   INFO --- [main]  org.apache.spark.rdd.SequenceFileRDDFunctions(line:54) : Saving as sequence file of type (NullWritable,BytesWritable)
2019-02-15 17:30:54,501   INFO --- [main]  org.apache.hadoop.conf.Configuration.deprecation(line:1129) : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2019-02-15 17:30:54,506   INFO --- [main]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-02-15 17:30:54,563   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: runJob at SparkHadoopWriter.scala:78
2019-02-15 17:30:54,602   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2019-02-15 17:30:54,603   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (runJob at SparkHadoopWriter.scala:78)
2019-02-15 17:30:54,604   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 17:30:54,606   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 17:30:54,615   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[2] at saveAsObjectFile at ObjectFileTest.java:20), which has no missing parents
2019-02-15 17:30:54,737   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-15 17:30:54,748   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 65.6 KB, free 1048.7 MB)
2019-02-15 17:30:54,849   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.4 KB, free 1048.7 MB)
2019-02-15 17:30:54,853   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:26841 (size: 23.4 KB, free: 1048.8 MB)
2019-02-15 17:30:54,855   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-15 17:30:54,872   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at saveAsObjectFile at ObjectFileTest.java:20) (first 15 tasks are for partitions Vector(0))
2019-02-15 17:30:54,873   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-15 17:30:54,950   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7468 bytes)
2019-02-15 17:30:54,965   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-15 17:30:55,187   INFO --- [Executor task launch worker for task 0]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-02-15 17:30:55,484   INFO --- [Executor task launch worker for task 0]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190215173054_0002_m_000000_0' to file:/C:/Users/10160/eclipse-workspace/spark-vlearn/out_user/_temporary/0/task_20190215173054_0002_m_000000
2019-02-15 17:30:55,485   INFO --- [Executor task launch worker for task 0]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190215173054_0002_m_000000_0: Committed
2019-02-15 17:30:55,506   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1078 bytes result sent to driver
2019-02-15 17:30:55,515   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 594 ms on localhost (executor driver) (1/1)
2019-02-15 17:30:55,519   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-15 17:30:55,524   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (runJob at SparkHadoopWriter.scala:78) finished in 0.880 s
2019-02-15 17:30:55,531   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 0.966876 s
2019-02-15 17:30:55,764   INFO --- [main]  org.apache.spark.internal.io.SparkHadoopWriter(line:54) : Job job_20190215173054_0002 committed.
2019-02-15 17:30:55,833   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 192.8 KB, free 1048.5 MB)
2019-02-15 17:30:55,861   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1048.5 MB)
2019-02-15 17:30:55,862   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:26841 (size: 20.4 KB, free: 1048.8 MB)
2019-02-15 17:30:55,863   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from objectFile at ObjectFileTest.java:21
2019-02-15 17:30:56,108   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-02-15 17:30:56,126   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at ObjectFileTest.java:22
2019-02-15 17:30:56,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (foreach at ObjectFileTest.java:22) with 1 output partitions
2019-02-15 17:30:56,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (foreach at ObjectFileTest.java:22)
2019-02-15 17:30:56,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 17:30:56,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 17:30:56,128   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[4] at objectFile at ObjectFileTest.java:21), which has no missing parents
2019-02-15 17:30:56,143   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.8 KB, free 1048.5 MB)
2019-02-15 17:30:56,150   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1048.5 MB)
2019-02-15 17:30:56,152   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:26841 (size: 2.6 KB, free: 1048.8 MB)
2019-02-15 17:30:56,153   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-15 17:30:56,154   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at objectFile at ObjectFileTest.java:21) (first 15 tasks are for partitions Vector(0))
2019-02-15 17:30:56,154   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-15 17:30:56,162   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7410 bytes)
2019-02-15 17:30:56,162   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-15 17:30:56,251   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/C:/Users/10160/eclipse-workspace/spark-vlearn/out_user/part-00000:0+355
2019-02-15 17:30:56,276   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 843 bytes result sent to driver
2019-02-15 17:30:56,278   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 124 ms on localhost (executor driver) (1/1)
2019-02-15 17:30:56,278   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-15 17:30:56,279   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (foreach at ObjectFileTest.java:22) finished in 0.140 s
2019-02-15 17:30:56,279   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: foreach at ObjectFileTest.java:22, took 0.152784 s
2019-02-15 17:30:56,282   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-15 17:30:56,288   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 17:30:56,290   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-15 17:30:56,301   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-15 17:30:56,342   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-15 17:30:56,342   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-15 17:30:56,350   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-15 17:30:56,353   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-15 17:30:56,361   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-15 17:30:56,361   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-15 17:30:56,362   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-56ff347d-2291-46f7-b943-5201697106f1
2019-02-15 17:33:07,319   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-15 17:33:07,738   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-15 17:33:07,872   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-15 17:33:07,985   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-15 17:33:07,986   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-15 17:33:07,986   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-15 17:33:07,987   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-15 17:33:07,987   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-15 17:33:09,577   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 26903.
2019-02-15 17:33:09,610   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-15 17:33:09,638   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-15 17:33:09,641   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-15 17:33:09,642   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-15 17:33:09,658   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-245727f6-ea68-470e-bf2f-195423c62620
2019-02-15 17:33:09,694   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-15 17:33:09,716   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-15 17:33:09,842   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4305ms
2019-02-15 17:33:09,919   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-15 17:33:09,935   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4398ms
2019-02-15 17:33:09,959   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 17:33:09,959   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-15 17:33:09,985   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@12c9ba6{/jobs,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,986   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/jobs/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,986   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,988   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,989   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/stage,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,989   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,990   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/stages/pool,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,990   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,991   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,991   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,992   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,992   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,993   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/environment,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,995   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/environment/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,996   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,997   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,998   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-15 17:33:09,998   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34ace1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:10,005   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fe6067{/static,null,AVAILABLE,@Spark}
2019-02-15 17:33:10,005   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@a4842b{/,null,AVAILABLE,@Spark}
2019-02-15 17:33:10,006   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/api,null,AVAILABLE,@Spark}
2019-02-15 17:33:10,007   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5b0b86{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-15 17:33:10,007   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13c354a{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-15 17:33:10,009   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-15 17:33:10,140   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-15 17:33:10,200   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 26912.
2019-02-15 17:33:10,201   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:26912
2019-02-15 17:33:10,204   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-15 17:33:10,240   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 26912, None)
2019-02-15 17:33:10,245   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:26912 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 26912, None)
2019-02-15 17:33:10,248   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 26912, None)
2019-02-15 17:33:10,249   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 26912, None)
2019-02-15 17:33:10,481   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1de5768{/metrics/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:11,067   WARN --- [main]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-15 17:33:11,132   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 192.7 KB, free 1048.6 MB)
2019-02-15 17:33:11,276   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1048.6 MB)
2019-02-15 17:33:11,281   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:26912 (size: 20.4 KB, free: 1048.8 MB)
2019-02-15 17:33:11,285   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from sequenceFile at ObjectFileTest.java:24
2019-02-15 17:33:11,376   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-15 17:33:11,383   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 17:33:11,384   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-15 17:33:11,409   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-15 17:33:11,435   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-15 17:33:11,435   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-15 17:33:11,442   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-15 17:33:11,448   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-15 17:33:11,457   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-15 17:33:11,458   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-15 17:33:11,459   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-ed6dd0a5-9429-4780-9faf-4d9ce461da4f
2019-02-15 17:33:48,348   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-15 17:33:48,758   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-15 17:33:48,896   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-15 17:33:49,015   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-15 17:33:49,015   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-15 17:33:49,016   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-15 17:33:49,016   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-15 17:33:49,017   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-15 17:33:50,596   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 26933.
2019-02-15 17:33:50,654   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-15 17:33:50,687   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-15 17:33:50,691   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-15 17:33:50,691   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-15 17:33:50,710   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-47f7b1da-9c4a-484d-bf04-e8f1a27a98e5
2019-02-15 17:33:50,756   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-15 17:33:50,780   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-15 17:33:50,922   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4385ms
2019-02-15 17:33:50,999   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-15 17:33:51,014   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4478ms
2019-02-15 17:33:51,040   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 17:33:51,040   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-15 17:33:51,067   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@12c9ba6{/jobs,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,067   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/jobs/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,068   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,069   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,069   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,069   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,070   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/stage,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,071   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,071   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/stages/pool,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,072   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,072   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,073   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,073   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,074   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,074   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/environment,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,075   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/environment/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,076   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,077   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,077   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,078   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34ace1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,084   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fe6067{/static,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,084   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@a4842b{/,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,085   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/api,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,086   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5b0b86{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,087   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13c354a{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-15 17:33:51,089   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-15 17:33:51,218   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-15 17:33:51,281   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 26942.
2019-02-15 17:33:51,282   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:26942
2019-02-15 17:33:51,284   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-15 17:33:51,323   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 26942, None)
2019-02-15 17:33:51,327   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:26942 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 26942, None)
2019-02-15 17:33:51,331   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 26942, None)
2019-02-15 17:33:51,332   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 26942, None)
2019-02-15 17:33:51,591   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1de5768{/metrics/json,null,AVAILABLE,@Spark}
2019-02-15 17:33:52,116   WARN --- [main]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-15 17:33:52,167   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 192.7 KB, free 1048.6 MB)
2019-02-15 17:33:52,298   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1048.6 MB)
2019-02-15 17:33:52,301   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:26942 (size: 20.4 KB, free: 1048.8 MB)
2019-02-15 17:33:52,306   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from sequenceFile at ObjectFileTest.java:24
2019-02-15 17:33:52,847   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-02-15 17:33:52,904   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at ObjectFileTest.java:25
2019-02-15 17:33:52,933   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at ObjectFileTest.java:25) with 1 output partitions
2019-02-15 17:33:52,933   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (foreach at ObjectFileTest.java:25)
2019-02-15 17:33:52,934   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 17:33:52,935   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 17:33:52,941   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (out_user HadoopRDD[0] at sequenceFile at ObjectFileTest.java:24), which has no missing parents
2019-02-15 17:33:52,998   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 3.5 KB, free 1048.6 MB)
2019-02-15 17:33:53,001   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1048.6 MB)
2019-02-15 17:33:53,002   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:26942 (size: 2.1 KB, free: 1048.8 MB)
2019-02-15 17:33:53,003   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-15 17:33:53,017   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (out_user HadoopRDD[0] at sequenceFile at ObjectFileTest.java:24) (first 15 tasks are for partitions Vector(0))
2019-02-15 17:33:53,018   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-15 17:33:53,082   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7410 bytes)
2019-02-15 17:33:53,093   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-15 17:33:53,434   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/C:/Users/10160/eclipse-workspace/spark-vlearn/out_user/part-00000:0+355
2019-02-15 17:33:53,493   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 800 bytes result sent to driver
2019-02-15 17:33:53,500   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 436 ms on localhost (executor driver) (1/1)
2019-02-15 17:33:53,504   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-15 17:33:53,509   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (foreach at ObjectFileTest.java:25) finished in 0.530 s
2019-02-15 17:33:53,516   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: foreach at ObjectFileTest.java:25, took 0.611064 s
2019-02-15 17:33:53,520   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-15 17:33:53,527   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 17:33:53,529   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-15 17:33:53,545   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-15 17:33:53,570   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-15 17:33:53,570   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-15 17:33:53,578   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-15 17:33:53,580   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-15 17:33:53,588   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-15 17:33:53,588   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-15 17:33:53,589   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-a10b4383-bc26-4863-bf56-b7b2d5376549
2019-02-15 17:34:56,370   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-15 17:34:56,836   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-15 17:34:56,974   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-15 17:34:57,085   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-15 17:34:57,085   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-15 17:34:57,086   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-15 17:34:57,086   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-15 17:34:57,088   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-15 17:34:58,773   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 26974.
2019-02-15 17:34:58,810   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-15 17:34:58,852   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-15 17:34:58,856   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-15 17:34:58,856   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-15 17:34:58,872   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-34d64c3c-1bea-4be9-8a2d-98a9ec7561e3
2019-02-15 17:34:58,914   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-15 17:34:58,939   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-15 17:34:59,063   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @4850ms
2019-02-15 17:34:59,146   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-15 17:34:59,170   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @4958ms
2019-02-15 17:34:59,233   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 17:34:59,234   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-15 17:34:59,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@12c9ba6{/jobs,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/jobs/json,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,268   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,268   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,269   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,269   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/json,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,270   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/stage,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,271   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,271   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/stages/pool,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,272   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,272   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,273   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/json,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,273   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,275   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,276   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/environment,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,276   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/environment/json,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,277   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,277   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/json,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,278   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,278   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34ace1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,285   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fe6067{/static,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,285   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@a4842b{/,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,287   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/api,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,288   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5b0b86{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,288   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13c354a{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-15 17:34:59,291   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-15 17:34:59,460   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-15 17:34:59,586   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 26983.
2019-02-15 17:34:59,588   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:26983
2019-02-15 17:34:59,592   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-15 17:34:59,641   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 26983, None)
2019-02-15 17:34:59,645   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:26983 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 26983, None)
2019-02-15 17:34:59,649   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 26983, None)
2019-02-15 17:34:59,650   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 26983, None)
2019-02-15 17:34:59,904   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1de5768{/metrics/json,null,AVAILABLE,@Spark}
2019-02-15 17:35:00,538   WARN --- [main]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-15 17:35:00,591   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 192.7 KB, free 1048.6 MB)
2019-02-15 17:35:00,718   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1048.6 MB)
2019-02-15 17:35:00,722   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:26983 (size: 20.4 KB, free: 1048.8 MB)
2019-02-15 17:35:00,729   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from sequenceFile at ObjectFileTest.java:24
2019-02-15 17:35:01,059   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-02-15 17:35:01,158   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at ObjectFileTest.java:25
2019-02-15 17:35:01,188   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (foreach at ObjectFileTest.java:25) with 1 output partitions
2019-02-15 17:35:01,189   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (foreach at ObjectFileTest.java:25)
2019-02-15 17:35:01,189   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 17:35:01,191   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 17:35:01,196   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (out_user HadoopRDD[0] at sequenceFile at ObjectFileTest.java:24), which has no missing parents
2019-02-15 17:35:01,256   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 3.5 KB, free 1048.6 MB)
2019-02-15 17:35:01,259   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1048.6 MB)
2019-02-15 17:35:01,260   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:26983 (size: 2.1 KB, free: 1048.8 MB)
2019-02-15 17:35:01,261   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-02-15 17:35:01,273   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (out_user HadoopRDD[0] at sequenceFile at ObjectFileTest.java:24) (first 15 tasks are for partitions Vector(0))
2019-02-15 17:35:01,274   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-15 17:35:01,347   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7410 bytes)
2019-02-15 17:35:01,359   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-15 17:35:01,707   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/C:/Users/10160/eclipse-workspace/spark-vlearn/out_user/part-00000:0+355
2019-02-15 17:35:01,772  ERROR --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:91) : Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ClassCastException: org.apache.hadoop.io.NullWritable cannot be cast to java.lang.String
	at com.spark.input.ObjectFileTest.lambda$main$1282d8df$1(ObjectFileTest.java:25)
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$foreach$1(JavaRDDLike.scala:351)
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$foreach$1$adapted(JavaRDDLike.scala:351)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:927)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:927)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2019-02-15 17:35:01,813   WARN --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:66) : Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ClassCastException: org.apache.hadoop.io.NullWritable cannot be cast to java.lang.String
	at com.spark.input.ObjectFileTest.lambda$main$1282d8df$1(ObjectFileTest.java:25)
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$foreach$1(JavaRDDLike.scala:351)
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$foreach$1$adapted(JavaRDDLike.scala:351)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:927)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:927)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2019-02-15 17:35:01,815  ERROR --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:70) : Task 0 in stage 0.0 failed 1 times; aborting job
2019-02-15 17:35:01,817   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-15 17:35:01,830   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Cancelling stage 0
2019-02-15 17:35:01,831   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Killing all running tasks in stage 0: Stage cancelled
2019-02-15 17:35:01,834   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (foreach at ObjectFileTest.java:25) failed in 0.600 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ClassCastException: org.apache.hadoop.io.NullWritable cannot be cast to java.lang.String
	at com.spark.input.ObjectFileTest.lambda$main$1282d8df$1(ObjectFileTest.java:25)
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$foreach$1(JavaRDDLike.scala:351)
	at org.apache.spark.api.java.JavaRDDLike.$anonfun$foreach$1$adapted(JavaRDDLike.scala:351)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2(RDD.scala:927)
	at org.apache.spark.rdd.RDD.$anonfun$foreach$2$adapted(RDD.scala:927)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
2019-02-15 17:35:01,840   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 failed: foreach at ObjectFileTest.java:25, took 0.681236 s
2019-02-15 17:35:01,847   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-02-15 17:35:01,853   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 17:35:01,855   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-15 17:35:01,866   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-15 17:35:01,891   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-15 17:35:01,892   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-15 17:35:01,898   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-15 17:35:01,901   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-15 17:35:01,908   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-15 17:35:01,908   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-15 17:35:01,909   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-5e3d1c4e-5ab4-423b-9e76-6e53e44b560e
2019-02-15 19:44:25,131   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-15 19:44:25,747   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-15 19:44:25,958   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-15 19:44:26,125   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-15 19:44:26,126   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-15 19:44:26,127   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-15 19:44:26,127   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-15 19:44:26,128   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-15 19:44:28,152   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 30201.
2019-02-15 19:44:28,197   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-15 19:44:28,237   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-15 19:44:28,241   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-15 19:44:28,242   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-15 19:44:28,265   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-a38fe976-5532-4e35-a0c4-1ccabb97ff0b
2019-02-15 19:44:28,313   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-15 19:44:28,344   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-15 19:44:28,497   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5794ms
2019-02-15 19:44:28,604   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-15 19:44:28,628   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5926ms
2019-02-15 19:44:28,662   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 19:44:28,663   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-15 19:44:28,698   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@12c9ba6{/jobs,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,703   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/jobs/json,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,704   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,705   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,705   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,706   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/json,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,707   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/stage,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,708   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,709   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/stages/pool,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,709   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,710   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,710   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/json,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,712   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,713   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,714   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/environment,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,714   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/environment/json,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,715   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,716   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/json,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,717   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,717   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34ace1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,728   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fe6067{/static,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,729   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@a4842b{/,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,730   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/api,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,731   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5b0b86{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,732   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13c354a{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-15 19:44:28,735   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-15 19:44:28,912   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-15 19:44:28,999   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 30210.
2019-02-15 19:44:29,000   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:30210
2019-02-15 19:44:29,003   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-15 19:44:29,057   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 30210, None)
2019-02-15 19:44:29,063   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:30210 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 30210, None)
2019-02-15 19:44:29,069   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 30210, None)
2019-02-15 19:44:29,070   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 30210, None)
2019-02-15 19:44:29,395   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1de5768{/metrics/json,null,AVAILABLE,@Spark}
2019-02-15 19:44:30,236   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: runJob at SparkHadoopWriter.scala:78
2019-02-15 19:44:30,279   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2019-02-15 19:44:30,280   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (runJob at SparkHadoopWriter.scala:78)
2019-02-15 19:44:30,281   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 19:44:30,283   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 19:44:30,292   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelizePairs at HadoopAPITest.java:23), which has no missing parents
2019-02-15 19:44:30,460   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-15 19:44:30,476   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 64.0 KB, free 1048.7 MB)
2019-02-15 19:44:30,610   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 1048.7 MB)
2019-02-15 19:44:30,616   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:30210 (size: 22.4 KB, free: 1048.8 MB)
2019-02-15 19:44:30,620   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-15 19:44:30,642   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelizePairs at HadoopAPITest.java:23) (first 15 tasks are for partitions Vector(0))
2019-02-15 19:44:30,644   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-15 19:44:30,773   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7437 bytes)
2019-02-15 19:44:30,792   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-15 19:44:31,434   INFO --- [Executor task launch worker for task 0]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190215194429_0000_r_000000_0' to file:/C:/Users/10160/eclipse-workspace/spark-vlearn/out_h/_temporary/0/task_20190215194429_0000_r_000000
2019-02-15 19:44:31,435   INFO --- [Executor task launch worker for task 0]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190215194429_0000_r_000000_0: Committed
2019-02-15 19:44:31,461   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1078 bytes result sent to driver
2019-02-15 19:44:31,472   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 744 ms on localhost (executor driver) (1/1)
2019-02-15 19:44:31,477   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-15 19:44:31,487   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (runJob at SparkHadoopWriter.scala:78) finished in 1.160 s
2019-02-15 19:44:31,496   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 1.259324 s
2019-02-15 19:44:31,793   INFO --- [main]  org.apache.spark.internal.io.SparkHadoopWriter(line:54) : Job job_20190215194429_0000 committed.
2019-02-15 19:44:31,808   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 19:44:31,810   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-15 19:44:31,827   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-15 19:44:31,857   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-15 19:44:31,858   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-15 19:44:31,866   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-15 19:44:31,871   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-15 19:44:31,878   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-15 19:44:31,882   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-15 19:44:31,882   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-25229722-d139-4e2e-8bfa-e5efaee2d6c8
2019-02-15 19:50:37,302   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.0
2019-02-15 19:50:37,771   WARN --- [main]  org.apache.hadoop.util.NativeCodeLoader(line:62) : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-02-15 19:50:37,928   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: test
2019-02-15 19:50:38,065   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: 10160
2019-02-15 19:50:38,066   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: 10160
2019-02-15 19:50:38,067   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-02-15 19:50:38,068   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-02-15 19:50:38,069   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(10160); groups with view permissions: Set(); users  with modify permissions: Set(10160); groups with modify permissions: Set()
2019-02-15 19:50:40,041   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 30339.
2019-02-15 19:50:40,094   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-02-15 19:50:40,128   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-02-15 19:50:40,132   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-02-15 19:50:40,133   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-02-15 19:50:40,157   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\10160\AppData\Local\Temp\blockmgr-118f1280-66dd-4840-a6df-f2602a3f3f8c
2019-02-15 19:50:40,212   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1048.8 MB
2019-02-15 19:50:40,239   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-02-15 19:50:40,387   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @5180ms
2019-02-15 19:50:40,503   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-02-15 19:50:40,523   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @5319ms
2019-02-15 19:50:40,559   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 19:50:40,560   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-02-15 19:50:40,597   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@12c9ba6{/jobs,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,598   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64b240{/jobs/json,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,599   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@104476e{/jobs/job,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,600   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b75258{/jobs/job/json,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,601   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@260bdc{/stages,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,603   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e01201{/stages/json,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,604   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@183717b{/stages/stage,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,605   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1363cf5{/stages/stage/json,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,606   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1829ac1{/stages/pool,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,606   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1af352a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1b1fde8{/storage,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,608   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1eebbff{/storage/json,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,609   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d6f11{/storage/rdd,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,610   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@990c1b{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,610   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53f95d{/environment,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,611   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@828f6b{/environment/json,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,612   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1be250d{/executors,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,613   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13f1d75{/executors/json,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,614   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8f9e{/executors/threadDump,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,615   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34ace1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,625   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fe6067{/static,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,626   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@a4842b{/,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,628   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38a7fe{/api,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,629   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5b0b86{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,630   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@13c354a{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-02-15 19:50:40,632   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://DESKTOP-Q1PPPMM:4040
2019-02-15 19:50:40,801   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-02-15 19:50:40,864   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 30348.
2019-02-15 19:50:40,865   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on DESKTOP-Q1PPPMM:30348
2019-02-15 19:50:40,868   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-02-15 19:50:40,923   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 30348, None)
2019-02-15 19:50:40,928   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager DESKTOP-Q1PPPMM:30348 with 1048.8 MB RAM, BlockManagerId(driver, DESKTOP-Q1PPPMM, 30348, None)
2019-02-15 19:50:40,934   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, DESKTOP-Q1PPPMM, 30348, None)
2019-02-15 19:50:40,934   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, DESKTOP-Q1PPPMM, 30348, None)
2019-02-15 19:50:41,209   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1de5768{/metrics/json,null,AVAILABLE,@Spark}
2019-02-15 19:50:41,904   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: runJob at SparkHadoopWriter.scala:78
2019-02-15 19:50:41,932   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2019-02-15 19:50:41,932   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (runJob at SparkHadoopWriter.scala:78)
2019-02-15 19:50:41,933   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 19:50:41,935   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 19:50:41,943   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelizePairs at HadoopAPITest.java:26), which has no missing parents
2019-02-15 19:50:42,090   WARN --- [dag-scheduler-event-loop]  org.apache.spark.util.SizeEstimator(line:66) : Failed to check whether UseCompressedOops is set; assuming yes
2019-02-15 19:50:42,113   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 64.0 KB, free 1048.7 MB)
2019-02-15 19:50:42,263   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.4 KB, free 1048.7 MB)
2019-02-15 19:50:42,268   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on DESKTOP-Q1PPPMM:30348 (size: 22.4 KB, free: 1048.8 MB)
2019-02-15 19:50:42,271   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-02-15 19:50:42,294   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelizePairs at HadoopAPITest.java:26) (first 15 tasks are for partitions Vector(0))
2019-02-15 19:50:42,296   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-02-15 19:50:42,409   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7437 bytes)
2019-02-15 19:50:42,428   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-02-15 19:50:42,872   INFO --- [Executor task launch worker for task 0]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190215195041_0000_r_000000_0' to file:/C:/Users/10160/eclipse-workspace/spark-vlearn/out_h/_temporary/0/task_20190215195041_0000_r_000000
2019-02-15 19:50:42,873   INFO --- [Executor task launch worker for task 0]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190215195041_0000_r_000000_0: Committed
2019-02-15 19:50:42,892   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1078 bytes result sent to driver
2019-02-15 19:50:42,902   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 543 ms on localhost (executor driver) (1/1)
2019-02-15 19:50:42,908   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-02-15 19:50:42,916   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (runJob at SparkHadoopWriter.scala:78) finished in 0.941 s
2019-02-15 19:50:42,924   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 1.019580 s
2019-02-15 19:50:43,221   INFO --- [main]  org.apache.spark.internal.io.SparkHadoopWriter(line:54) : Job job_20190215195041_0000 committed.
2019-02-15 19:50:43,317   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 192.9 KB, free 1048.5 MB)
2019-02-15 19:50:43,345   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.5 KB, free 1048.5 MB)
2019-02-15 19:50:43,346   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on DESKTOP-Q1PPPMM:30348 (size: 20.5 KB, free: 1048.8 MB)
2019-02-15 19:50:43,347   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from newAPIHadoopFile at HadoopAPITest.java:35
2019-02-15 19:50:43,611   INFO --- [main]  org.apache.hadoop.mapreduce.lib.input.FileInputFormat(line:281) : Total input paths to process : 1
2019-02-15 19:50:43,653   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: foreach at HadoopAPITest.java:41
2019-02-15 19:50:43,655   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (foreach at HadoopAPITest.java:41) with 1 output partitions
2019-02-15 19:50:43,655   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (foreach at HadoopAPITest.java:41)
2019-02-15 19:50:43,655   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-02-15 19:50:43,655   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-02-15 19:50:43,656   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (out_h NewHadoopRDD[1] at newAPIHadoopFile at HadoopAPITest.java:35), which has no missing parents
2019-02-15 19:50:43,675   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 3.1 KB, free 1048.5 MB)
2019-02-15 19:50:43,680   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 1877.0 B, free 1048.5 MB)
2019-02-15 19:50:43,683   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on DESKTOP-Q1PPPMM:30348 (size: 1877.0 B, free: 1048.8 MB)
2019-02-15 19:50:43,683   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-02-15 19:50:43,685   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (out_h NewHadoopRDD[1] at newAPIHadoopFile at HadoopAPITest.java:35) (first 15 tasks are for partitions Vector(0))
2019-02-15 19:50:43,685   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-02-15 19:50:43,696   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7451 bytes)
2019-02-15 19:50:43,696   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-02-15 19:50:43,964   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.NewHadoopRDD(line:54) : Input split: file:/C:/Users/10160/eclipse-workspace/spark-vlearn/out_h/part-r-00000:0+19
2019-02-15 19:50:43,996   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 843 bytes result sent to driver
2019-02-15 19:50:43,998   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 311 ms on localhost (executor driver) (1/1)
2019-02-15 19:50:43,998   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-02-15 19:50:43,999   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (foreach at HadoopAPITest.java:41) finished in 0.326 s
2019-02-15 19:50:43,999   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: foreach at HadoopAPITest.java:41, took 0.345376 s
2019-02-15 19:50:44,006   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@63ec0b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-02-15 19:50:44,007   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://DESKTOP-Q1PPPMM:4040
2019-02-15 19:50:44,021   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-02-15 19:50:44,061   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-02-15 19:50:44,061   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-02-15 19:50:44,069   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-02-15 19:50:44,074   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-02-15 19:50:44,082   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-02-15 19:50:44,085   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-02-15 19:50:44,086   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\10160\AppData\Local\Temp\spark-d6457dce-695c-4075-a54d-9addd2656481
